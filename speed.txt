
batch 1

============================================================
SUMMARY
============================================================
K     -logP(avg)      chars/s         batch 
------------------------------------------------------------
1     612.28          111.8           1     
2     558.32          96.3            1     
4     558.38          86.6            1     
8     558.47          74.7            1     
16    557.85          60.0            1     
32    558.67          43.8            1     

============================================================
SUMMARY
============================================================
K     -logP           chars/s        
------------------------------------------------------------
1     612.28          102.7          
2     558.32          82.8           
4     558.37          67.5           
8     558.46          50.3           
16    557.84          33.3           
32    558.67          20.2           


============================================================
SUMMARY
============================================================
K     -logP(avg)      chars/s         batch 
------------------------------------------------------------
1     612.75          435.9           8     
2     558.69          270.1           8     
4     558.95          159.0           8     
8     557.77          86.5            8     
16    558.22          45.3            8     
32    558.21          22.8            8     

============================================================
SUMMARY
============================================================
K     -logP(avg)      chars/s         batch 
------------------------------------------------------------
1     612.11          579.1           16    
2     558.35          315.7           16    
4     558.85          178.0           16    
8     558.30          93.4            16    
16    558.34          47.3            16    
32    558.34          23.8            16   

python benchmark_k_values.py --batch-size 1
python benchmark_k_values.py --batch-size 8 --trie-max-batch 256

After numpy removing:
on 1000
============================================================
SUMMARY
============================================================
K     -logP(avg)      chars/s         batch
------------------------------------------------------------
1     612.11          1005.8          16
2     558.35          666.5           16
4     558.85          425.0           16
8     558.30          235.7           16
16    558.35          124.3           16
32    558.34          63.2            16


on 4000

============================================================
SUMMARY
============================================================
K     -logP(avg)      chars/s         batch 
------------------------------------------------------------
1     1937.00         866.2           16    
2     1756.54         546.7           16    
4     1748.70         325.2           16    
8     1747.44         167.7           16    
16    1748.65         84.5            16    
32    1748.47         42.6            16    


- so I get OOM, why?

- speculative decoding for charlm, coalescence

all the changes, so now by default it runs 1 batch, how do I just generate not necessarily

try batch size 1 speed, oom issues, error at the end from vllm

QUESTIONS

the tokenization is batched since the trie is batched?

explain all changes done in detail

explain batched trie

why the .item()??
 so now by default it runs 1 batch, how do I just generate not necessarily? 
 For example, if I want to just run the algorithm normally with 4 beams like before, 
 will it break or no? and what are the default batch sizes for sentences and trie?