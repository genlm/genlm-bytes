{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":"<p>GenLM Bytes is a Python library for byte-level language modeling. It contains algorithms for turning token-level language models into byte-level language models.</p> <p>See the docs for details and basic usage.</p> <p>Note: This project is under active development \u2014 expect bugs, missing features, and breaking changes. Please report any issues or suggestions in the issue tracker.</p>"},{"location":"#quick-start","title":"Quick Start","text":"<p>This library requires python&gt;=3.11 and can be installed using pip:</p> <pre><code>pip install genlm-bytes\n</code></pre> <p>For faster and less error-prone installs, consider using <code>uv</code>:</p> <pre><code>uv pip install genlm-bytes\n</code></pre> <p>See DEVELOPING.md for details on how to install the project for development.</p>"},{"location":"#usage","title":"Usage","text":"<pre><code>from genlm.bytes import ByteBeamState, BeamParams\nfrom genlm.backend import load_model_by_name\n\n# Load a token-level language model from a huggingface model name\n# (Note: for fast GPU inference, specify `backend=\"vllm\"`)\nllm = load_model_by_name(\"gpt2-medium\")\n\n# Initialize a beam state with a maximum beam width of 5 and a prune threshold of 0.05 (higher threshold values lead to more aggressive pruning).\nbeam = await ByteBeamState.initial(llm, BeamParams(K=5, prune_threshold=0.05))\n\n# Populate the beam state with byte context.\nbeam = await beam.prefill(b\"An apple a day keeps the \")\n\n# Get the log probability distribution over the next byte.\nlogp_next = await beam.logp_next()\nlogp_next.pretty().top(5)\n# Example output:\n# b'd' -0.5766762743944795\n# b'b' -2.8732729803080233\n# b's' -2.9816068063730867\n# b'w' -3.3758250127787264\n# b'm' -3.528177345847574\n\n# Prune the beam and extend it with a new byte\nnew_beam = await (beam.prune() &lt;&lt; 100) # 100 is the byte value of 'd'\n</code></pre> <p>See basic usage for a more detailed example.</p>"},{"location":"gen_ref_pages/","title":"Gen ref pages","text":"In\u00a0[\u00a0]: Copied! <pre>from pathlib import Path\nimport shutil\nimport mkdocs_gen_files\n</pre> from pathlib import Path import shutil import mkdocs_gen_files In\u00a0[\u00a0]: Copied! <pre>readme = Path(\"README.md\")\nindex_md = Path(\"docs/index.md\")\nlogo = Path(\"assets/logo.png\")\n</pre> readme = Path(\"README.md\") index_md = Path(\"docs/index.md\") logo = Path(\"assets/logo.png\") In\u00a0[\u00a0]: Copied! <pre>def files_are_different(src: Path, dst: Path) -&gt; bool:\n    if not dst.exists():\n        return True\n    return src.read_bytes() != dst.read_bytes()\n</pre> def files_are_different(src: Path, dst: Path) -&gt; bool:     if not dst.exists():         return True     return src.read_bytes() != dst.read_bytes() In\u00a0[\u00a0]: Copied! <pre>if readme.exists() and files_are_different(readme, index_md):\n    shutil.copyfile(readme, index_md)\n</pre> if readme.exists() and files_are_different(readme, index_md):     shutil.copyfile(readme, index_md) In\u00a0[\u00a0]: Copied! <pre>if logo.exists() and files_are_different(logo, Path(\"docs/assets/logo.png\")):\n    shutil.copyfile(logo, \"docs/assets/logo.png\")\n</pre> if logo.exists() and files_are_different(logo, Path(\"docs/assets/logo.png\")):     shutil.copyfile(logo, \"docs/assets/logo.png\") In\u00a0[\u00a0]: Copied! <pre>nav = mkdocs_gen_files.Nav()\n</pre> nav = mkdocs_gen_files.Nav() In\u00a0[\u00a0]: Copied! <pre>for path in sorted(Path(\"genlm/bytes\").rglob(\"*.py\")):\n    if any(part.startswith(\".\") for part in path.parts):\n        continue\n\n    module_path = path.relative_to(\".\").with_suffix(\"\")\n    doc_path = path.relative_to(\".\").with_suffix(\".md\")\n    full_doc_path = Path(\"reference\", doc_path)\n\n    parts = tuple(module_path.parts)\n\n    if parts[-1] == \"__init__\":\n        print(f\"init, making parts {parts[:-1]}\")\n        parts = parts[:-1]\n    elif parts[-1] == \"__main__\":\n        continue\n\n    nav[parts] = doc_path.as_posix()\n\n    with mkdocs_gen_files.open(full_doc_path, \"w\") as fd:\n        ident = \".\".join(parts)\n        fd.write(f\"::: {ident}\\n\")\n        fd.write(\"    options:\\n\")\n        fd.write(\"      show_root_heading: true\\n\")\n        fd.write(\"      show_source: true\\n\")\n        fd.write(\"      heading_level: 2\\n\")\n\n    mkdocs_gen_files.set_edit_path(full_doc_path, path)\n</pre> for path in sorted(Path(\"genlm/bytes\").rglob(\"*.py\")):     if any(part.startswith(\".\") for part in path.parts):         continue      module_path = path.relative_to(\".\").with_suffix(\"\")     doc_path = path.relative_to(\".\").with_suffix(\".md\")     full_doc_path = Path(\"reference\", doc_path)      parts = tuple(module_path.parts)      if parts[-1] == \"__init__\":         print(f\"init, making parts {parts[:-1]}\")         parts = parts[:-1]     elif parts[-1] == \"__main__\":         continue      nav[parts] = doc_path.as_posix()      with mkdocs_gen_files.open(full_doc_path, \"w\") as fd:         ident = \".\".join(parts)         fd.write(f\"::: {ident}\\n\")         fd.write(\"    options:\\n\")         fd.write(\"      show_root_heading: true\\n\")         fd.write(\"      show_source: true\\n\")         fd.write(\"      heading_level: 2\\n\")      mkdocs_gen_files.set_edit_path(full_doc_path, path) In\u00a0[\u00a0]: Copied! <pre>with mkdocs_gen_files.open(\"reference/SUMMARY.md\", \"w\") as nav_file:\n    nav_file.writelines(nav.build_literate_nav())\n</pre> with mkdocs_gen_files.open(\"reference/SUMMARY.md\", \"w\") as nav_file:     nav_file.writelines(nav.build_literate_nav())"},{"location":"usage/","title":"Basic usage","text":"In\u00a0[1]: Copied! <pre>from genlm.bytes import ByteBeamState, BeamParams\nfrom genlm.backend import load_model_by_name\n</pre> from genlm.bytes import ByteBeamState, BeamParams from genlm.backend import load_model_by_name <pre>/opt/homebrew/Caskroom/miniconda/base/envs/genlm-tokenization/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n</pre> <p>First, load a token-level language model from a huggingface model name. Dependeing on whether CUDA is available, the model will be loaded using either a huggingface (CPU) or vllm (GPU) backend.</p> In\u00a0[2]: Copied! <pre>llm = load_model_by_name(\"gpt2-medium\")\n</pre> llm = load_model_by_name(\"gpt2-medium\") <pre>/opt/homebrew/Caskroom/miniconda/base/envs/genlm-tokenization/lib/python3.11/site-packages/genlm/backend/tokenization/vocab.py:98: UserWarning: Duplicate tokens found in string vocabulary. This may lead to downstream issues with the string vocabulary; we recommend using the byte vocabulary.\n  warnings.warn(\n</pre> <p>Initialize a beam state with a maximum beam width of 5.</p> In\u00a0[3]: Copied! <pre>beam = await ByteBeamState.initial(llm, BeamParams(K=5))\n</pre> beam = await ByteBeamState.initial(llm, BeamParams(K=5)) <pre>/Users/benlebrun/new-genlm/genlm-tokenization/genlm/bytes/trie.py:208: UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/SparseCsrTensorImpl.cpp:55.)\n  ).to_sparse_csr()\n</pre> <p>Populate the beam state with the context. The return value is a new beam state.</p> In\u00a0[4]: Copied! <pre>beam = await beam.prefill(b\"An apple a day keeps the \")\nbeam\n</pre> beam = await beam.prefill(b\"An apple a day keeps the \") beam Out[4]: <pre>Z: -19.598907929485275\nCandidates:\n(1.0000) -19.60: &lt;|endoftext|&gt;|An|\u2423apple|\u2423a|\u2423day|\u2423keeps|\u2423the|\u2423\n(0.0000) -31.03: &lt;|endoftext|&gt;|An|\u2423apple|\u2423a|\u2423day|\u2423keep|s|\u2423the|\u2423\n(0.0000) -36.22: &lt;|endoftext|&gt;|An|\u2423app|le|\u2423a|\u2423day|\u2423keeps|\u2423the|\u2423\n(0.0000) -36.49: &lt;|endoftext|&gt;|An|\u2423apple|\u2423a|\u2423day|\u2423keeps|\u2423th|e|\u2423\n(0.0000) -40.52: &lt;|endoftext|&gt;|An|\u2423apple|\u2423a|\u2423day|\u2423keeps|\u2423t|he|\u2423</pre> <ul> <li>Each candidate in the beam corresponds to a sequence of tokens (in purple) and a partial token (in green).</li> <li>Each candidate has an associated log weight (the negative numbers in grey), which is the log probability of the sequence of tokens and the partial token.</li> <li>The <code>Z</code> value corresponds to our estimate of the log partition function, which is the estimate of the prefix probability of the context under the language model.</li> <li>Each candidate also has an associated probability (shown on the left in green), which is the weight normalized by the partition function.</li> </ul> <p>We can use the <code>logp_next</code> method to get the (log) probability distribution over the next byte.</p> In\u00a0[5]: Copied! <pre># Get the log probability distribution over the next byte.\nlogp_next = await beam.logp_next()\nlogp_next.pretty().top(5)  # Show the top 5 most probable next bytes\n</pre> # Get the log probability distribution over the next byte. logp_next = await beam.logp_next() logp_next.pretty().top(5)  # Show the top 5 most probable next bytes Out[5]: keyvalue<pre>b'd'</pre><pre>-0.5768002911707057</pre> <pre>b'b'</pre><pre>-2.8733914084455527</pre> <pre>b's'</pre><pre>-2.981722712805219</pre> <pre>b'w'</pre><pre>-3.375940367664043</pre> <pre>b'm'</pre><pre>-3.5282914648667756</pre> <p>To advance the beam by the next byte, we first prune it to keep only the top 5 candidates, and then use the <code>&lt;&lt;</code> operator to feed in the next byte.</p> In\u00a0[6]: Copied! <pre>new_beam = await (beam.prune() &lt;&lt; 100)  # 100 is the byte value of 'd'\nnew_beam\n</pre> new_beam = await (beam.prune() &lt;&lt; 100)  # 100 is the byte value of 'd' new_beam Out[6]: <pre>Z: -20.17567801749765\nCandidates:\n(1.0000) -20.18: &lt;|endoftext|&gt;|An|\u2423apple|\u2423a|\u2423day|\u2423keeps|\u2423the|\u2423d\n(0.0000) -31.93: &lt;|endoftext|&gt;|An|\u2423apple|\u2423a|\u2423day|\u2423keep|s|\u2423the|\u2423d\n(0.0000) -38.71: &lt;|endoftext|&gt;|An|\u2423app|le|\u2423a|\u2423day|\u2423keeps|\u2423the|\u2423d\n(0.0000) -39.28: &lt;|endoftext|&gt;|An|\u2423apple|\u2423a|\u2423day|\u2423keeps|\u2423th|e|\u2423d\n(0.0000) -40.25: &lt;|endoftext|&gt;|An|\u2423apple|\u2423a|\u2423day|\u2423keeps|\u2423the|\u2423|d\n(0.0000) -43.16: &lt;|endoftext|&gt;|An|\u2423apple|\u2423a|\u2423day|\u2423keeps|\u2423t|he|\u2423d\n(0.0000) -51.34: &lt;|endoftext|&gt;|An|\u2423apple|\u2423a|\u2423day|\u2423keep|s|\u2423the|\u2423|d\n(0.0000) -54.79: &lt;|endoftext|&gt;|An|\u2423apple|\u2423a|\u2423day|\u2423keeps|\u2423th|e|\u2423|d\n(0.0000) -56.64: &lt;|endoftext|&gt;|An|\u2423app|le|\u2423a|\u2423day|\u2423keeps|\u2423the|\u2423|d\n(0.0000) -58.94: &lt;|endoftext|&gt;|An|\u2423apple|\u2423a|\u2423day|\u2423keeps|\u2423t|he|\u2423|d</pre> <p>Since extending the beam by one byte can grow the number of candidates, we can again prune it to keep only the top 5 candidates:</p> In\u00a0[7]: Copied! <pre>pruned_beam = new_beam.prune()\npruned_beam\n</pre> pruned_beam = new_beam.prune() pruned_beam Out[7]: <pre>Z: -20.175678017602173\nCandidates:\n(1.0000) -20.18: &lt;|endoftext|&gt;|An|\u2423apple|\u2423a|\u2423day|\u2423keeps|\u2423the|\u2423d\n(0.0000) -31.93: &lt;|endoftext|&gt;|An|\u2423apple|\u2423a|\u2423day|\u2423keep|s|\u2423the|\u2423d\n(0.0000) -38.71: &lt;|endoftext|&gt;|An|\u2423app|le|\u2423a|\u2423day|\u2423keeps|\u2423the|\u2423d\n(0.0000) -39.28: &lt;|endoftext|&gt;|An|\u2423apple|\u2423a|\u2423day|\u2423keeps|\u2423th|e|\u2423d\n(0.0000) -40.25: &lt;|endoftext|&gt;|An|\u2423apple|\u2423a|\u2423day|\u2423keeps|\u2423the|\u2423|d</pre> <p>We can further speed up the algorithm with more a more aggressive pruning strategy.</p> <p>In particular, <code>BeamParams</code> has a <code>prune_threshold</code> parameter which controls the minimum probability that a candidate must have to be kept in the beam. Higher values lead to more aggressive pruning, which significantly reduces the number of language model calls we need to make.</p> In\u00a0[8]: Copied! <pre>beam = await ByteBeamState.initial(llm, BeamParams(K=5, prune_threshold=0.05))\nbeam = await beam.prefill(b\"An apple a day keeps the \")\nbeam\n</pre> beam = await ByteBeamState.initial(llm, BeamParams(K=5, prune_threshold=0.05)) beam = await beam.prefill(b\"An apple a day keeps the \") beam Out[8]: <pre>Z: -19.598918914794922\nCandidates:\n(1.0000) -19.60: &lt;|endoftext|&gt;|An|\u2423apple|\u2423a|\u2423day|\u2423keeps|\u2423the|\u2423</pre> In\u00a0[9]: Copied! <pre>logp_next = await beam.logp_next()\nlogp_next.pretty().top(5)\n</pre> logp_next = await beam.logp_next() logp_next.pretty().top(5) Out[9]: keyvalue<pre>b'd'</pre><pre>-0.5766762743944795</pre> <pre>b'b'</pre><pre>-2.8732729803080233</pre> <pre>b's'</pre><pre>-2.9816068063730867</pre> <pre>b'w'</pre><pre>-3.3758250127787264</pre> <pre>b'm'</pre><pre>-3.528177345847574</pre> <p>Putting it all together, we can generate a sequence of bytes by repeatedly selecting a next byte from the log probability distribution and advancing the beam by that byte.</p> <p>One selection strategy is to always select the byte with the highest log probability, which is what <code>greedy</code> does:</p> In\u00a0[10]: Copied! <pre>beam = await ByteBeamState.initial(\n    llm, BeamParams(K=5, prune_threshold=0.05, verbose=True)\n)\nsampled = await beam.greedy(b\"An apple a day keeps the \", steps=12)\nsampled\n</pre> beam = await ByteBeamState.initial(     llm, BeamParams(K=5, prune_threshold=0.05, verbose=True) ) sampled = await beam.greedy(b\"An apple a day keeps the \", steps=12) sampled <pre>\nZ: -2.174436330795288\nCandidates:\n(1.0000) -2.17: &lt;|endoftext|&gt;|A\n\n\nZ: -4.501037198697343\nCandidates:\n(0.9977) -4.50: &lt;|endoftext|&gt;|An\n(0.0023) -10.56: &lt;|endoftext|&gt;|A|n\n\n\nZ: -5.643285751342773\nCandidates:\n(1.0000) -5.64: &lt;|endoftext|&gt;|An|\u2423\n\n\nZ: -7.201362133026123\nCandidates:\n(1.0000) -7.20: &lt;|endoftext|&gt;|An|\u2423a\n\n\nZ: -10.39808464050293\nCandidates:\n(1.0000) -10.40: &lt;|endoftext|&gt;|An|\u2423ap\n\n\nZ: -10.627063751220703\nCandidates:\n(1.0000) -10.63: &lt;|endoftext|&gt;|An|\u2423app\n\n\nZ: -12.216539396150903\nCandidates:\n(0.9934) -12.22: &lt;|endoftext|&gt;|An|\u2423appl\n(0.0066) -17.23: &lt;|endoftext|&gt;|An|\u2423app|l\n\n\nZ: -13.993473052978516\nCandidates:\n(1.0000) -13.99: &lt;|endoftext|&gt;|An|\u2423apple\n\n\nZ: -14.135584831237793\nCandidates:\n(1.0000) -14.14: &lt;|endoftext|&gt;|An|\u2423apple|\u2423\n\n\nZ: -16.980817794799805\nCandidates:\n(1.0000) -16.98: &lt;|endoftext|&gt;|An|\u2423apple|\u2423a\n\n\nZ: -17.91916847229004\nCandidates:\n(1.0000) -17.92: &lt;|endoftext|&gt;|An|\u2423apple|\u2423a|\u2423\n\n\nZ: -18.107898712158203\nCandidates:\n(1.0000) -18.11: &lt;|endoftext|&gt;|An|\u2423apple|\u2423a|\u2423d\n\n\nZ: -18.110923767089844\nCandidates:\n(1.0000) -18.11: &lt;|endoftext|&gt;|An|\u2423apple|\u2423a|\u2423da\n\n\nZ: -18.111148834228516\nCandidates:\n(1.0000) -18.11: &lt;|endoftext|&gt;|An|\u2423apple|\u2423a|\u2423day\n\n\nZ: -18.135374069213867\nCandidates:\n(1.0000) -18.14: &lt;|endoftext|&gt;|An|\u2423apple|\u2423a|\u2423day|\u2423\n\n\nZ: -18.454233169555664\nCandidates:\n(1.0000) -18.45: &lt;|endoftext|&gt;|An|\u2423apple|\u2423a|\u2423day|\u2423k\n\n\nZ: -18.4615478515625\nCandidates:\n(1.0000) -18.46: &lt;|endoftext|&gt;|An|\u2423apple|\u2423a|\u2423day|\u2423ke\n\n\nZ: -18.469377517700195\nCandidates:\n(1.0000) -18.47: &lt;|endoftext|&gt;|An|\u2423apple|\u2423a|\u2423day|\u2423kee\n\n\nZ: -18.469377517700195\nCandidates:\n(1.0000) -18.47: &lt;|endoftext|&gt;|An|\u2423apple|\u2423a|\u2423day|\u2423keep\n\n\nZ: -18.469871520996094\nCandidates:\n(1.0000) -18.47: &lt;|endoftext|&gt;|An|\u2423apple|\u2423a|\u2423day|\u2423keeps\n\n\nZ: -18.472919464111328\nCandidates:\n(1.0000) -18.47: &lt;|endoftext|&gt;|An|\u2423apple|\u2423a|\u2423day|\u2423keeps|\u2423\n\n\nZ: -19.51340675354004\nCandidates:\n(1.0000) -19.51: &lt;|endoftext|&gt;|An|\u2423apple|\u2423a|\u2423day|\u2423keeps|\u2423t\n\n\nZ: -19.53944206237793\nCandidates:\n(1.0000) -19.54: &lt;|endoftext|&gt;|An|\u2423apple|\u2423a|\u2423day|\u2423keeps|\u2423th\n\n\nZ: -19.567386627197266\nCandidates:\n(1.0000) -19.57: &lt;|endoftext|&gt;|An|\u2423apple|\u2423a|\u2423day|\u2423keeps|\u2423the\n\n\nZ: -19.598918914794922\nCandidates:\n(1.0000) -19.60: &lt;|endoftext|&gt;|An|\u2423apple|\u2423a|\u2423day|\u2423keeps|\u2423the|\u2423\n\n\nZ: -20.17568588256836\nCandidates:\n(1.0000) -20.18: &lt;|endoftext|&gt;|An|\u2423apple|\u2423a|\u2423day|\u2423keeps|\u2423the|\u2423d\n\n\nZ: -20.21005630493164\nCandidates:\n(1.0000) -20.21: &lt;|endoftext|&gt;|An|\u2423apple|\u2423a|\u2423day|\u2423keeps|\u2423the|\u2423do\n\n\nZ: -20.214828491210938\nCandidates:\n(1.0000) -20.21: &lt;|endoftext|&gt;|An|\u2423apple|\u2423a|\u2423day|\u2423keeps|\u2423the|\u2423doc\n\n\nZ: -20.21491241455078\nCandidates:\n(1.0000) -20.21: &lt;|endoftext|&gt;|An|\u2423apple|\u2423a|\u2423day|\u2423keeps|\u2423the|\u2423doct\n\n\nZ: -20.214920043945312\nCandidates:\n(1.0000) -20.21: &lt;|endoftext|&gt;|An|\u2423apple|\u2423a|\u2423day|\u2423keeps|\u2423the|\u2423docto\n\n\nZ: -20.214920043945312\nCandidates:\n(1.0000) -20.21: &lt;|endoftext|&gt;|An|\u2423apple|\u2423a|\u2423day|\u2423keeps|\u2423the|\u2423doctor\n\n\nZ: -20.248199462890625\nCandidates:\n(1.0000) -20.25: &lt;|endoftext|&gt;|An|\u2423apple|\u2423a|\u2423day|\u2423keeps|\u2423the|\u2423doctor|\u2423\n\n\nZ: -20.292394638061523\nCandidates:\n(1.0000) -20.29: &lt;|endoftext|&gt;|An|\u2423apple|\u2423a|\u2423day|\u2423keeps|\u2423the|\u2423doctor|\u2423a\n\n\nZ: -20.30491065979004\nCandidates:\n(1.0000) -20.30: &lt;|endoftext|&gt;|An|\u2423apple|\u2423a|\u2423day|\u2423keeps|\u2423the|\u2423doctor|\u2423aw\n\n\nZ: -20.304912567138672\nCandidates:\n(1.0000) -20.30: &lt;|endoftext|&gt;|An|\u2423apple|\u2423a|\u2423day|\u2423keeps|\u2423the|\u2423doctor|\u2423awa\n\n\nZ: -20.30513572692871\nCandidates:\n(1.0000) -20.31: &lt;|endoftext|&gt;|An|\u2423apple|\u2423a|\u2423day|\u2423keeps|\u2423the|\u2423doctor|\u2423away\n\n\nZ: -21.068042755126953\nCandidates:\n(1.0000) -21.07: &lt;|endoftext|&gt;|An|\u2423apple|\u2423a|\u2423day|\u2423keeps|\u2423the|\u2423doctor|\u2423away|.\n\n</pre> Out[10]: <pre>b'An apple a day keeps the doctor away.'</pre> <p>We can also sample from the log probability distribution over the next byte:</p> In\u00a0[11]: Copied! <pre>beam = await ByteBeamState.initial(\n    llm, BeamParams(K=5, prune_threshold=0.05, verbose=True)\n)\nsampled = await beam.sample(b\"An apple a day keeps the \", steps=12)\nsampled\n</pre> beam = await ByteBeamState.initial(     llm, BeamParams(K=5, prune_threshold=0.05, verbose=True) ) sampled = await beam.sample(b\"An apple a day keeps the \", steps=12) sampled <pre>\nZ: -2.174436330795288\nCandidates:\n(1.0000) -2.17: &lt;|endoftext|&gt;|A\n\n\nZ: -4.501037198697343\nCandidates:\n(0.9977) -4.50: &lt;|endoftext|&gt;|An\n(0.0023) -10.56: &lt;|endoftext|&gt;|A|n\n\n\nZ: -5.643285751342773\nCandidates:\n(1.0000) -5.64: &lt;|endoftext|&gt;|An|\u2423\n\n\nZ: -7.201362133026123\nCandidates:\n(1.0000) -7.20: &lt;|endoftext|&gt;|An|\u2423a\n\n\nZ: -10.39808464050293\nCandidates:\n(1.0000) -10.40: &lt;|endoftext|&gt;|An|\u2423ap\n\n\nZ: -10.627063751220703\nCandidates:\n(1.0000) -10.63: &lt;|endoftext|&gt;|An|\u2423app\n\n\nZ: -12.216539396150903\nCandidates:\n(0.9934) -12.22: &lt;|endoftext|&gt;|An|\u2423appl\n(0.0066) -17.23: &lt;|endoftext|&gt;|An|\u2423app|l\n\n\nZ: -13.993473052978516\nCandidates:\n(1.0000) -13.99: &lt;|endoftext|&gt;|An|\u2423apple\n\n\nZ: -14.135584831237793\nCandidates:\n(1.0000) -14.14: &lt;|endoftext|&gt;|An|\u2423apple|\u2423\n\n\nZ: -16.980817794799805\nCandidates:\n(1.0000) -16.98: &lt;|endoftext|&gt;|An|\u2423apple|\u2423a\n\n\nZ: -17.91916847229004\nCandidates:\n(1.0000) -17.92: &lt;|endoftext|&gt;|An|\u2423apple|\u2423a|\u2423\n\n\nZ: -18.107898712158203\nCandidates:\n(1.0000) -18.11: &lt;|endoftext|&gt;|An|\u2423apple|\u2423a|\u2423d\n\n\nZ: -18.110923767089844\nCandidates:\n(1.0000) -18.11: &lt;|endoftext|&gt;|An|\u2423apple|\u2423a|\u2423da\n\n\nZ: -18.111148834228516\nCandidates:\n(1.0000) -18.11: &lt;|endoftext|&gt;|An|\u2423apple|\u2423a|\u2423day\n\n\nZ: -18.135374069213867\nCandidates:\n(1.0000) -18.14: &lt;|endoftext|&gt;|An|\u2423apple|\u2423a|\u2423day|\u2423\n\n\nZ: -18.454233169555664\nCandidates:\n(1.0000) -18.45: &lt;|endoftext|&gt;|An|\u2423apple|\u2423a|\u2423day|\u2423k\n\n\nZ: -18.4615478515625\nCandidates:\n(1.0000) -18.46: &lt;|endoftext|&gt;|An|\u2423apple|\u2423a|\u2423day|\u2423ke\n\n\nZ: -18.469377517700195\nCandidates:\n(1.0000) -18.47: &lt;|endoftext|&gt;|An|\u2423apple|\u2423a|\u2423day|\u2423kee\n\n\nZ: -18.469377517700195\nCandidates:\n(1.0000) -18.47: &lt;|endoftext|&gt;|An|\u2423apple|\u2423a|\u2423day|\u2423keep\n\n\nZ: -18.469871520996094\nCandidates:\n(1.0000) -18.47: &lt;|endoftext|&gt;|An|\u2423apple|\u2423a|\u2423day|\u2423keeps\n\n\nZ: -18.472919464111328\nCandidates:\n(1.0000) -18.47: &lt;|endoftext|&gt;|An|\u2423apple|\u2423a|\u2423day|\u2423keeps|\u2423\n\n\nZ: -19.51340675354004\nCandidates:\n(1.0000) -19.51: &lt;|endoftext|&gt;|An|\u2423apple|\u2423a|\u2423day|\u2423keeps|\u2423t\n\n\nZ: -19.53944206237793\nCandidates:\n(1.0000) -19.54: &lt;|endoftext|&gt;|An|\u2423apple|\u2423a|\u2423day|\u2423keeps|\u2423th\n\n\nZ: -19.567386627197266\nCandidates:\n(1.0000) -19.57: &lt;|endoftext|&gt;|An|\u2423apple|\u2423a|\u2423day|\u2423keeps|\u2423the\n\n\nZ: -19.598918914794922\nCandidates:\n(1.0000) -19.60: &lt;|endoftext|&gt;|An|\u2423apple|\u2423a|\u2423day|\u2423keeps|\u2423the|\u2423\n\n\nZ: -24.765562057495117\nCandidates:\n(1.0000) -24.77: &lt;|endoftext|&gt;|An|\u2423apple|\u2423a|\u2423day|\u2423keeps|\u2423the|\u2423D\n\n\nZ: -27.6703574912875\nCandidates:\n(0.9409) -27.73: &lt;|endoftext|&gt;|An|\u2423apple|\u2423a|\u2423day|\u2423keeps|\u2423the|\u2423Du\n(0.0591) -30.50: &lt;|endoftext|&gt;|An|\u2423apple|\u2423a|\u2423day|\u2423keeps|\u2423the|\u2423D|u\n\n\nZ: -28.567639703725348\nCandidates:\n(0.9906) -28.58: &lt;|endoftext|&gt;|An|\u2423apple|\u2423a|\u2423day|\u2423keeps|\u2423the|\u2423Dut\n(0.0094) -33.24: &lt;|endoftext|&gt;|An|\u2423apple|\u2423a|\u2423day|\u2423keeps|\u2423the|\u2423D|ut\n\n\nZ: -28.577476501464844\nCandidates:\n(1.0000) -28.58: &lt;|endoftext|&gt;|An|\u2423apple|\u2423a|\u2423day|\u2423keeps|\u2423the|\u2423Dutc\n\n\nZ: -28.577476501464844\nCandidates:\n(1.0000) -28.58: &lt;|endoftext|&gt;|An|\u2423apple|\u2423a|\u2423day|\u2423keeps|\u2423the|\u2423Dutch\n\n\nZ: -28.774438858032227\nCandidates:\n(1.0000) -28.77: &lt;|endoftext|&gt;|An|\u2423apple|\u2423a|\u2423day|\u2423keeps|\u2423the|\u2423Dutch|\u2423\n\n\nZ: -31.404659271240234\nCandidates:\n(1.0000) -31.40: &lt;|endoftext|&gt;|An|\u2423apple|\u2423a|\u2423day|\u2423keeps|\u2423the|\u2423Dutch|\u2423a\n\n\nZ: -32.12074279785156\nCandidates:\n(1.0000) -32.12: &lt;|endoftext|&gt;|An|\u2423apple|\u2423a|\u2423day|\u2423keeps|\u2423the|\u2423Dutch|\u2423aw\n\n\nZ: -32.12383270263672\nCandidates:\n(1.0000) -32.12: &lt;|endoftext|&gt;|An|\u2423apple|\u2423a|\u2423day|\u2423keeps|\u2423the|\u2423Dutch|\u2423awa\n\n\nZ: -32.18830490112305\nCandidates:\n(1.0000) -32.19: &lt;|endoftext|&gt;|An|\u2423apple|\u2423a|\u2423day|\u2423keeps|\u2423the|\u2423Dutch|\u2423away\n\n\nZ: -33.56272506713867\nCandidates:\n(1.0000) -33.56: &lt;|endoftext|&gt;|An|\u2423apple|\u2423a|\u2423day|\u2423keeps|\u2423the|\u2423Dutch|\u2423away|.\n\n\nZ: -34.75253677368164\nCandidates:\n(1.0000) -34.75: &lt;|endoftext|&gt;|An|\u2423apple|\u2423a|\u2423day|\u2423keeps|\u2423the|\u2423Dutch|\u2423away|.|\\n\n\n</pre> Out[11]: <pre>b'An apple a day keeps the Dutch away.\\n'</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"usage/#basic-usage","title":"Basic usage\u00b6","text":"<p>This example shows how to use the <code>genlm-bytes</code> library for byte-level language modeling.</p>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>genlm<ul> <li>bytes<ul> <li>byte_lm<ul> <li>beam</li> <li>heal</li> <li>lm_state</li> <li>trie_state</li> </ul> </li> <li>trie</li> <li>util</li> </ul> </li> </ul> </li> </ul>"},{"location":"reference/genlm/bytes/__init__/","title":"bytes","text":""},{"location":"reference/genlm/bytes/__init__/#genlm.bytes","title":"<code>genlm.bytes</code>","text":""},{"location":"reference/genlm/bytes/__init__/#genlm.bytes.ByteBeamState","title":"<code>ByteBeamState</code>","text":"<p>               Bases: <code>StatefulByteLM</code></p> <p>Represents the state of the beam during byte-level language modeling.</p> <p>Tracks multiple candidate states and their probabilities, pruning low-probability candidates.</p> <p>Parameters:</p> Name Type Description Default <code>states</code> <code>list[LazyTrieState]</code> <p>List of candidate states to track</p> required <code>params</code> <code>BeamParams</code> <p>Parameters controlling beam search behavior</p> required Source code in <code>genlm/bytes/byte_lm/beam.py</code> <pre><code>class ByteBeamState(StatefulByteLM):\n    \"\"\"Represents the state of the beam during byte-level language modeling.\n\n    Tracks multiple candidate states and their probabilities, pruning low-probability\n    candidates.\n\n    Args:\n        states (list[LazyTrieState]): List of candidate states to track\n        params (BeamParams): Parameters controlling beam search behavior\n    \"\"\"\n\n    def __init__(self, states, params):\n        self.states = sorted(states, key=lambda b: -b.weight)\n        self.params = params\n\n    @classmethod\n    async def initial(cls, llm, params, trie_opts=None):\n        \"\"\"Creates initial beam state.\n\n        Args:\n            llm (StatefulTokenizedLM): Token-level language model to use.\n            params (BeamParams): Beam search parameters.\n            trie_opts (dict, optional): Additional keyword arguments passed to\n                AsyncTokenByteTrie.from_vocab. For example, {\"max_batch_size\": 100}.\n\n        Returns:\n            (ByteBeamState): Initial beam state.\n        \"\"\"\n        # Handle EOS tokens\n        trie_opts = trie_opts or {}\n        trie_opts[\"eos_tokens\"] = params.eos_tokens\n\n        async_trie = AsyncTokenByteTrie.from_vocab(\n            get_byte_vocab(llm.tokenizer), **trie_opts\n        )\n        state = LazyTrieState.initial(llm, async_trie, mode=TrieMode.WITH_EOS)\n        return cls([await state.materialize()], params)\n\n    def __iter__(self):\n        return iter(self.states)\n\n    def __len__(self):\n        return len(self.states)\n\n    @cached_property\n    def logZ(self):\n        \"\"\"Estimate of the partition function (sum of weights) for current beam.\n        This is the estimate of the prefix probability of the bytes consumed so far.\n        \"\"\"\n        return logsumexp([state.weight for state in self])\n\n    async def __lshift__(self, a):\n        \"\"\"Advances the beam state with a new byte.\n\n        Args:\n            a (int): Byte to add to states.\n\n        Returns:\n            (ByteBeamState): New beam state after processing the byte.\n        \"\"\"\n        new_states = []\n        for state in self:\n            if new_state := state &lt;&lt; a:\n                new_states.append(new_state)\n\n        logZ = logsumexp([s.weight for s in new_states]) if new_states else -np.inf\n        for state in await self.extend(logZ):\n            if new_state := state &lt;&lt; a:\n                new_states.append(new_state)\n\n        new_state = ByteBeamState(new_states, self.params)\n\n        # If advancing would empty the beam, do adaptive healing if enabled\n        if self.params.heal and len(new_state) == 0:\n            healed = await self._adaptive_heal(a)\n            if healed is not None:\n                if self.params.verbose:\n                    print(\"[heal] Applied adaptive token healing\")\n                return healed\n\n        if self.params.verbose:\n            print()\n            print(new_state)\n\n        return new_state\n\n    async def logp_next(self):\n        \"\"\"Computes log probabilities for the next byte across all beam candidates.\n\n        Returns:\n            (LazyByteProbs): Log probabilities for next possible bytes.\n        \"\"\"\n        assert len(self) &gt; 0, \"Beam is empty\"\n\n        logqs = []\n        for state in self:\n            logqs.append(state.logp_next.ps + state.weight)\n\n        for state in await self.extend(self.logZ):\n            logqs.append(state.logp_next.ps + state.weight)\n\n        logqs = np.stack(logqs, axis=0)  # shape: (num_states, array_size)\n        # mask EOT positions of non-extended (EOT is at index 256)\n        logqs[: len(self), -2] = -np.inf\n        logps = scipy_logsumexp(logqs, axis=0)\n\n        return LazyByteProbs(logps - logsumexp(logps))\n\n    async def extend(self, logZ):\n        \"\"\"Attempts to advance each candidate in the beam by a token (EOT).\n\n        For each candididate with EOT available, this ends the current token and\n        starts a new one in preparation for the next byte.\n\n        Args:\n            logZ (float): Current estimated of the partition function for pruning\n\n        Returns:\n            (list[LazyTrieState]): New candidate states after extension\n        \"\"\"\n        extends = []\n        for state in self:\n            if new_state := state.extend():\n                logZ = np.logaddexp(logZ, new_state.weight)\n                extends.append(new_state)\n\n        coros = []\n        for state in extends:\n            if state.weight - logZ &gt; self.params.log_prune_threshold:\n                coros.append(state.materialize())\n\n        return await asyncio.gather(*coros)\n\n    def prune(self):\n        \"\"\"Prunes beam to maintain beam width and probability threshold.\n\n        Returns:\n            (ByteBeamState): New state with pruned candidates.\n        \"\"\"\n        new_states = [\n            state\n            for state in self\n            if state.weight - self.logZ &gt; self.params.log_prune_threshold\n        ][: self.params.K]\n        return ByteBeamState(new_states, self.params)\n\n    def __repr__(self):\n        desc = colors.bold % f\"Z: {self.logZ}\\n\" + colors.bold % \"Candidates:\\n\"\n        for state in self:\n            P = np.exp(state.weight - self.logZ)\n            color = colors.green if P &gt; self.params.prune_threshold else colors.red\n            desc += f\"({color % f'{P:.4f}'}) {repr(state)}\\n\"\n        return desc\n\n    def with_mode(self, mode):\n        \"\"\"Create a new beam state with specified trie mode.\n\n        Args:\n            mode (TrieMode): Trie mode for the new beam state\n\n        Returns:\n            (ByteBeamState): New beam state with updated mode\n        \"\"\"\n        return ByteBeamState(\n            states=[state.with_mode(mode) for state in self.states],\n            params=self.params,\n        )\n\n    async def prefill(self, bs):\n        \"\"\"Prefill the beam on a sequence of bytes.\n\n        During prefilling, EOS tokens are treated as normal tokens and don't cause termination.\n\n        Args:\n            bs (bytes): Byte sequence to prefill on\n\n        Returns:\n            (ByteBeamState): New beam state after prefilling\n        \"\"\"\n        # Create no_eos beam for prefill (EOS tokens treated as normal)\n        no_eos_beam = self.with_mode(TrieMode.WITHOUT_EOS)\n\n        # Do prefill operations on no_eos beam\n        for b in bs:\n            no_eos_beam = await (no_eos_beam.prune() &lt;&lt; b)\n\n        # Return as with_eos beam (EOS tokens get special handling after prefill)\n        return no_eos_beam.with_mode(TrieMode.WITH_EOS)\n\n    async def cleanup(self):\n        \"\"\"Cleans up resources used by the candidates.\"\"\"\n        await asyncio.gather(*[state.cleanup() for state in self])\n\n    async def _adaptive_heal(self, next_byte: int):\n        \"\"\"Attempt adaptive token healing using TokenHealer.\n\n        Returns a new beam advanced by `next_byte` if healing succeeds, else None.\n        \"\"\"\n        healer = TokenHealer(\n            max_backoff=self.params.heal_max_backoff,\n            max_splits=self.params.heal_max_splits,\n            verbose=self.params.verbose,\n        )\n\n        for state in self.states:\n            healed_state = await healer.try_heal(state, next_byte)\n            if healed_state is not None:\n                return ByteBeamState([healed_state], self.params)\n\n        return None\n</code></pre>"},{"location":"reference/genlm/bytes/__init__/#genlm.bytes.ByteBeamState.initial","title":"<code>initial(llm, params, trie_opts=None)</code>  <code>async</code> <code>classmethod</code>","text":"<p>Creates initial beam state.</p> <p>Parameters:</p> Name Type Description Default <code>llm</code> <code>StatefulTokenizedLM</code> <p>Token-level language model to use.</p> required <code>params</code> <code>BeamParams</code> <p>Beam search parameters.</p> required <code>trie_opts</code> <code>dict</code> <p>Additional keyword arguments passed to AsyncTokenByteTrie.from_vocab. For example, {\"max_batch_size\": 100}.</p> <code>None</code> <p>Returns:</p> Type Description <code>ByteBeamState</code> <p>Initial beam state.</p> Source code in <code>genlm/bytes/byte_lm/beam.py</code> <pre><code>@classmethod\nasync def initial(cls, llm, params, trie_opts=None):\n    \"\"\"Creates initial beam state.\n\n    Args:\n        llm (StatefulTokenizedLM): Token-level language model to use.\n        params (BeamParams): Beam search parameters.\n        trie_opts (dict, optional): Additional keyword arguments passed to\n            AsyncTokenByteTrie.from_vocab. For example, {\"max_batch_size\": 100}.\n\n    Returns:\n        (ByteBeamState): Initial beam state.\n    \"\"\"\n    # Handle EOS tokens\n    trie_opts = trie_opts or {}\n    trie_opts[\"eos_tokens\"] = params.eos_tokens\n\n    async_trie = AsyncTokenByteTrie.from_vocab(\n        get_byte_vocab(llm.tokenizer), **trie_opts\n    )\n    state = LazyTrieState.initial(llm, async_trie, mode=TrieMode.WITH_EOS)\n    return cls([await state.materialize()], params)\n</code></pre>"},{"location":"reference/genlm/bytes/__init__/#genlm.bytes.ByteBeamState.logZ","title":"<code>logZ</code>  <code>cached</code> <code>property</code>","text":"<p>Estimate of the partition function (sum of weights) for current beam. This is the estimate of the prefix probability of the bytes consumed so far.</p>"},{"location":"reference/genlm/bytes/__init__/#genlm.bytes.ByteBeamState.__lshift__","title":"<code>__lshift__(a)</code>  <code>async</code>","text":"<p>Advances the beam state with a new byte.</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>int</code> <p>Byte to add to states.</p> required <p>Returns:</p> Type Description <code>ByteBeamState</code> <p>New beam state after processing the byte.</p> Source code in <code>genlm/bytes/byte_lm/beam.py</code> <pre><code>async def __lshift__(self, a):\n    \"\"\"Advances the beam state with a new byte.\n\n    Args:\n        a (int): Byte to add to states.\n\n    Returns:\n        (ByteBeamState): New beam state after processing the byte.\n    \"\"\"\n    new_states = []\n    for state in self:\n        if new_state := state &lt;&lt; a:\n            new_states.append(new_state)\n\n    logZ = logsumexp([s.weight for s in new_states]) if new_states else -np.inf\n    for state in await self.extend(logZ):\n        if new_state := state &lt;&lt; a:\n            new_states.append(new_state)\n\n    new_state = ByteBeamState(new_states, self.params)\n\n    # If advancing would empty the beam, do adaptive healing if enabled\n    if self.params.heal and len(new_state) == 0:\n        healed = await self._adaptive_heal(a)\n        if healed is not None:\n            if self.params.verbose:\n                print(\"[heal] Applied adaptive token healing\")\n            return healed\n\n    if self.params.verbose:\n        print()\n        print(new_state)\n\n    return new_state\n</code></pre>"},{"location":"reference/genlm/bytes/__init__/#genlm.bytes.ByteBeamState.logp_next","title":"<code>logp_next()</code>  <code>async</code>","text":"<p>Computes log probabilities for the next byte across all beam candidates.</p> <p>Returns:</p> Type Description <code>LazyByteProbs</code> <p>Log probabilities for next possible bytes.</p> Source code in <code>genlm/bytes/byte_lm/beam.py</code> <pre><code>async def logp_next(self):\n    \"\"\"Computes log probabilities for the next byte across all beam candidates.\n\n    Returns:\n        (LazyByteProbs): Log probabilities for next possible bytes.\n    \"\"\"\n    assert len(self) &gt; 0, \"Beam is empty\"\n\n    logqs = []\n    for state in self:\n        logqs.append(state.logp_next.ps + state.weight)\n\n    for state in await self.extend(self.logZ):\n        logqs.append(state.logp_next.ps + state.weight)\n\n    logqs = np.stack(logqs, axis=0)  # shape: (num_states, array_size)\n    # mask EOT positions of non-extended (EOT is at index 256)\n    logqs[: len(self), -2] = -np.inf\n    logps = scipy_logsumexp(logqs, axis=0)\n\n    return LazyByteProbs(logps - logsumexp(logps))\n</code></pre>"},{"location":"reference/genlm/bytes/__init__/#genlm.bytes.ByteBeamState.extend","title":"<code>extend(logZ)</code>  <code>async</code>","text":"<p>Attempts to advance each candidate in the beam by a token (EOT).</p> <p>For each candididate with EOT available, this ends the current token and starts a new one in preparation for the next byte.</p> <p>Parameters:</p> Name Type Description Default <code>logZ</code> <code>float</code> <p>Current estimated of the partition function for pruning</p> required <p>Returns:</p> Type Description <code>list[LazyTrieState]</code> <p>New candidate states after extension</p> Source code in <code>genlm/bytes/byte_lm/beam.py</code> <pre><code>async def extend(self, logZ):\n    \"\"\"Attempts to advance each candidate in the beam by a token (EOT).\n\n    For each candididate with EOT available, this ends the current token and\n    starts a new one in preparation for the next byte.\n\n    Args:\n        logZ (float): Current estimated of the partition function for pruning\n\n    Returns:\n        (list[LazyTrieState]): New candidate states after extension\n    \"\"\"\n    extends = []\n    for state in self:\n        if new_state := state.extend():\n            logZ = np.logaddexp(logZ, new_state.weight)\n            extends.append(new_state)\n\n    coros = []\n    for state in extends:\n        if state.weight - logZ &gt; self.params.log_prune_threshold:\n            coros.append(state.materialize())\n\n    return await asyncio.gather(*coros)\n</code></pre>"},{"location":"reference/genlm/bytes/__init__/#genlm.bytes.ByteBeamState.prune","title":"<code>prune()</code>","text":"<p>Prunes beam to maintain beam width and probability threshold.</p> <p>Returns:</p> Type Description <code>ByteBeamState</code> <p>New state with pruned candidates.</p> Source code in <code>genlm/bytes/byte_lm/beam.py</code> <pre><code>def prune(self):\n    \"\"\"Prunes beam to maintain beam width and probability threshold.\n\n    Returns:\n        (ByteBeamState): New state with pruned candidates.\n    \"\"\"\n    new_states = [\n        state\n        for state in self\n        if state.weight - self.logZ &gt; self.params.log_prune_threshold\n    ][: self.params.K]\n    return ByteBeamState(new_states, self.params)\n</code></pre>"},{"location":"reference/genlm/bytes/__init__/#genlm.bytes.ByteBeamState.with_mode","title":"<code>with_mode(mode)</code>","text":"<p>Create a new beam state with specified trie mode.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>TrieMode</code> <p>Trie mode for the new beam state</p> required <p>Returns:</p> Type Description <code>ByteBeamState</code> <p>New beam state with updated mode</p> Source code in <code>genlm/bytes/byte_lm/beam.py</code> <pre><code>def with_mode(self, mode):\n    \"\"\"Create a new beam state with specified trie mode.\n\n    Args:\n        mode (TrieMode): Trie mode for the new beam state\n\n    Returns:\n        (ByteBeamState): New beam state with updated mode\n    \"\"\"\n    return ByteBeamState(\n        states=[state.with_mode(mode) for state in self.states],\n        params=self.params,\n    )\n</code></pre>"},{"location":"reference/genlm/bytes/__init__/#genlm.bytes.ByteBeamState.prefill","title":"<code>prefill(bs)</code>  <code>async</code>","text":"<p>Prefill the beam on a sequence of bytes.</p> <p>During prefilling, EOS tokens are treated as normal tokens and don't cause termination.</p> <p>Parameters:</p> Name Type Description Default <code>bs</code> <code>bytes</code> <p>Byte sequence to prefill on</p> required <p>Returns:</p> Type Description <code>ByteBeamState</code> <p>New beam state after prefilling</p> Source code in <code>genlm/bytes/byte_lm/beam.py</code> <pre><code>async def prefill(self, bs):\n    \"\"\"Prefill the beam on a sequence of bytes.\n\n    During prefilling, EOS tokens are treated as normal tokens and don't cause termination.\n\n    Args:\n        bs (bytes): Byte sequence to prefill on\n\n    Returns:\n        (ByteBeamState): New beam state after prefilling\n    \"\"\"\n    # Create no_eos beam for prefill (EOS tokens treated as normal)\n    no_eos_beam = self.with_mode(TrieMode.WITHOUT_EOS)\n\n    # Do prefill operations on no_eos beam\n    for b in bs:\n        no_eos_beam = await (no_eos_beam.prune() &lt;&lt; b)\n\n    # Return as with_eos beam (EOS tokens get special handling after prefill)\n    return no_eos_beam.with_mode(TrieMode.WITH_EOS)\n</code></pre>"},{"location":"reference/genlm/bytes/__init__/#genlm.bytes.ByteBeamState.cleanup","title":"<code>cleanup()</code>  <code>async</code>","text":"<p>Cleans up resources used by the candidates.</p> Source code in <code>genlm/bytes/byte_lm/beam.py</code> <pre><code>async def cleanup(self):\n    \"\"\"Cleans up resources used by the candidates.\"\"\"\n    await asyncio.gather(*[state.cleanup() for state in self])\n</code></pre>"},{"location":"reference/genlm/bytes/__init__/#genlm.bytes.LazyTrieState","title":"<code>LazyTrieState</code>","text":"<p>A lazy-evaluated state of a TokenByteTrie traversal.</p> <p>This class maintains the state of a language model while traversing a trie structure, lazily evaluating probabilities and maintaining the weight of the current path through the trie for beam search.</p> <p>Parameters:</p> Name Type Description Default <code>lm_state</code> <code>StatefulTokenizedLM</code> <p>Current language model state</p> required <code>trie</code> <code>TokenByteTrie</code> <p>Trie structure mapping tokens to byte sequences</p> required <code>node</code> <code>int</code> <p>Current node in the trie</p> required <code>weight</code> <code>float</code> <p>Cumulative log probability of the path to this node</p> required <code>mass</code> <code>ndarray</code> <p>Masses for each node in the trie for the current state</p> <code>None</code> <code>mode</code> <code>TrieMode</code> <p>Trie mode to use</p> <code>WITH_EOS</code> <code>terminated</code> <code>bool</code> <p>Whether the state is terminated (EOS has been consumed)</p> <code>False</code> Source code in <code>genlm/bytes/byte_lm/trie_state.py</code> <pre><code>class LazyTrieState:\n    \"\"\"A lazy-evaluated state of a TokenByteTrie traversal.\n\n    This class maintains the state of a language model while traversing a trie structure,\n    lazily evaluating probabilities and maintaining the weight of the current path through the trie\n    for beam search.\n\n    Args:\n        lm_state (StatefulTokenizedLM): Current language model state\n        trie (TokenByteTrie): Trie structure mapping tokens to byte sequences\n        node (int): Current node in the trie\n        weight (float): Cumulative log probability of the path to this node\n        mass (numpy.ndarray, optional): Masses for each node in the trie for the current state\n        mode (TrieMode): Trie mode to use\n        terminated (bool): Whether the state is terminated (EOS has been consumed)\n    \"\"\"\n\n    def __init__(\n        self,\n        lm_state,\n        trie,\n        node,\n        weight,\n        mass=None,\n        mode=TrieMode.WITH_EOS,\n        terminated=False,\n    ):\n        self.lm_state = lm_state\n        self.trie = trie\n        self.node = node\n        self.weight = weight\n        self._mass = mass\n        self._extend = None\n        self.mode = mode\n        self.root = self.trie.trie.root\n        self.children = self.trie.trie.children\n        self.terminated = terminated\n\n    @classmethod\n    def initial(cls, lm, trie, mode=TrieMode.WITH_EOS):\n        \"\"\"Creates an initial trie state.\n\n        Args:\n            lm (genlm.backend.AsyncLM): Language model to use\n            trie (TokenByteTrie): TokenByteTrie structure for byte-to-token mapping\n            mode (TrieMode): Trie mode to use\n\n        Returns:\n            (LazyTrieState): Initial state at root of trie with weight 0.0\n        \"\"\"\n        return cls(\n            trie=trie,\n            node=trie.trie.root,\n            lm_state=StatefulTokenizedLM.initial(lm),\n            weight=0.0,\n            mode=mode,\n        )\n\n    @property\n    def partial(self):\n        \"\"\"Returns the byte sequence corresponding to the current node in the trie.\"\"\"\n        return self.trie.trie.node2prefix[self.node]\n\n    @property\n    def mass(self):\n        \"\"\"Returns the log mass for each node in the trie.\n\n        The mass at a node corresponds to the sum of the probabilities of all\n        tokens which share the prefix (`self.partial`) represented by that node.\n\n        Raises:\n            ValueError: If state hasn't been materialized yet\n        \"\"\"\n        if self._mass is None:\n            raise ValueError(\"State is not yet materialized.\")\n        return self._mass\n\n    def with_mode(self, mode):\n        \"\"\"Returns a new state with the given mode.\"\"\"\n        return LazyTrieState(\n            lm_state=self.lm_state,\n            trie=self.trie,\n            node=self.node,\n            weight=self.weight,\n            mass=self._mass,\n            mode=mode,\n            terminated=self.terminated,\n        )\n\n    def actions(self):\n        \"\"\"Returns possible byte transitions from current node.\"\"\"\n        return self.children[self.node]\n\n    def get_EOT(self):\n        \"\"\"Returns the end-of-token node if available from current position in the trie.\"\"\"\n        return self.children[self.node].get(self.trie.trie.eot_token)\n\n    def __lshift__(self, b):\n        \"\"\"Transitions to a new state by consuming a byte.\n\n        Args:\n            b (int): Byte to consume\n\n        Returns:\n            (LazyTrieState|None): New state after consuming byte, or None if transition invalid (terminated or EOS)\n        \"\"\"\n        if self.terminated:\n            return None\n\n        if node := self.children[self.node].get(b):\n            mass = self.mass\n            return LazyTrieState(\n                lm_state=self.lm_state,\n                trie=self.trie,\n                mass=mass,\n                node=node,\n                weight=self.weight + mass[node] - mass[self.node],\n                mode=self.mode,\n                terminated=b == EOS,\n            )\n\n    def extend(self):\n        \"\"\"Extends current state by consuming an end-of-token if possible.\n\n        Returns:\n            (LazyTrieState|None): New state after consuming EOT, or None if not possible\n        \"\"\"\n        if self._extend is None:\n            if (eot_node := self.get_EOT()) is not None:\n                mass = self.mass\n                self._extend = LazyTrieState(\n                    lm_state=self.lm_state\n                    &lt;&lt; int(self.trie.trie.leaf2token_id[eot_node]),\n                    trie=self.trie,\n                    node=self.root,\n                    weight=self.weight + mass[eot_node] - mass[self.node],\n                    mode=self.mode,\n                )\n        return self._extend\n\n    @cached_property\n    def logp_next(self):\n        \"\"\"Computes log probabilities for next possible transitions.\n\n        Returns:\n            (LazyByteProbs): Lazy log probability distribution over possible next bytes\n        \"\"\"\n        logps = np.full(258, -np.inf)  # 258 for EOT, EOS + 256 for normal bytes\n        mass = self.mass\n        logZ = mass[self.node]\n\n        for byte, node in self.actions().items():\n            logps[byte if byte is not None else 256] = mass[node] - logZ\n\n        return LazyByteProbs(logps)\n\n    async def materialize(self):\n        \"\"\"Materializes the masses for each node in the trie for the current state.\n\n        This makes a call to the language model and the underlying trie.\n\n        Returns:\n            (LazyTrieState): Self with materialized masses\n        \"\"\"\n        if self._mass is None:\n            logp_next = await self.lm_state.logp_next()\n            log_mass = await self.trie.weight_sum(torch.exp(logp_next), self.mode)\n            mass = torch.log(log_mass)\n            self._mass = mass.cpu().numpy()\n        return self\n\n    def __repr__(self):\n        context = colors.green % (\"|\" + escape(bytes(self.partial)))\n        if self.terminated:\n            context += colors.yellow % \"&lt;EOS&gt;\"\n        return f\"{self.weight:.2f}: {self.lm_state}\" + context\n\n    async def cleanup(self):\n        \"\"\"Cleans up resources used by the trie.\"\"\"\n        await self.trie.cleanup()\n</code></pre>"},{"location":"reference/genlm/bytes/__init__/#genlm.bytes.LazyTrieState.initial","title":"<code>initial(lm, trie, mode=TrieMode.WITH_EOS)</code>  <code>classmethod</code>","text":"<p>Creates an initial trie state.</p> <p>Parameters:</p> Name Type Description Default <code>lm</code> <code>AsyncLM</code> <p>Language model to use</p> required <code>trie</code> <code>TokenByteTrie</code> <p>TokenByteTrie structure for byte-to-token mapping</p> required <code>mode</code> <code>TrieMode</code> <p>Trie mode to use</p> <code>WITH_EOS</code> <p>Returns:</p> Type Description <code>LazyTrieState</code> <p>Initial state at root of trie with weight 0.0</p> Source code in <code>genlm/bytes/byte_lm/trie_state.py</code> <pre><code>@classmethod\ndef initial(cls, lm, trie, mode=TrieMode.WITH_EOS):\n    \"\"\"Creates an initial trie state.\n\n    Args:\n        lm (genlm.backend.AsyncLM): Language model to use\n        trie (TokenByteTrie): TokenByteTrie structure for byte-to-token mapping\n        mode (TrieMode): Trie mode to use\n\n    Returns:\n        (LazyTrieState): Initial state at root of trie with weight 0.0\n    \"\"\"\n    return cls(\n        trie=trie,\n        node=trie.trie.root,\n        lm_state=StatefulTokenizedLM.initial(lm),\n        weight=0.0,\n        mode=mode,\n    )\n</code></pre>"},{"location":"reference/genlm/bytes/__init__/#genlm.bytes.LazyTrieState.partial","title":"<code>partial</code>  <code>property</code>","text":"<p>Returns the byte sequence corresponding to the current node in the trie.</p>"},{"location":"reference/genlm/bytes/__init__/#genlm.bytes.LazyTrieState.mass","title":"<code>mass</code>  <code>property</code>","text":"<p>Returns the log mass for each node in the trie.</p> <p>The mass at a node corresponds to the sum of the probabilities of all tokens which share the prefix (<code>self.partial</code>) represented by that node.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If state hasn't been materialized yet</p>"},{"location":"reference/genlm/bytes/__init__/#genlm.bytes.LazyTrieState.with_mode","title":"<code>with_mode(mode)</code>","text":"<p>Returns a new state with the given mode.</p> Source code in <code>genlm/bytes/byte_lm/trie_state.py</code> <pre><code>def with_mode(self, mode):\n    \"\"\"Returns a new state with the given mode.\"\"\"\n    return LazyTrieState(\n        lm_state=self.lm_state,\n        trie=self.trie,\n        node=self.node,\n        weight=self.weight,\n        mass=self._mass,\n        mode=mode,\n        terminated=self.terminated,\n    )\n</code></pre>"},{"location":"reference/genlm/bytes/__init__/#genlm.bytes.LazyTrieState.actions","title":"<code>actions()</code>","text":"<p>Returns possible byte transitions from current node.</p> Source code in <code>genlm/bytes/byte_lm/trie_state.py</code> <pre><code>def actions(self):\n    \"\"\"Returns possible byte transitions from current node.\"\"\"\n    return self.children[self.node]\n</code></pre>"},{"location":"reference/genlm/bytes/__init__/#genlm.bytes.LazyTrieState.get_EOT","title":"<code>get_EOT()</code>","text":"<p>Returns the end-of-token node if available from current position in the trie.</p> Source code in <code>genlm/bytes/byte_lm/trie_state.py</code> <pre><code>def get_EOT(self):\n    \"\"\"Returns the end-of-token node if available from current position in the trie.\"\"\"\n    return self.children[self.node].get(self.trie.trie.eot_token)\n</code></pre>"},{"location":"reference/genlm/bytes/__init__/#genlm.bytes.LazyTrieState.__lshift__","title":"<code>__lshift__(b)</code>","text":"<p>Transitions to a new state by consuming a byte.</p> <p>Parameters:</p> Name Type Description Default <code>b</code> <code>int</code> <p>Byte to consume</p> required <p>Returns:</p> Type Description <code>LazyTrieState | None</code> <p>New state after consuming byte, or None if transition invalid (terminated or EOS)</p> Source code in <code>genlm/bytes/byte_lm/trie_state.py</code> <pre><code>def __lshift__(self, b):\n    \"\"\"Transitions to a new state by consuming a byte.\n\n    Args:\n        b (int): Byte to consume\n\n    Returns:\n        (LazyTrieState|None): New state after consuming byte, or None if transition invalid (terminated or EOS)\n    \"\"\"\n    if self.terminated:\n        return None\n\n    if node := self.children[self.node].get(b):\n        mass = self.mass\n        return LazyTrieState(\n            lm_state=self.lm_state,\n            trie=self.trie,\n            mass=mass,\n            node=node,\n            weight=self.weight + mass[node] - mass[self.node],\n            mode=self.mode,\n            terminated=b == EOS,\n        )\n</code></pre>"},{"location":"reference/genlm/bytes/__init__/#genlm.bytes.LazyTrieState.extend","title":"<code>extend()</code>","text":"<p>Extends current state by consuming an end-of-token if possible.</p> <p>Returns:</p> Type Description <code>LazyTrieState | None</code> <p>New state after consuming EOT, or None if not possible</p> Source code in <code>genlm/bytes/byte_lm/trie_state.py</code> <pre><code>def extend(self):\n    \"\"\"Extends current state by consuming an end-of-token if possible.\n\n    Returns:\n        (LazyTrieState|None): New state after consuming EOT, or None if not possible\n    \"\"\"\n    if self._extend is None:\n        if (eot_node := self.get_EOT()) is not None:\n            mass = self.mass\n            self._extend = LazyTrieState(\n                lm_state=self.lm_state\n                &lt;&lt; int(self.trie.trie.leaf2token_id[eot_node]),\n                trie=self.trie,\n                node=self.root,\n                weight=self.weight + mass[eot_node] - mass[self.node],\n                mode=self.mode,\n            )\n    return self._extend\n</code></pre>"},{"location":"reference/genlm/bytes/__init__/#genlm.bytes.LazyTrieState.logp_next","title":"<code>logp_next</code>  <code>cached</code> <code>property</code>","text":"<p>Computes log probabilities for next possible transitions.</p> <p>Returns:</p> Type Description <code>LazyByteProbs</code> <p>Lazy log probability distribution over possible next bytes</p>"},{"location":"reference/genlm/bytes/__init__/#genlm.bytes.LazyTrieState.materialize","title":"<code>materialize()</code>  <code>async</code>","text":"<p>Materializes the masses for each node in the trie for the current state.</p> <p>This makes a call to the language model and the underlying trie.</p> <p>Returns:</p> Type Description <code>LazyTrieState</code> <p>Self with materialized masses</p> Source code in <code>genlm/bytes/byte_lm/trie_state.py</code> <pre><code>async def materialize(self):\n    \"\"\"Materializes the masses for each node in the trie for the current state.\n\n    This makes a call to the language model and the underlying trie.\n\n    Returns:\n        (LazyTrieState): Self with materialized masses\n    \"\"\"\n    if self._mass is None:\n        logp_next = await self.lm_state.logp_next()\n        log_mass = await self.trie.weight_sum(torch.exp(logp_next), self.mode)\n        mass = torch.log(log_mass)\n        self._mass = mass.cpu().numpy()\n    return self\n</code></pre>"},{"location":"reference/genlm/bytes/__init__/#genlm.bytes.LazyTrieState.cleanup","title":"<code>cleanup()</code>  <code>async</code>","text":"<p>Cleans up resources used by the trie.</p> Source code in <code>genlm/bytes/byte_lm/trie_state.py</code> <pre><code>async def cleanup(self):\n    \"\"\"Cleans up resources used by the trie.\"\"\"\n    await self.trie.cleanup()\n</code></pre>"},{"location":"reference/genlm/bytes/__init__/#genlm.bytes.StatefulTokenizedLM","title":"<code>StatefulTokenizedLM</code>","text":"<p>A stateful tokenized language model that maintains context and generates next token logprobs.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>AsyncLM</code> <p>The underlying language model</p> required <code>context</code> <code>list</code> <p>List of token IDs representing the current context</p> required <code>n_calls</code> <code>int</code> <p>Number of times the model has been called</p> <code>0</code> <code>max_context_length</code> <code>int</code> <p>Maximum length of context to maintain</p> <code>None</code> Source code in <code>genlm/bytes/byte_lm/lm_state.py</code> <pre><code>class StatefulTokenizedLM:\n    \"\"\"A stateful tokenized language model that maintains context and generates next token logprobs.\n\n    Args:\n        model (genlm.backend.AsyncLM): The underlying language model\n        context (list): List of token IDs representing the current context\n        n_calls (int): Number of times the model has been called\n        max_context_length (int, optional): Maximum length of context to maintain\n    \"\"\"\n\n    def __init__(self, model, context, n_calls=0, max_context_length=None):\n        self.model = model\n        self.context = context\n        self._n_calls = n_calls\n        self.max_context_length = max_context_length\n\n    @classmethod\n    def initial(cls, model, initial_context=None, max_context_length=None):\n        \"\"\"Creates an initial state for the language model.\n\n        Args:\n            model (genlm.backend.AsyncLM): The language model to use\n            initial_context (list, optional): Initial context of token IDs. Defaults to [tokenizer.bos_token_id]\n            max_context_length (int, optional): Maximum context length to maintain\n\n        Returns:\n            (StatefulTokenizedLM): A new instance with initial state\n        \"\"\"\n        if initial_context is None:\n            initial_context = [model.tokenizer.bos_token_id]\n        return cls(model, initial_context, max_context_length=max_context_length)\n\n    def __lshift__(self, token):\n        \"\"\"Adds a new token to the context and returns a new state.\n\n        Args:\n            token (int): Token ID to add to context\n\n        Returns:\n            (StatefulTokenizedLM): New state with updated context\n        \"\"\"\n        assert isinstance(token, int)\n        if (\n            self.max_context_length is not None\n            and len(self.context) &gt;= self.max_context_length\n        ):\n            self.context = self.context[-(self.max_context_length - 1) :]\n        return StatefulTokenizedLM(\n            self.model, self.context + [token], n_calls=self._n_calls\n        )\n\n    async def logp_next(self):\n        \"\"\"Computes log probabilities for the next token given the current context.\n\n        Returns:\n            (torch.Tensor): Log probabilities for next tokens\n        \"\"\"\n        self._n_calls += 1\n        return await self.model.next_token_logprobs(self.context)\n\n    def __repr__(self):\n        return colors.purple % (\n            \"|\".join([escape(self.model.byte_vocab[x]) for x in self.context])\n        )\n</code></pre>"},{"location":"reference/genlm/bytes/__init__/#genlm.bytes.StatefulTokenizedLM.initial","title":"<code>initial(model, initial_context=None, max_context_length=None)</code>  <code>classmethod</code>","text":"<p>Creates an initial state for the language model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>AsyncLM</code> <p>The language model to use</p> required <code>initial_context</code> <code>list</code> <p>Initial context of token IDs. Defaults to [tokenizer.bos_token_id]</p> <code>None</code> <code>max_context_length</code> <code>int</code> <p>Maximum context length to maintain</p> <code>None</code> <p>Returns:</p> Type Description <code>StatefulTokenizedLM</code> <p>A new instance with initial state</p> Source code in <code>genlm/bytes/byte_lm/lm_state.py</code> <pre><code>@classmethod\ndef initial(cls, model, initial_context=None, max_context_length=None):\n    \"\"\"Creates an initial state for the language model.\n\n    Args:\n        model (genlm.backend.AsyncLM): The language model to use\n        initial_context (list, optional): Initial context of token IDs. Defaults to [tokenizer.bos_token_id]\n        max_context_length (int, optional): Maximum context length to maintain\n\n    Returns:\n        (StatefulTokenizedLM): A new instance with initial state\n    \"\"\"\n    if initial_context is None:\n        initial_context = [model.tokenizer.bos_token_id]\n    return cls(model, initial_context, max_context_length=max_context_length)\n</code></pre>"},{"location":"reference/genlm/bytes/__init__/#genlm.bytes.StatefulTokenizedLM.__lshift__","title":"<code>__lshift__(token)</code>","text":"<p>Adds a new token to the context and returns a new state.</p> <p>Parameters:</p> Name Type Description Default <code>token</code> <code>int</code> <p>Token ID to add to context</p> required <p>Returns:</p> Type Description <code>StatefulTokenizedLM</code> <p>New state with updated context</p> Source code in <code>genlm/bytes/byte_lm/lm_state.py</code> <pre><code>def __lshift__(self, token):\n    \"\"\"Adds a new token to the context and returns a new state.\n\n    Args:\n        token (int): Token ID to add to context\n\n    Returns:\n        (StatefulTokenizedLM): New state with updated context\n    \"\"\"\n    assert isinstance(token, int)\n    if (\n        self.max_context_length is not None\n        and len(self.context) &gt;= self.max_context_length\n    ):\n        self.context = self.context[-(self.max_context_length - 1) :]\n    return StatefulTokenizedLM(\n        self.model, self.context + [token], n_calls=self._n_calls\n    )\n</code></pre>"},{"location":"reference/genlm/bytes/__init__/#genlm.bytes.StatefulTokenizedLM.logp_next","title":"<code>logp_next()</code>  <code>async</code>","text":"<p>Computes log probabilities for the next token given the current context.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>Log probabilities for next tokens</p> Source code in <code>genlm/bytes/byte_lm/lm_state.py</code> <pre><code>async def logp_next(self):\n    \"\"\"Computes log probabilities for the next token given the current context.\n\n    Returns:\n        (torch.Tensor): Log probabilities for next tokens\n    \"\"\"\n    self._n_calls += 1\n    return await self.model.next_token_logprobs(self.context)\n</code></pre>"},{"location":"reference/genlm/bytes/__init__/#genlm.bytes.BeamParams","title":"<code>BeamParams</code>  <code>dataclass</code>","text":"<p>Parameters for byte-level beam summing algorithm.</p> <p>Parameters:</p> Name Type Description Default <code>K</code> <code>int</code> <p>Beam width - maximum number of candidates to maintain.</p> required <code>prune_threshold</code> <code>float</code> <p>Probability threshold for pruning candidates. Candidates with probability below this are removed. Defaults to 0.0</p> <code>0.0</code> <code>verbose</code> <code>bool</code> <p>Whether to print the beam state at each step. Defaults to False</p> <code>False</code> <code>eos_tokens</code> <code>list[bytes]</code> <p>List of tokens that should be treated as EOS. When configured, EOS tokens will terminate generation when sampled. Defaults to None</p> <code>None</code> <code>heal</code> <code>bool</code> <p>Whether to enable adaptive token healing. Defaults to True</p> <code>True</code> <code>heal_max_backoff</code> <code>int</code> <p>Maximum number of bytes to back off when healing. Defaults to None</p> <code>None</code> <code>heal_max_splits</code> <code>int</code> <p>Maximum number of intra-suffix commits allowed during a single healing attempt. Defaults to None</p> <code>None</code> Source code in <code>genlm/bytes/byte_lm/beam.py</code> <pre><code>@dataclass\nclass BeamParams:\n    \"\"\"Parameters for byte-level beam summing algorithm.\n\n    Args:\n        K (int): Beam width - maximum number of candidates to maintain.\n        prune_threshold (float, optional): Probability threshold for pruning candidates.\n            Candidates with probability below this are removed. Defaults to 0.0\n        verbose (bool, optional): Whether to print the beam state at each step. Defaults to False\n        eos_tokens (list[bytes], optional): List of tokens that should be treated as EOS. When configured,\n            EOS tokens will terminate generation when sampled. Defaults to None\n        heal (bool, optional): Whether to enable adaptive token healing. Defaults to True\n        heal_max_backoff (int, optional): Maximum number of bytes to back off when healing. Defaults to None\n        heal_max_splits (int, optional): Maximum number of intra-suffix commits allowed during a single healing attempt. Defaults to None\n    \"\"\"\n\n    K: int\n    prune_threshold: float = 0.0\n    verbose: bool = False\n    eos_tokens: list[bytes] = None\n    heal: bool = True\n    heal_max_backoff: int | None = None\n    # Optional cap on how many intra-partial commits are allowed during a\n    # single healing attempt. None means unlimited. Set to 0 to disable\n    # multi-split behavior (i.e., single-split only).\n    heal_max_splits: int | None = None\n\n    def __post_init__(self):\n        if self.prune_threshold &lt; 0:\n            raise ValueError(\n                f\"prune_threshold must be non-negative, got {self.prune_threshold}\"\n            )\n        self.log_prune_threshold = (\n            np.log(self.prune_threshold) if self.prune_threshold &gt; 0 else -np.inf\n        )\n        self.eos_tokens = set(self.eos_tokens) if self.eos_tokens else set()\n</code></pre>"},{"location":"reference/genlm/bytes/__init__/#genlm.bytes.TokenByteTrie","title":"<code>TokenByteTrie</code>","text":"<p>A trie data structure for efficient token-to-byte mapping.</p> Source code in <code>genlm/bytes/trie.py</code> <pre><code>class TokenByteTrie:\n    \"\"\"A trie data structure for efficient token-to-byte mapping.\"\"\"\n\n    def __init__(\n        self,\n        decode,\n        device=None,\n        atomic_tokens=None,\n        eot_token=None,\n        eos_tokens=None,\n        max_batch_size=64,\n    ):\n        \"\"\"Initialize a `TokenByteTrie`.\n\n        Args:\n            decode (list[bytes]): List representing the token vocabulary.\n            device (str, optional): Device to use for weight sum and max computations ('cpu' or 'cuda').\n            atomic_tokens (list[bytes], optional): List of tokens that should be treated as atomic units rather than being split into bytes.\n            eot_token (bytes|None, optional): End-of-token token. Default is None, which represents EOT as None.\n            eos_tokens (set[bytes], optional): Set of tokens that should be treated as EOS (End of Sequence).\n            max_batch_size (int, optional): Maximum batch size for weight sum sparse matrix multiplication.\n        \"\"\"\n        self.decode = decode\n        self.max_batch_size = max_batch_size\n\n        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        if self.device not in [\"cpu\", \"cuda\"]:\n            raise ValueError(f\"Invalid device: {device}. Must be 'cpu', 'cuda' or None\")\n\n        self.eot_token = eot_token\n        self.eos_tokens = set(eos_tokens or [])\n        self.eos_token_ids = [\n            i for i, token in enumerate(decode) if token in self.eos_tokens\n        ]\n\n        self._build_trie(atomic_tokens or [])\n        self._renumber()\n        self._build_node2prefix()\n        self._build_reachability_matrix()\n        self.token_ids = torch.tensor(\n            self.token_id_to_leaf[:, 0], dtype=torch.long, device=self.device\n        )\n\n    def _build_trie(self, atomic_tokens):\n        \"\"\"Builds a trie data structure from the vocabulary.\n\n        Returns:\n            (dict): A dictionary where keys are token IDs and values are lists of characters.\n        \"\"\"\n        for token in atomic_tokens:\n            if token not in self.decode:\n                raise ValueError(f\"Atomic token {token} not in vocabulary\")\n\n        for token in self.eos_tokens:\n            if token not in self.decode:\n                raise ValueError(f\"EOS token {token} not in vocabulary\")\n\n        self.word2leaf = {}\n        self.children = [{}]  # First node is root\n        self.root = 0\n        self.token_id_to_leaf = []\n        self.lookup = {}\n\n        for token_id, word in enumerate(self.decode):\n            if word in self.lookup:\n                raise ValueError(f\"Duplicate word in vocabulary: {word}\")\n            self.lookup[word] = token_id\n\n            # Build ALL tokens in trie (including EOS tokens for conditioning mode)\n            curr = self.root\n            letters = [word] if word in atomic_tokens else word\n            for letter in letters:\n                if letter not in self.children[curr]:\n                    self.children[curr][letter] = len(self.children)\n                    self.children.append({})\n                curr = self.children[curr][letter]\n\n            self.children[curr][self.eot_token] = last = len(self.children)\n            self.children.append({})\n            assert word not in self.word2leaf\n            self.word2leaf[word] = last\n            self.token_id_to_leaf.append((token_id, last))\n\n        self.eos_node = len(self.children)\n        self.children.append({})  # Create the EOS node\n        self.children[self.root][EOS] = self.eos_node\n\n        self.leaf2word = dict(zip(self.word2leaf.values(), self.word2leaf.keys()))\n        self.jump = [\n            np.array(sorted(x.values()), dtype=np.int32) for x in self.children\n        ]\n\n    def _renumber(self):\n        \"\"\"Renumber the states of the trie so that they are named by a contiguous\n        range of integers and those integers respect the topological ordering\n        of the trie. This improves the efficiency of the updating the trie as\n        it improves memory locality.\n        \"\"\"\n        self.ordering = np.array(list(self._order(self.root)), np.int32)\n        ordering = {}\n        for i, x in enumerate(self._order_full(self.root)):\n            ordering[x] = i\n        self._rename(f=lambda x: ordering[x])\n\n    def _order(self, node):\n        \"\"\"Generate a topological ordering of nodes beneath the given node.\n\n        Args:\n            node (int): Starting node index\n\n        Yields:\n            int: Node indices in topological order\n        \"\"\"\n        for a in self.children[node]:\n            if a is not None:\n                yield from self._order(self.children[node][a])\n        yield node\n\n    def _order_full(self, node):\n        \"\"\"Generate a complete topological ordering including all child nodes.\n\n        Args:\n            node (int): Starting node index\n\n        Yields:\n            (int): Node indices in complete topological order\n        \"\"\"\n        for a in self.children[node]:\n            yield from self._order_full(self.children[node][a])\n        yield node\n\n    def _rename(self, f):\n        \"\"\"Rename all node indices in the trie using the provided mapping function.\n\n        Args:\n            f (callable): Function that maps old node indices to new node indices\n        \"\"\"\n        N = len(self.children)\n\n        new_children = [{} for _ in range(N)]\n        nodes = range(N)\n\n        for x in nodes:\n            for letter, y in self.children[x].items():\n                new_children[f(x)][letter] = f(y)\n\n        self.root = f(self.root)\n        self.children = new_children\n        self.word2leaf = {w: f(x) for w, x in self.word2leaf.items()}\n        self.leaf2word = dict(zip(self.word2leaf.values(), self.word2leaf.keys()))\n\n        self.token_id_to_leaf = np.array(\n            [(i, f(x)) for i, x in self.token_id_to_leaf], dtype=np.int32\n        )\n        self.leaf2token_id = dict(\n            zip(self.token_id_to_leaf[:, 1], self.token_id_to_leaf[:, 0])\n        )\n\n        self.ordering = np.array([f(x) for x in self.ordering])\n        self.jump = [np.array(sorted(x.values()), dtype=np.int32) for x in new_children]\n\n        # Update EOS node after renumbering\n        self.eos_node = f(self.eos_node)\n\n    def _build_node2prefix(self):\n        \"\"\"Builds a mapping from each node to its prefix.\n\n        Returns:\n            (dict): A dictionary where keys are node IDs and values are lists of characters.\n        \"\"\"\n        node2prefix = {self.root: []}\n        for x in reversed(range(len(self.children))):\n            for letter, y in self.children[x].items():\n                if letter is None:\n                    node2prefix[y] = node2prefix[x]\n                elif isinstance(letter, bytes):\n                    node2prefix[y] = node2prefix[x] + list(letter)\n                else:\n                    node2prefix[y] = node2prefix[x] + [letter]\n\n        self.node2prefix = node2prefix\n\n    def _build_parent_map(self):\n        \"\"\"Builds a mapping from each node to its parent node in the trie.\n\n        Returns:\n            (dict): A dictionary where keys are child nodes and values are their parent nodes.\n        \"\"\"\n        parent = {}\n        for node in range(len(self.children)):\n            for child in self.jump[node]:\n                parent[child] = node\n        return parent\n\n    def _build_reachability_matrix(self):\n        \"\"\"Constructs dual sparse reachability matrices for efficient weight propagation.\n\n        The matrix M is constructed such that M[i,j] = 1 if node j is either:\n        - The leaf node i itself (self-connection)\n        - An ancestor of leaf node i in the trie\n\n        For propagate_eos mode, EOS tokens contribute directly to eos_node and root.\n        \"\"\"\n        leaf_indices = self.token_id_to_leaf[:, 1]\n        parent = self._build_parent_map()\n        # Build no_eos matrix (includes all tokens, doesn't map any tokens to the eos_node)\n        rows_no_eos, cols_no_eos = [], []\n        # Build with_eos matrix (maps EOS tokens to the eos_node only)\n        rows_with_eos, cols_with_eos = [], []\n\n        for i, node in enumerate(leaf_indices):\n            token_id = self.token_id_to_leaf[i, 0]\n            token = self.decode[token_id]\n\n            # self-connection\n            rows_no_eos.append(i)\n            cols_no_eos.append(node)\n            if token not in self.eos_tokens:\n                rows_with_eos.append(i)\n                cols_with_eos.append(node)\n            else:\n                # EOS tokens: contribute directly to eos_node and root\n                rows_with_eos.append(i)\n                cols_with_eos.append(self.eos_node)\n                rows_with_eos.append(i)\n                cols_with_eos.append(self.root)\n\n            current = node\n            while current in parent:\n                ancestor = parent[current]\n                rows_no_eos.append(i)\n                cols_no_eos.append(ancestor)\n                if token not in self.eos_tokens:\n                    rows_with_eos.append(i)\n                    cols_with_eos.append(ancestor)\n                current = ancestor\n\n        # Build without_eos matrix\n        indices_no_eos = torch.tensor(\n            [rows_no_eos, cols_no_eos], dtype=torch.long, device=self.device\n        )\n        values_no_eos = torch.ones(len(rows_no_eos), device=self.device)\n        self.M_no_eos = torch.sparse_coo_tensor(\n            indices_no_eos, values_no_eos, (len(leaf_indices), len(self.children))\n        ).to_sparse_csr()\n\n        # Build with_eos matrix\n        indices_with_eos = torch.tensor(\n            [rows_with_eos, cols_with_eos], dtype=torch.long, device=self.device\n        )\n        values_with_eos = torch.ones(len(rows_with_eos), device=self.device)\n        self.M_with_eos = torch.sparse_coo_tensor(\n            indices_with_eos, values_with_eos, (len(leaf_indices), len(self.children))\n        ).to_sparse_csr()\n\n        # Keep the old matrix for backward compatibility\n        self.M = self.M_no_eos\n        self.src_indices = torch.tensor(\n            rows_no_eos, dtype=torch.long, device=self.device\n        )\n        self.dst_indices = torch.tensor(\n            cols_no_eos, dtype=torch.long, device=self.device\n        )\n\n    def _preprocess_ws(self, batch_ws):\n        \"\"\"Preprocess weight sums for batch processing.\n\n        Args:\n            batch_ws (list|np.ndarray|torch.Tensor): List of weight sum tensors or lists of weight sums.\n\n        Returns:\n            (torch.Tensor): Stacked weight sum tensor.\n        \"\"\"\n        processed_batch_ws = []\n        for ws in batch_ws:\n            if not isinstance(ws, torch.Tensor):\n                ws = torch.tensor(ws, device=self.device, dtype=torch.float32)\n            elif ws.device != self.device or ws.dtype != torch.float32:\n                ws = ws.to(device=self.device, dtype=torch.float32)\n            assert ws.shape[0] == len(self.decode), [ws.shape[0], len(self.decode)]\n            processed_batch_ws.append(ws)\n        return torch.stack(processed_batch_ws)\n\n    def weight_sum(self, ws, mode=None):\n        \"\"\"Computes the sum of weights of all leaf nodes (tokens) that are descendants of each node in the trie.\n\n        Args:\n            ws (torch.Tensor): Token weights, shape (`len(self.decode)`,).\n            mode (TrieMode, optional): Trie mode - determines matrix selection.\n                                     If None, defaults to WITHOUT_EOS.\n\n        Returns:\n            (numpy.ndarray): Summed weights for each node in the trie, shape (num_nodes,).\n        \"\"\"\n        mode = mode or TrieMode.WITHOUT_EOS\n        return self.batch_weight_sum(self._preprocess_ws([ws]), mode=mode)[0]\n\n    def batch_weight_sum(self, ws, mode=None):\n        \"\"\"Batch version of `weight_sum`.\n\n        Args:\n            ws (torch.Tensor): Batch of token weights, shape (batch_size \u00d7 `len(self.decode)`).\n            mode (TrieMode, optional): Trie mode - determines matrix selection.\n                                     If None, defaults to WITHOUT_EOS.\n\n        Returns:\n            (numpy.ndarray): Summed weights for each node in the trie, shape (batch_size \u00d7 num_nodes).\n        \"\"\"\n        mode = mode or TrieMode.WITHOUT_EOS\n\n        ws = self._preprocess_ws(ws)\n        batch_size = ws.shape[0]\n        all_masses = []\n\n        # Choose matrix based on mode\n        matrix = self.M_with_eos if mode == TrieMode.WITH_EOS else self.M_no_eos\n\n        # If you are getting illegal memory access errors here,\n        # try reducing the max_batch_size.\n        for i in range(0, batch_size, self.max_batch_size):\n            batch_ws = ws[i : i + self.max_batch_size]\n            masses = torch.sparse.mm(batch_ws[:, self.token_ids], matrix)\n            all_masses.append(masses)\n        return torch.cat(all_masses, dim=0)\n\n    def weight_max(self, ws):\n        \"\"\"Computes the maximum weight of all descendant leaf nodes (tokens) for each node in the trie.\n\n        Args:\n            ws (torch.Tensor): Token weights, shape (`len(self.decode)`,).\n\n        Returns:\n            (numpy.ndarray): Maximum weights for each node in the trie, shape (num_nodes,).\n        \"\"\"\n        return self.batch_weight_max(self._preprocess_ws([ws]))[0]\n\n    def batch_weight_max(self, ws):\n        \"\"\"Batch version of `weight_max`.\n\n        Args:\n            ws (torch.Tensor): Batch of token weights, shape (batch_size \u00d7 `len(self.decode)`).\n\n        Returns:\n            (numpy.ndarray): Maximum weights for each node in the trie, shape (batch_size \u00d7 num_nodes).\n        \"\"\"\n        ws = self._preprocess_ws(ws)\n\n        # Get leaf weights\n        leaf_weights = ws[:, self.token_ids]  # shape: (batch_size \u00d7 num_leafs)\n        batch_size = leaf_weights.shape[0]\n\n        # Use scatter_reduce to propagate maximum values in parallel\n        result = torch.zeros((batch_size, len(self.children)), device=self.device)\n        result.scatter_reduce_(\n            dim=1,\n            index=self.dst_indices.expand(batch_size, -1),\n            src=leaf_weights[:, self.src_indices],\n            reduce=\"amax\",\n            include_self=False,\n        )\n\n        return result\n\n    def visualize(self, ws=None):\n        \"\"\"Visualize the trie structure using Graphviz.\n\n        Args:\n            ws (np.ndarray|None): Optional weight vector to display at each node. Should be of length `len(self.children)`.\n\n        Returns:\n            (graphviz.Digraph): The generated graph object\n        \"\"\"\n        try:\n            import graphviz\n        except ImportError:  # pragma: no cover\n            raise ImportError(\n                \"Please install graphviz: pip install graphviz\"\n            )  # pragma: no cover\n\n        if ws is not None and len(ws) != len(self.children):\n            raise ValueError(\n                f\"Weight vector length ({len(ws)}) must match number of nodes ({len(self.children)})\"\n            )\n\n        dot = graphviz.Digraph(comment=\"Token Character Trie\")\n        dot.attr(rankdir=\"LR\")\n\n        # Create a subgraph for the legend\n        with dot.subgraph(name=\"cluster_legend\") as legend:\n            legend.attr(label=\"Legend\", fontsize=\"10\")\n            legend.attr(\"node\", fontsize=\"7\", width=\"0.1\", height=\"0.1\")\n\n            # Example internal node\n            legend.node(\n                \"legend_internal\",\n                \"Internal Node ID\\n'Prefix'\\nWeight (if provided)\",\n                shape=\"circle\",\n            )\n\n            # Example leaf node\n            legend.node(\"legend_leaf\", \"Complete Token\", shape=\"doublecircle\")\n\n            legend.edge(\n                \"legend_internal\",\n                \"legend_leaf\",\n                label=\"Token item\",\n                fontsize=\"10\",\n            )\n\n            # Align legend horizontally\n            legend.attr(rankdir=\"TB\")\n            legend.attr(rank=\"same\")\n\n        # Add the main trie nodes and edges\n        for node_id in range(len(self.children)):\n            prefix = self.node2prefix[node_id]\n\n            if ws is not None:\n                label = f\"{node_id}\\n'{prefix}'\\n{ws[node_id]:.4f}\"\n            else:\n                label = f\"{node_id}\\n'{prefix}'\"\n\n            # Color nodes based on mass if provided\n            if ws is not None:\n                max_ws = ws.max()\n                if max_ws &gt; 0:\n                    intensity = int(255 * (1 - ws[node_id] / max_ws))\n                    color = f\"#{intensity:02x}{255:02x}{intensity:02x}\"\n                else:\n                    color = \"#ffffff\"  # white for zero mass\n            else:\n                color = \"#ffffff\"  # default white\n\n            if node_id in self.leaf2word:\n                dot.node(\n                    str(node_id),\n                    label,\n                    shape=\"doublecircle\",\n                    style=\"filled\",\n                    fillcolor=color,\n                )\n            else:\n                dot.node(\n                    str(node_id), label, shape=\"circle\", style=\"filled\", fillcolor=color\n                )\n\n        for node_id, children in enumerate(self.children):\n            for char, child_id in children.items():\n                if char is not None:\n                    edge_label = str(char)\n                else:\n                    edge_label = \"End-of-Token\"\n\n                dot.edge(str(node_id), str(child_id), label=edge_label)\n\n        return dot\n</code></pre>"},{"location":"reference/genlm/bytes/__init__/#genlm.bytes.TokenByteTrie.__init__","title":"<code>__init__(decode, device=None, atomic_tokens=None, eot_token=None, eos_tokens=None, max_batch_size=64)</code>","text":"<p>Initialize a <code>TokenByteTrie</code>.</p> <p>Parameters:</p> Name Type Description Default <code>decode</code> <code>list[bytes]</code> <p>List representing the token vocabulary.</p> required <code>device</code> <code>str</code> <p>Device to use for weight sum and max computations ('cpu' or 'cuda').</p> <code>None</code> <code>atomic_tokens</code> <code>list[bytes]</code> <p>List of tokens that should be treated as atomic units rather than being split into bytes.</p> <code>None</code> <code>eot_token</code> <code>bytes | None</code> <p>End-of-token token. Default is None, which represents EOT as None.</p> <code>None</code> <code>eos_tokens</code> <code>set[bytes]</code> <p>Set of tokens that should be treated as EOS (End of Sequence).</p> <code>None</code> <code>max_batch_size</code> <code>int</code> <p>Maximum batch size for weight sum sparse matrix multiplication.</p> <code>64</code> Source code in <code>genlm/bytes/trie.py</code> <pre><code>def __init__(\n    self,\n    decode,\n    device=None,\n    atomic_tokens=None,\n    eot_token=None,\n    eos_tokens=None,\n    max_batch_size=64,\n):\n    \"\"\"Initialize a `TokenByteTrie`.\n\n    Args:\n        decode (list[bytes]): List representing the token vocabulary.\n        device (str, optional): Device to use for weight sum and max computations ('cpu' or 'cuda').\n        atomic_tokens (list[bytes], optional): List of tokens that should be treated as atomic units rather than being split into bytes.\n        eot_token (bytes|None, optional): End-of-token token. Default is None, which represents EOT as None.\n        eos_tokens (set[bytes], optional): Set of tokens that should be treated as EOS (End of Sequence).\n        max_batch_size (int, optional): Maximum batch size for weight sum sparse matrix multiplication.\n    \"\"\"\n    self.decode = decode\n    self.max_batch_size = max_batch_size\n\n    self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    if self.device not in [\"cpu\", \"cuda\"]:\n        raise ValueError(f\"Invalid device: {device}. Must be 'cpu', 'cuda' or None\")\n\n    self.eot_token = eot_token\n    self.eos_tokens = set(eos_tokens or [])\n    self.eos_token_ids = [\n        i for i, token in enumerate(decode) if token in self.eos_tokens\n    ]\n\n    self._build_trie(atomic_tokens or [])\n    self._renumber()\n    self._build_node2prefix()\n    self._build_reachability_matrix()\n    self.token_ids = torch.tensor(\n        self.token_id_to_leaf[:, 0], dtype=torch.long, device=self.device\n    )\n</code></pre>"},{"location":"reference/genlm/bytes/__init__/#genlm.bytes.TokenByteTrie.weight_sum","title":"<code>weight_sum(ws, mode=None)</code>","text":"<p>Computes the sum of weights of all leaf nodes (tokens) that are descendants of each node in the trie.</p> <p>Parameters:</p> Name Type Description Default <code>ws</code> <code>Tensor</code> <p>Token weights, shape (<code>len(self.decode)</code>,).</p> required <code>mode</code> <code>TrieMode</code> <p>Trie mode - determines matrix selection.                      If None, defaults to WITHOUT_EOS.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Summed weights for each node in the trie, shape (num_nodes,).</p> Source code in <code>genlm/bytes/trie.py</code> <pre><code>def weight_sum(self, ws, mode=None):\n    \"\"\"Computes the sum of weights of all leaf nodes (tokens) that are descendants of each node in the trie.\n\n    Args:\n        ws (torch.Tensor): Token weights, shape (`len(self.decode)`,).\n        mode (TrieMode, optional): Trie mode - determines matrix selection.\n                                 If None, defaults to WITHOUT_EOS.\n\n    Returns:\n        (numpy.ndarray): Summed weights for each node in the trie, shape (num_nodes,).\n    \"\"\"\n    mode = mode or TrieMode.WITHOUT_EOS\n    return self.batch_weight_sum(self._preprocess_ws([ws]), mode=mode)[0]\n</code></pre>"},{"location":"reference/genlm/bytes/__init__/#genlm.bytes.TokenByteTrie.batch_weight_sum","title":"<code>batch_weight_sum(ws, mode=None)</code>","text":"<p>Batch version of <code>weight_sum</code>.</p> <p>Parameters:</p> Name Type Description Default <code>ws</code> <code>Tensor</code> <p>Batch of token weights, shape (batch_size \u00d7 <code>len(self.decode)</code>).</p> required <code>mode</code> <code>TrieMode</code> <p>Trie mode - determines matrix selection.                      If None, defaults to WITHOUT_EOS.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Summed weights for each node in the trie, shape (batch_size \u00d7 num_nodes).</p> Source code in <code>genlm/bytes/trie.py</code> <pre><code>def batch_weight_sum(self, ws, mode=None):\n    \"\"\"Batch version of `weight_sum`.\n\n    Args:\n        ws (torch.Tensor): Batch of token weights, shape (batch_size \u00d7 `len(self.decode)`).\n        mode (TrieMode, optional): Trie mode - determines matrix selection.\n                                 If None, defaults to WITHOUT_EOS.\n\n    Returns:\n        (numpy.ndarray): Summed weights for each node in the trie, shape (batch_size \u00d7 num_nodes).\n    \"\"\"\n    mode = mode or TrieMode.WITHOUT_EOS\n\n    ws = self._preprocess_ws(ws)\n    batch_size = ws.shape[0]\n    all_masses = []\n\n    # Choose matrix based on mode\n    matrix = self.M_with_eos if mode == TrieMode.WITH_EOS else self.M_no_eos\n\n    # If you are getting illegal memory access errors here,\n    # try reducing the max_batch_size.\n    for i in range(0, batch_size, self.max_batch_size):\n        batch_ws = ws[i : i + self.max_batch_size]\n        masses = torch.sparse.mm(batch_ws[:, self.token_ids], matrix)\n        all_masses.append(masses)\n    return torch.cat(all_masses, dim=0)\n</code></pre>"},{"location":"reference/genlm/bytes/__init__/#genlm.bytes.TokenByteTrie.weight_max","title":"<code>weight_max(ws)</code>","text":"<p>Computes the maximum weight of all descendant leaf nodes (tokens) for each node in the trie.</p> <p>Parameters:</p> Name Type Description Default <code>ws</code> <code>Tensor</code> <p>Token weights, shape (<code>len(self.decode)</code>,).</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Maximum weights for each node in the trie, shape (num_nodes,).</p> Source code in <code>genlm/bytes/trie.py</code> <pre><code>def weight_max(self, ws):\n    \"\"\"Computes the maximum weight of all descendant leaf nodes (tokens) for each node in the trie.\n\n    Args:\n        ws (torch.Tensor): Token weights, shape (`len(self.decode)`,).\n\n    Returns:\n        (numpy.ndarray): Maximum weights for each node in the trie, shape (num_nodes,).\n    \"\"\"\n    return self.batch_weight_max(self._preprocess_ws([ws]))[0]\n</code></pre>"},{"location":"reference/genlm/bytes/__init__/#genlm.bytes.TokenByteTrie.batch_weight_max","title":"<code>batch_weight_max(ws)</code>","text":"<p>Batch version of <code>weight_max</code>.</p> <p>Parameters:</p> Name Type Description Default <code>ws</code> <code>Tensor</code> <p>Batch of token weights, shape (batch_size \u00d7 <code>len(self.decode)</code>).</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Maximum weights for each node in the trie, shape (batch_size \u00d7 num_nodes).</p> Source code in <code>genlm/bytes/trie.py</code> <pre><code>def batch_weight_max(self, ws):\n    \"\"\"Batch version of `weight_max`.\n\n    Args:\n        ws (torch.Tensor): Batch of token weights, shape (batch_size \u00d7 `len(self.decode)`).\n\n    Returns:\n        (numpy.ndarray): Maximum weights for each node in the trie, shape (batch_size \u00d7 num_nodes).\n    \"\"\"\n    ws = self._preprocess_ws(ws)\n\n    # Get leaf weights\n    leaf_weights = ws[:, self.token_ids]  # shape: (batch_size \u00d7 num_leafs)\n    batch_size = leaf_weights.shape[0]\n\n    # Use scatter_reduce to propagate maximum values in parallel\n    result = torch.zeros((batch_size, len(self.children)), device=self.device)\n    result.scatter_reduce_(\n        dim=1,\n        index=self.dst_indices.expand(batch_size, -1),\n        src=leaf_weights[:, self.src_indices],\n        reduce=\"amax\",\n        include_self=False,\n    )\n\n    return result\n</code></pre>"},{"location":"reference/genlm/bytes/__init__/#genlm.bytes.TokenByteTrie.visualize","title":"<code>visualize(ws=None)</code>","text":"<p>Visualize the trie structure using Graphviz.</p> <p>Parameters:</p> Name Type Description Default <code>ws</code> <code>ndarray | None</code> <p>Optional weight vector to display at each node. Should be of length <code>len(self.children)</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>Digraph</code> <p>The generated graph object</p> Source code in <code>genlm/bytes/trie.py</code> <pre><code>def visualize(self, ws=None):\n    \"\"\"Visualize the trie structure using Graphviz.\n\n    Args:\n        ws (np.ndarray|None): Optional weight vector to display at each node. Should be of length `len(self.children)`.\n\n    Returns:\n        (graphviz.Digraph): The generated graph object\n    \"\"\"\n    try:\n        import graphviz\n    except ImportError:  # pragma: no cover\n        raise ImportError(\n            \"Please install graphviz: pip install graphviz\"\n        )  # pragma: no cover\n\n    if ws is not None and len(ws) != len(self.children):\n        raise ValueError(\n            f\"Weight vector length ({len(ws)}) must match number of nodes ({len(self.children)})\"\n        )\n\n    dot = graphviz.Digraph(comment=\"Token Character Trie\")\n    dot.attr(rankdir=\"LR\")\n\n    # Create a subgraph for the legend\n    with dot.subgraph(name=\"cluster_legend\") as legend:\n        legend.attr(label=\"Legend\", fontsize=\"10\")\n        legend.attr(\"node\", fontsize=\"7\", width=\"0.1\", height=\"0.1\")\n\n        # Example internal node\n        legend.node(\n            \"legend_internal\",\n            \"Internal Node ID\\n'Prefix'\\nWeight (if provided)\",\n            shape=\"circle\",\n        )\n\n        # Example leaf node\n        legend.node(\"legend_leaf\", \"Complete Token\", shape=\"doublecircle\")\n\n        legend.edge(\n            \"legend_internal\",\n            \"legend_leaf\",\n            label=\"Token item\",\n            fontsize=\"10\",\n        )\n\n        # Align legend horizontally\n        legend.attr(rankdir=\"TB\")\n        legend.attr(rank=\"same\")\n\n    # Add the main trie nodes and edges\n    for node_id in range(len(self.children)):\n        prefix = self.node2prefix[node_id]\n\n        if ws is not None:\n            label = f\"{node_id}\\n'{prefix}'\\n{ws[node_id]:.4f}\"\n        else:\n            label = f\"{node_id}\\n'{prefix}'\"\n\n        # Color nodes based on mass if provided\n        if ws is not None:\n            max_ws = ws.max()\n            if max_ws &gt; 0:\n                intensity = int(255 * (1 - ws[node_id] / max_ws))\n                color = f\"#{intensity:02x}{255:02x}{intensity:02x}\"\n            else:\n                color = \"#ffffff\"  # white for zero mass\n        else:\n            color = \"#ffffff\"  # default white\n\n        if node_id in self.leaf2word:\n            dot.node(\n                str(node_id),\n                label,\n                shape=\"doublecircle\",\n                style=\"filled\",\n                fillcolor=color,\n            )\n        else:\n            dot.node(\n                str(node_id), label, shape=\"circle\", style=\"filled\", fillcolor=color\n            )\n\n    for node_id, children in enumerate(self.children):\n        for char, child_id in children.items():\n            if char is not None:\n                edge_label = str(char)\n            else:\n                edge_label = \"End-of-Token\"\n\n            dot.edge(str(node_id), str(child_id), label=edge_label)\n\n    return dot\n</code></pre>"},{"location":"reference/genlm/bytes/__init__/#genlm.bytes.AsyncTokenByteTrie","title":"<code>AsyncTokenByteTrie</code>","text":"<p>An asynchronous wrapper for TokenByteTrie implementations that provides automatic request batching.</p> Source code in <code>genlm/bytes/trie.py</code> <pre><code>class AsyncTokenByteTrie:\n    \"\"\"An asynchronous wrapper for TokenByteTrie implementations that provides automatic request batching.\"\"\"\n\n    def __init__(self, trie):\n        \"\"\"Initialize an `AsyncTokenByteTrie`.\n\n        Args:\n            trie (TokenByteTrie): The underlying `TokenByteTrie` instance\n        \"\"\"\n        self.trie = trie\n        self._queue = None\n        self._task = None\n\n    @classmethod\n    def from_vocab(cls, vocab, **kwargs):\n        \"\"\"Creates an `AsyncTokenByteTrie` from a vocabulary.\n\n        Args:\n            vocab (list): The vocabulary over which the trie will be defined.\n            **kwargs (dict): Additional arguments passed to the trie constructor.\n                             Can include 'eos_tokens' for EOS support.\n\n        Returns:\n            (AsyncTokenByteTrie): The initialized asynchronous trie instance.\n        \"\"\"\n        trie = TokenByteTrie(decode=vocab, **kwargs)\n        return cls(trie)\n\n    def _queue_request(self, ws, mode, op):\n        if not self._task or self._task.done():\n            self.start()\n\n        future = asyncio.get_running_loop().create_future()\n        self._queue.put_nowait(((ws, mode), future, op))\n        return future\n\n    async def weight_sum(self, ws, mode=None):\n        \"\"\"Queue a `weight_sum` request. Multiple concurrent calls will be automatically batched\n        together by (operation, mode) pairs.\n\n        Args:\n            ws (torch.Tensor): Token weights, shape (`len(self.trie.decode)`,).\n            mode (TrieMode, optional): Trie mode determining EOS handling. Defaults to WITHOUT_EOS.\n\n        Returns:\n            (np.ndarray): The calculated mass sums for the given distribution.\n        \"\"\"\n        mode = mode or TrieMode.WITHOUT_EOS\n        return await self._queue_request(ws, mode, TrieOp.SUM)\n\n    async def weight_max(self, ws):\n        \"\"\"Queue a `weight_max` request. Multiple concurrent calls will be automatically batched\n        together.\n\n        Args:\n            ws (torch.Tensor): Token weights, shape (`len(self.trie.decode)`,).\n\n        Returns:\n            (np.ndarray): The calculated max weights for the given distribution.\n        \"\"\"\n        # For MAX, mode doesn't matter so use WITHOUT_EOS as default\n        return await self._queue_request(ws, TrieMode.WITHOUT_EOS, TrieOp.MAX)\n\n    def start(self):\n        \"\"\"Start the background processing task if not already running.\"\"\"\n        if not self._task or self._task.done():\n            logger.debug(\"starting background loop\")\n            # Create a new queue so that it is bound to the current event loop\n            self._queue = asyncio.Queue()\n            self._task = asyncio.create_task(self._background_loop())\n\n    async def _background_loop(self):\n        \"\"\"Background task that processes queued weight sum and max requests.\n\n        Continuously monitors the queue for new requests and processes them in batches\n        grouped by (operation, mode) pairs using the underlying trie implementation.\n\n        Raises:\n            (Exception): If any error occurs during processing, it is propagated to all\n                         pending futures in the current batch.\n        \"\"\"\n        while True:\n            try:\n                # Group by (operation, mode) pairs for efficient batching\n                op_mode_groups = defaultdict(list)\n\n                (ws, mode), future, op = await self._queue.get()\n                op_mode_groups[(op, mode)].append(((ws, mode), future))\n\n                try:\n                    while True:\n                        (ws, mode), future, op = self._queue.get_nowait()\n                        op_mode_groups[(op, mode)].append(((ws, mode), future))\n                except asyncio.QueueEmpty:\n                    pass\n\n                for (op, mode), group in op_mode_groups.items():\n                    requests, futures = zip(*group)\n                    # Extract just the ws tensors from the (ws, mode) tuples\n                    ws_list = [req[0] for req in requests]\n\n                    if op == TrieOp.SUM:\n                        if logger.isEnabledFor(logging.DEBUG):\n                            logger.debug(\n                                f\"processing {len(ws_list)} sum requests with mode {mode}\"\n                            )  # pragma: no cover\n                        results = self.trie.batch_weight_sum(ws_list, mode=mode)\n                    elif op == TrieOp.MAX:\n                        if logger.isEnabledFor(logging.DEBUG):\n                            logger.debug(\n                                f\"processing {len(ws_list)} max requests\"\n                            )  # pragma: no cover\n                        # MAX operations don't need mode, so use the original batch_weight_max\n                        results = self.trie.batch_weight_max(ws_list)\n                    else:\n                        raise ValueError(f\"Unknown trie operation: {op}\")\n\n                    for future, result in zip(futures, results):\n                        future.set_result(result)\n\n            except Exception as e:\n                for group in op_mode_groups.values():\n                    for _, future in group:\n                        if not future.done():\n                            future.set_exception(e)\n                raise\n\n    async def cleanup(self):\n        \"\"\"Async cleanup - preferred method\"\"\"\n        if self._task and not self._task.done():\n            self._task.cancel()\n            try:\n                await self._task\n            except asyncio.CancelledError:\n                pass\n            self._task = None\n\n    def shutdown(self):\n        \"\"\"Stop the background processing task and cleanup resources.\"\"\"\n        if self._task is not None:\n            try:\n                self._task.cancel()\n            except RuntimeError:  # pragma: no cover\n                # Ignore runtime errors that might occur if event loop is closed\n                pass\n            self._task = None\n\n    def __del__(self):\n        self.shutdown()\n</code></pre>"},{"location":"reference/genlm/bytes/__init__/#genlm.bytes.AsyncTokenByteTrie.__init__","title":"<code>__init__(trie)</code>","text":"<p>Initialize an <code>AsyncTokenByteTrie</code>.</p> <p>Parameters:</p> Name Type Description Default <code>trie</code> <code>TokenByteTrie</code> <p>The underlying <code>TokenByteTrie</code> instance</p> required Source code in <code>genlm/bytes/trie.py</code> <pre><code>def __init__(self, trie):\n    \"\"\"Initialize an `AsyncTokenByteTrie`.\n\n    Args:\n        trie (TokenByteTrie): The underlying `TokenByteTrie` instance\n    \"\"\"\n    self.trie = trie\n    self._queue = None\n    self._task = None\n</code></pre>"},{"location":"reference/genlm/bytes/__init__/#genlm.bytes.AsyncTokenByteTrie.from_vocab","title":"<code>from_vocab(vocab, **kwargs)</code>  <code>classmethod</code>","text":"<p>Creates an <code>AsyncTokenByteTrie</code> from a vocabulary.</p> <p>Parameters:</p> Name Type Description Default <code>vocab</code> <code>list</code> <p>The vocabulary over which the trie will be defined.</p> required <code>**kwargs</code> <code>dict</code> <p>Additional arguments passed to the trie constructor.              Can include 'eos_tokens' for EOS support.</p> <code>{}</code> <p>Returns:</p> Type Description <code>AsyncTokenByteTrie</code> <p>The initialized asynchronous trie instance.</p> Source code in <code>genlm/bytes/trie.py</code> <pre><code>@classmethod\ndef from_vocab(cls, vocab, **kwargs):\n    \"\"\"Creates an `AsyncTokenByteTrie` from a vocabulary.\n\n    Args:\n        vocab (list): The vocabulary over which the trie will be defined.\n        **kwargs (dict): Additional arguments passed to the trie constructor.\n                         Can include 'eos_tokens' for EOS support.\n\n    Returns:\n        (AsyncTokenByteTrie): The initialized asynchronous trie instance.\n    \"\"\"\n    trie = TokenByteTrie(decode=vocab, **kwargs)\n    return cls(trie)\n</code></pre>"},{"location":"reference/genlm/bytes/__init__/#genlm.bytes.AsyncTokenByteTrie.weight_sum","title":"<code>weight_sum(ws, mode=None)</code>  <code>async</code>","text":"<p>Queue a <code>weight_sum</code> request. Multiple concurrent calls will be automatically batched together by (operation, mode) pairs.</p> <p>Parameters:</p> Name Type Description Default <code>ws</code> <code>Tensor</code> <p>Token weights, shape (<code>len(self.trie.decode)</code>,).</p> required <code>mode</code> <code>TrieMode</code> <p>Trie mode determining EOS handling. Defaults to WITHOUT_EOS.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>The calculated mass sums for the given distribution.</p> Source code in <code>genlm/bytes/trie.py</code> <pre><code>async def weight_sum(self, ws, mode=None):\n    \"\"\"Queue a `weight_sum` request. Multiple concurrent calls will be automatically batched\n    together by (operation, mode) pairs.\n\n    Args:\n        ws (torch.Tensor): Token weights, shape (`len(self.trie.decode)`,).\n        mode (TrieMode, optional): Trie mode determining EOS handling. Defaults to WITHOUT_EOS.\n\n    Returns:\n        (np.ndarray): The calculated mass sums for the given distribution.\n    \"\"\"\n    mode = mode or TrieMode.WITHOUT_EOS\n    return await self._queue_request(ws, mode, TrieOp.SUM)\n</code></pre>"},{"location":"reference/genlm/bytes/__init__/#genlm.bytes.AsyncTokenByteTrie.weight_max","title":"<code>weight_max(ws)</code>  <code>async</code>","text":"<p>Queue a <code>weight_max</code> request. Multiple concurrent calls will be automatically batched together.</p> <p>Parameters:</p> Name Type Description Default <code>ws</code> <code>Tensor</code> <p>Token weights, shape (<code>len(self.trie.decode)</code>,).</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>The calculated max weights for the given distribution.</p> Source code in <code>genlm/bytes/trie.py</code> <pre><code>async def weight_max(self, ws):\n    \"\"\"Queue a `weight_max` request. Multiple concurrent calls will be automatically batched\n    together.\n\n    Args:\n        ws (torch.Tensor): Token weights, shape (`len(self.trie.decode)`,).\n\n    Returns:\n        (np.ndarray): The calculated max weights for the given distribution.\n    \"\"\"\n    # For MAX, mode doesn't matter so use WITHOUT_EOS as default\n    return await self._queue_request(ws, TrieMode.WITHOUT_EOS, TrieOp.MAX)\n</code></pre>"},{"location":"reference/genlm/bytes/__init__/#genlm.bytes.AsyncTokenByteTrie.start","title":"<code>start()</code>","text":"<p>Start the background processing task if not already running.</p> Source code in <code>genlm/bytes/trie.py</code> <pre><code>def start(self):\n    \"\"\"Start the background processing task if not already running.\"\"\"\n    if not self._task or self._task.done():\n        logger.debug(\"starting background loop\")\n        # Create a new queue so that it is bound to the current event loop\n        self._queue = asyncio.Queue()\n        self._task = asyncio.create_task(self._background_loop())\n</code></pre>"},{"location":"reference/genlm/bytes/__init__/#genlm.bytes.AsyncTokenByteTrie.cleanup","title":"<code>cleanup()</code>  <code>async</code>","text":"<p>Async cleanup - preferred method</p> Source code in <code>genlm/bytes/trie.py</code> <pre><code>async def cleanup(self):\n    \"\"\"Async cleanup - preferred method\"\"\"\n    if self._task and not self._task.done():\n        self._task.cancel()\n        try:\n            await self._task\n        except asyncio.CancelledError:\n            pass\n        self._task = None\n</code></pre>"},{"location":"reference/genlm/bytes/__init__/#genlm.bytes.AsyncTokenByteTrie.shutdown","title":"<code>shutdown()</code>","text":"<p>Stop the background processing task and cleanup resources.</p> Source code in <code>genlm/bytes/trie.py</code> <pre><code>def shutdown(self):\n    \"\"\"Stop the background processing task and cleanup resources.\"\"\"\n    if self._task is not None:\n        try:\n            self._task.cancel()\n        except RuntimeError:  # pragma: no cover\n            # Ignore runtime errors that might occur if event loop is closed\n            pass\n        self._task = None\n</code></pre>"},{"location":"reference/genlm/bytes/__init__/#genlm.bytes.Chart","title":"<code>Chart</code>","text":"<p>               Bases: <code>dict</code></p> <p>A specialized dictionary for managing probability distributions.</p> <p>Extends dict with operations useful for probability distributions and numeric computations, including arithmetic operations, normalization, and visualization.</p> <p>Parameters:</p> Name Type Description Default <code>zero</code> <code>Any</code> <p>Default value for missing keys</p> required <code>vals</code> <code>tuple</code> <p>Initial (key, value) pairs</p> <code>()</code> Source code in <code>genlm/bytes/util.py</code> <pre><code>class Chart(dict):\n    \"\"\"A specialized dictionary for managing probability distributions.\n\n    Extends dict with operations useful for probability distributions and numeric computations,\n    including arithmetic operations, normalization, and visualization.\n\n    Args:\n        zero (Any): Default value for missing keys\n        vals (tuple, optional): Initial (key, value) pairs\n    \"\"\"\n\n    def __init__(self, zero, vals=()):\n        self.zero = zero\n        super().__init__(vals)\n\n    def __missing__(self, k):\n        return self.zero\n\n    def spawn(self):\n        return Chart(self.zero)\n\n    def __add__(self, other):\n        new = self.spawn()\n        for k, v in self.items():\n            new[k] += v\n        for k, v in other.items():\n            new[k] += v\n        return new\n\n    def __mul__(self, other):\n        new = self.spawn()\n        for k in self:\n            v = self[k] * other[k]\n            if v == self.zero:\n                continue\n            new[k] += v\n        return new\n\n    def copy(self):\n        return Chart(self.zero, self)\n\n    def trim(self):\n        return Chart(self.zero, {k: v for k, v in self.items() if v != self.zero})\n\n    def metric(self, other):\n        assert isinstance(other, Chart)\n        err = 0\n        for x in self.keys() | other.keys():\n            err = max(err, abs(self[x] - other[x]))\n        return err\n\n    def _repr_html_(self):\n        return (\n            '&lt;div style=\"font-family: Monospace;\"&gt;'\n            + format_table(self.trim().items(), headings=[\"key\", \"value\"])\n            + \"&lt;/div&gt;\"\n        )\n\n    def __repr__(self):\n        return repr({k: v for k, v in self.items() if v != self.zero})\n\n    def __str__(self, style_value=lambda k, v: str(v)):\n        def key(k):\n            return -self[k]\n\n        return (\n            \"Chart {\\n\"\n            + \"\\n\".join(\n                f\"  {k!r}: {style_value(k, self[k])},\"\n                for k in sorted(self, key=key)\n                if self[k] != self.zero\n            )\n            + \"\\n}\"\n        )\n\n    def assert_equal(self, want, *, domain=None, tol=1e-5, verbose=False, throw=True):\n        if not isinstance(want, Chart):\n            want = Chart(self.zero, want)\n        if domain is None:\n            domain = self.keys() | want.keys()\n        assert verbose or throw\n        errors = []\n        for x in domain:\n            if abs(self[x] - want[x]) &lt;= tol:\n                if verbose:\n                    print(colors.mark(True), x, self[x])\n            else:\n                if verbose:\n                    print(colors.mark(False), x, self[x], want[x])\n                errors.append(x)\n        if throw:\n            for x in errors:\n                raise AssertionError(f\"{x}: {self[x]} {want[x]}\")\n\n    def argmax(self):\n        return max(self, key=self.__getitem__)\n\n    def argmin(self):\n        return min(self, key=self.__getitem__)\n\n    def top(self, k):\n        return Chart(\n            self.zero,\n            {k: self[k] for k in sorted(self, key=self.__getitem__, reverse=True)[:k]},\n        )\n\n    def max(self):\n        return max(self.values())\n\n    def min(self):\n        return min(self.values())\n\n    def sum(self):\n        return sum(self.values())\n\n    def sort(self, **kwargs):\n        return Chart(self.zero, [(k, self[k]) for k in sorted(self, **kwargs)])\n\n    def sort_descending(self):\n        return Chart(\n            self.zero, [(k, self[k]) for k in sorted(self, key=lambda k: -self[k])]\n        )\n\n    def normalize(self):\n        Z = self.sum()\n        if Z == 0:\n            return self\n        return Chart(self.zero, [(k, v / Z) for k, v in self.items()])\n\n    def filter(self, f):\n        return Chart(self.zero, [(k, v) for k, v in self.items() if f(k)])\n\n    def map_values(self, f):\n        return Chart(f(self.zero), [(k, f(v)) for k, v in self.items()])\n\n    def map_keys(self, f):\n        return Chart(self.zero, [(f(k), v) for k, v in self.items()])\n\n    def project(self, f):\n        \"Apply the function `f` to each key; summing when f-transformed keys overlap.\"\n        out = self.spawn()\n        for k, v in self.items():\n            out[f(k)] += v\n        return out\n\n    # TODO: the more general version of this method is join\n    def compare(self, other, *, domain=None):\n        if not isinstance(other, Chart):\n            other = Chart(self.zero, other)\n        if domain is None:\n            domain = self.keys() | other.keys()\n        rows = []\n        for x in domain:\n            m = abs(self[x] - other[x])\n            rows.append(dict(key=x, self=self[x], other=other[x], metric=m))\n        return pd.DataFrame(rows)\n\n    def to_dict(self):\n        return {k: v for k, v in self.items()}\n</code></pre>"},{"location":"reference/genlm/bytes/__init__/#genlm.bytes.Chart.project","title":"<code>project(f)</code>","text":"<p>Apply the function <code>f</code> to each key; summing when f-transformed keys overlap.</p> Source code in <code>genlm/bytes/util.py</code> <pre><code>def project(self, f):\n    \"Apply the function `f` to each key; summing when f-transformed keys overlap.\"\n    out = self.spawn()\n    for k, v in self.items():\n        out[f(k)] += v\n    return out\n</code></pre>"},{"location":"reference/genlm/bytes/trie/","title":"trie","text":""},{"location":"reference/genlm/bytes/trie/#genlm.bytes.trie","title":"<code>genlm.bytes.trie</code>","text":""},{"location":"reference/genlm/bytes/trie/#genlm.bytes.trie.TrieMode","title":"<code>TrieMode</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Modes for trie state behavior.</p> Source code in <code>genlm/bytes/trie.py</code> <pre><code>class TrieMode(Enum):\n    \"\"\"Modes for trie state behavior.\"\"\"\n\n    WITHOUT_EOS = \"without_eos\"  # EOS tokens are treated as normal tokens\n    WITH_EOS = \"with_eos\"  # EOS tokens get special handling (aggregated to EOS node)\n</code></pre>"},{"location":"reference/genlm/bytes/trie/#genlm.bytes.trie.TokenByteTrie","title":"<code>TokenByteTrie</code>","text":"<p>A trie data structure for efficient token-to-byte mapping.</p> Source code in <code>genlm/bytes/trie.py</code> <pre><code>class TokenByteTrie:\n    \"\"\"A trie data structure for efficient token-to-byte mapping.\"\"\"\n\n    def __init__(\n        self,\n        decode,\n        device=None,\n        atomic_tokens=None,\n        eot_token=None,\n        eos_tokens=None,\n        max_batch_size=64,\n    ):\n        \"\"\"Initialize a `TokenByteTrie`.\n\n        Args:\n            decode (list[bytes]): List representing the token vocabulary.\n            device (str, optional): Device to use for weight sum and max computations ('cpu' or 'cuda').\n            atomic_tokens (list[bytes], optional): List of tokens that should be treated as atomic units rather than being split into bytes.\n            eot_token (bytes|None, optional): End-of-token token. Default is None, which represents EOT as None.\n            eos_tokens (set[bytes], optional): Set of tokens that should be treated as EOS (End of Sequence).\n            max_batch_size (int, optional): Maximum batch size for weight sum sparse matrix multiplication.\n        \"\"\"\n        self.decode = decode\n        self.max_batch_size = max_batch_size\n\n        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        if self.device not in [\"cpu\", \"cuda\"]:\n            raise ValueError(f\"Invalid device: {device}. Must be 'cpu', 'cuda' or None\")\n\n        self.eot_token = eot_token\n        self.eos_tokens = set(eos_tokens or [])\n        self.eos_token_ids = [\n            i for i, token in enumerate(decode) if token in self.eos_tokens\n        ]\n\n        self._build_trie(atomic_tokens or [])\n        self._renumber()\n        self._build_node2prefix()\n        self._build_reachability_matrix()\n        self.token_ids = torch.tensor(\n            self.token_id_to_leaf[:, 0], dtype=torch.long, device=self.device\n        )\n\n    def _build_trie(self, atomic_tokens):\n        \"\"\"Builds a trie data structure from the vocabulary.\n\n        Returns:\n            (dict): A dictionary where keys are token IDs and values are lists of characters.\n        \"\"\"\n        for token in atomic_tokens:\n            if token not in self.decode:\n                raise ValueError(f\"Atomic token {token} not in vocabulary\")\n\n        for token in self.eos_tokens:\n            if token not in self.decode:\n                raise ValueError(f\"EOS token {token} not in vocabulary\")\n\n        self.word2leaf = {}\n        self.children = [{}]  # First node is root\n        self.root = 0\n        self.token_id_to_leaf = []\n        self.lookup = {}\n\n        for token_id, word in enumerate(self.decode):\n            if word in self.lookup:\n                raise ValueError(f\"Duplicate word in vocabulary: {word}\")\n            self.lookup[word] = token_id\n\n            # Build ALL tokens in trie (including EOS tokens for conditioning mode)\n            curr = self.root\n            letters = [word] if word in atomic_tokens else word\n            for letter in letters:\n                if letter not in self.children[curr]:\n                    self.children[curr][letter] = len(self.children)\n                    self.children.append({})\n                curr = self.children[curr][letter]\n\n            self.children[curr][self.eot_token] = last = len(self.children)\n            self.children.append({})\n            assert word not in self.word2leaf\n            self.word2leaf[word] = last\n            self.token_id_to_leaf.append((token_id, last))\n\n        self.eos_node = len(self.children)\n        self.children.append({})  # Create the EOS node\n        self.children[self.root][EOS] = self.eos_node\n\n        self.leaf2word = dict(zip(self.word2leaf.values(), self.word2leaf.keys()))\n        self.jump = [\n            np.array(sorted(x.values()), dtype=np.int32) for x in self.children\n        ]\n\n    def _renumber(self):\n        \"\"\"Renumber the states of the trie so that they are named by a contiguous\n        range of integers and those integers respect the topological ordering\n        of the trie. This improves the efficiency of the updating the trie as\n        it improves memory locality.\n        \"\"\"\n        self.ordering = np.array(list(self._order(self.root)), np.int32)\n        ordering = {}\n        for i, x in enumerate(self._order_full(self.root)):\n            ordering[x] = i\n        self._rename(f=lambda x: ordering[x])\n\n    def _order(self, node):\n        \"\"\"Generate a topological ordering of nodes beneath the given node.\n\n        Args:\n            node (int): Starting node index\n\n        Yields:\n            int: Node indices in topological order\n        \"\"\"\n        for a in self.children[node]:\n            if a is not None:\n                yield from self._order(self.children[node][a])\n        yield node\n\n    def _order_full(self, node):\n        \"\"\"Generate a complete topological ordering including all child nodes.\n\n        Args:\n            node (int): Starting node index\n\n        Yields:\n            (int): Node indices in complete topological order\n        \"\"\"\n        for a in self.children[node]:\n            yield from self._order_full(self.children[node][a])\n        yield node\n\n    def _rename(self, f):\n        \"\"\"Rename all node indices in the trie using the provided mapping function.\n\n        Args:\n            f (callable): Function that maps old node indices to new node indices\n        \"\"\"\n        N = len(self.children)\n\n        new_children = [{} for _ in range(N)]\n        nodes = range(N)\n\n        for x in nodes:\n            for letter, y in self.children[x].items():\n                new_children[f(x)][letter] = f(y)\n\n        self.root = f(self.root)\n        self.children = new_children\n        self.word2leaf = {w: f(x) for w, x in self.word2leaf.items()}\n        self.leaf2word = dict(zip(self.word2leaf.values(), self.word2leaf.keys()))\n\n        self.token_id_to_leaf = np.array(\n            [(i, f(x)) for i, x in self.token_id_to_leaf], dtype=np.int32\n        )\n        self.leaf2token_id = dict(\n            zip(self.token_id_to_leaf[:, 1], self.token_id_to_leaf[:, 0])\n        )\n\n        self.ordering = np.array([f(x) for x in self.ordering])\n        self.jump = [np.array(sorted(x.values()), dtype=np.int32) for x in new_children]\n\n        # Update EOS node after renumbering\n        self.eos_node = f(self.eos_node)\n\n    def _build_node2prefix(self):\n        \"\"\"Builds a mapping from each node to its prefix.\n\n        Returns:\n            (dict): A dictionary where keys are node IDs and values are lists of characters.\n        \"\"\"\n        node2prefix = {self.root: []}\n        for x in reversed(range(len(self.children))):\n            for letter, y in self.children[x].items():\n                if letter is None:\n                    node2prefix[y] = node2prefix[x]\n                elif isinstance(letter, bytes):\n                    node2prefix[y] = node2prefix[x] + list(letter)\n                else:\n                    node2prefix[y] = node2prefix[x] + [letter]\n\n        self.node2prefix = node2prefix\n\n    def _build_parent_map(self):\n        \"\"\"Builds a mapping from each node to its parent node in the trie.\n\n        Returns:\n            (dict): A dictionary where keys are child nodes and values are their parent nodes.\n        \"\"\"\n        parent = {}\n        for node in range(len(self.children)):\n            for child in self.jump[node]:\n                parent[child] = node\n        return parent\n\n    def _build_reachability_matrix(self):\n        \"\"\"Constructs dual sparse reachability matrices for efficient weight propagation.\n\n        The matrix M is constructed such that M[i,j] = 1 if node j is either:\n        - The leaf node i itself (self-connection)\n        - An ancestor of leaf node i in the trie\n\n        For propagate_eos mode, EOS tokens contribute directly to eos_node and root.\n        \"\"\"\n        leaf_indices = self.token_id_to_leaf[:, 1]\n        parent = self._build_parent_map()\n        # Build no_eos matrix (includes all tokens, doesn't map any tokens to the eos_node)\n        rows_no_eos, cols_no_eos = [], []\n        # Build with_eos matrix (maps EOS tokens to the eos_node only)\n        rows_with_eos, cols_with_eos = [], []\n\n        for i, node in enumerate(leaf_indices):\n            token_id = self.token_id_to_leaf[i, 0]\n            token = self.decode[token_id]\n\n            # self-connection\n            rows_no_eos.append(i)\n            cols_no_eos.append(node)\n            if token not in self.eos_tokens:\n                rows_with_eos.append(i)\n                cols_with_eos.append(node)\n            else:\n                # EOS tokens: contribute directly to eos_node and root\n                rows_with_eos.append(i)\n                cols_with_eos.append(self.eos_node)\n                rows_with_eos.append(i)\n                cols_with_eos.append(self.root)\n\n            current = node\n            while current in parent:\n                ancestor = parent[current]\n                rows_no_eos.append(i)\n                cols_no_eos.append(ancestor)\n                if token not in self.eos_tokens:\n                    rows_with_eos.append(i)\n                    cols_with_eos.append(ancestor)\n                current = ancestor\n\n        # Build without_eos matrix\n        indices_no_eos = torch.tensor(\n            [rows_no_eos, cols_no_eos], dtype=torch.long, device=self.device\n        )\n        values_no_eos = torch.ones(len(rows_no_eos), device=self.device)\n        self.M_no_eos = torch.sparse_coo_tensor(\n            indices_no_eos, values_no_eos, (len(leaf_indices), len(self.children))\n        ).to_sparse_csr()\n\n        # Build with_eos matrix\n        indices_with_eos = torch.tensor(\n            [rows_with_eos, cols_with_eos], dtype=torch.long, device=self.device\n        )\n        values_with_eos = torch.ones(len(rows_with_eos), device=self.device)\n        self.M_with_eos = torch.sparse_coo_tensor(\n            indices_with_eos, values_with_eos, (len(leaf_indices), len(self.children))\n        ).to_sparse_csr()\n\n        # Keep the old matrix for backward compatibility\n        self.M = self.M_no_eos\n        self.src_indices = torch.tensor(\n            rows_no_eos, dtype=torch.long, device=self.device\n        )\n        self.dst_indices = torch.tensor(\n            cols_no_eos, dtype=torch.long, device=self.device\n        )\n\n    def _preprocess_ws(self, batch_ws):\n        \"\"\"Preprocess weight sums for batch processing.\n\n        Args:\n            batch_ws (list|np.ndarray|torch.Tensor): List of weight sum tensors or lists of weight sums.\n\n        Returns:\n            (torch.Tensor): Stacked weight sum tensor.\n        \"\"\"\n        processed_batch_ws = []\n        for ws in batch_ws:\n            if not isinstance(ws, torch.Tensor):\n                ws = torch.tensor(ws, device=self.device, dtype=torch.float32)\n            elif ws.device != self.device or ws.dtype != torch.float32:\n                ws = ws.to(device=self.device, dtype=torch.float32)\n            assert ws.shape[0] == len(self.decode), [ws.shape[0], len(self.decode)]\n            processed_batch_ws.append(ws)\n        return torch.stack(processed_batch_ws)\n\n    def weight_sum(self, ws, mode=None):\n        \"\"\"Computes the sum of weights of all leaf nodes (tokens) that are descendants of each node in the trie.\n\n        Args:\n            ws (torch.Tensor): Token weights, shape (`len(self.decode)`,).\n            mode (TrieMode, optional): Trie mode - determines matrix selection.\n                                     If None, defaults to WITHOUT_EOS.\n\n        Returns:\n            (numpy.ndarray): Summed weights for each node in the trie, shape (num_nodes,).\n        \"\"\"\n        mode = mode or TrieMode.WITHOUT_EOS\n        return self.batch_weight_sum(self._preprocess_ws([ws]), mode=mode)[0]\n\n    def batch_weight_sum(self, ws, mode=None):\n        \"\"\"Batch version of `weight_sum`.\n\n        Args:\n            ws (torch.Tensor): Batch of token weights, shape (batch_size \u00d7 `len(self.decode)`).\n            mode (TrieMode, optional): Trie mode - determines matrix selection.\n                                     If None, defaults to WITHOUT_EOS.\n\n        Returns:\n            (numpy.ndarray): Summed weights for each node in the trie, shape (batch_size \u00d7 num_nodes).\n        \"\"\"\n        mode = mode or TrieMode.WITHOUT_EOS\n\n        ws = self._preprocess_ws(ws)\n        batch_size = ws.shape[0]\n        all_masses = []\n\n        # Choose matrix based on mode\n        matrix = self.M_with_eos if mode == TrieMode.WITH_EOS else self.M_no_eos\n\n        # If you are getting illegal memory access errors here,\n        # try reducing the max_batch_size.\n        for i in range(0, batch_size, self.max_batch_size):\n            batch_ws = ws[i : i + self.max_batch_size]\n            masses = torch.sparse.mm(batch_ws[:, self.token_ids], matrix)\n            all_masses.append(masses)\n        return torch.cat(all_masses, dim=0)\n\n    def weight_max(self, ws):\n        \"\"\"Computes the maximum weight of all descendant leaf nodes (tokens) for each node in the trie.\n\n        Args:\n            ws (torch.Tensor): Token weights, shape (`len(self.decode)`,).\n\n        Returns:\n            (numpy.ndarray): Maximum weights for each node in the trie, shape (num_nodes,).\n        \"\"\"\n        return self.batch_weight_max(self._preprocess_ws([ws]))[0]\n\n    def batch_weight_max(self, ws):\n        \"\"\"Batch version of `weight_max`.\n\n        Args:\n            ws (torch.Tensor): Batch of token weights, shape (batch_size \u00d7 `len(self.decode)`).\n\n        Returns:\n            (numpy.ndarray): Maximum weights for each node in the trie, shape (batch_size \u00d7 num_nodes).\n        \"\"\"\n        ws = self._preprocess_ws(ws)\n\n        # Get leaf weights\n        leaf_weights = ws[:, self.token_ids]  # shape: (batch_size \u00d7 num_leafs)\n        batch_size = leaf_weights.shape[0]\n\n        # Use scatter_reduce to propagate maximum values in parallel\n        result = torch.zeros((batch_size, len(self.children)), device=self.device)\n        result.scatter_reduce_(\n            dim=1,\n            index=self.dst_indices.expand(batch_size, -1),\n            src=leaf_weights[:, self.src_indices],\n            reduce=\"amax\",\n            include_self=False,\n        )\n\n        return result\n\n    def visualize(self, ws=None):\n        \"\"\"Visualize the trie structure using Graphviz.\n\n        Args:\n            ws (np.ndarray|None): Optional weight vector to display at each node. Should be of length `len(self.children)`.\n\n        Returns:\n            (graphviz.Digraph): The generated graph object\n        \"\"\"\n        try:\n            import graphviz\n        except ImportError:  # pragma: no cover\n            raise ImportError(\n                \"Please install graphviz: pip install graphviz\"\n            )  # pragma: no cover\n\n        if ws is not None and len(ws) != len(self.children):\n            raise ValueError(\n                f\"Weight vector length ({len(ws)}) must match number of nodes ({len(self.children)})\"\n            )\n\n        dot = graphviz.Digraph(comment=\"Token Character Trie\")\n        dot.attr(rankdir=\"LR\")\n\n        # Create a subgraph for the legend\n        with dot.subgraph(name=\"cluster_legend\") as legend:\n            legend.attr(label=\"Legend\", fontsize=\"10\")\n            legend.attr(\"node\", fontsize=\"7\", width=\"0.1\", height=\"0.1\")\n\n            # Example internal node\n            legend.node(\n                \"legend_internal\",\n                \"Internal Node ID\\n'Prefix'\\nWeight (if provided)\",\n                shape=\"circle\",\n            )\n\n            # Example leaf node\n            legend.node(\"legend_leaf\", \"Complete Token\", shape=\"doublecircle\")\n\n            legend.edge(\n                \"legend_internal\",\n                \"legend_leaf\",\n                label=\"Token item\",\n                fontsize=\"10\",\n            )\n\n            # Align legend horizontally\n            legend.attr(rankdir=\"TB\")\n            legend.attr(rank=\"same\")\n\n        # Add the main trie nodes and edges\n        for node_id in range(len(self.children)):\n            prefix = self.node2prefix[node_id]\n\n            if ws is not None:\n                label = f\"{node_id}\\n'{prefix}'\\n{ws[node_id]:.4f}\"\n            else:\n                label = f\"{node_id}\\n'{prefix}'\"\n\n            # Color nodes based on mass if provided\n            if ws is not None:\n                max_ws = ws.max()\n                if max_ws &gt; 0:\n                    intensity = int(255 * (1 - ws[node_id] / max_ws))\n                    color = f\"#{intensity:02x}{255:02x}{intensity:02x}\"\n                else:\n                    color = \"#ffffff\"  # white for zero mass\n            else:\n                color = \"#ffffff\"  # default white\n\n            if node_id in self.leaf2word:\n                dot.node(\n                    str(node_id),\n                    label,\n                    shape=\"doublecircle\",\n                    style=\"filled\",\n                    fillcolor=color,\n                )\n            else:\n                dot.node(\n                    str(node_id), label, shape=\"circle\", style=\"filled\", fillcolor=color\n                )\n\n        for node_id, children in enumerate(self.children):\n            for char, child_id in children.items():\n                if char is not None:\n                    edge_label = str(char)\n                else:\n                    edge_label = \"End-of-Token\"\n\n                dot.edge(str(node_id), str(child_id), label=edge_label)\n\n        return dot\n</code></pre>"},{"location":"reference/genlm/bytes/trie/#genlm.bytes.trie.TokenByteTrie.__init__","title":"<code>__init__(decode, device=None, atomic_tokens=None, eot_token=None, eos_tokens=None, max_batch_size=64)</code>","text":"<p>Initialize a <code>TokenByteTrie</code>.</p> <p>Parameters:</p> Name Type Description Default <code>decode</code> <code>list[bytes]</code> <p>List representing the token vocabulary.</p> required <code>device</code> <code>str</code> <p>Device to use for weight sum and max computations ('cpu' or 'cuda').</p> <code>None</code> <code>atomic_tokens</code> <code>list[bytes]</code> <p>List of tokens that should be treated as atomic units rather than being split into bytes.</p> <code>None</code> <code>eot_token</code> <code>bytes | None</code> <p>End-of-token token. Default is None, which represents EOT as None.</p> <code>None</code> <code>eos_tokens</code> <code>set[bytes]</code> <p>Set of tokens that should be treated as EOS (End of Sequence).</p> <code>None</code> <code>max_batch_size</code> <code>int</code> <p>Maximum batch size for weight sum sparse matrix multiplication.</p> <code>64</code> Source code in <code>genlm/bytes/trie.py</code> <pre><code>def __init__(\n    self,\n    decode,\n    device=None,\n    atomic_tokens=None,\n    eot_token=None,\n    eos_tokens=None,\n    max_batch_size=64,\n):\n    \"\"\"Initialize a `TokenByteTrie`.\n\n    Args:\n        decode (list[bytes]): List representing the token vocabulary.\n        device (str, optional): Device to use for weight sum and max computations ('cpu' or 'cuda').\n        atomic_tokens (list[bytes], optional): List of tokens that should be treated as atomic units rather than being split into bytes.\n        eot_token (bytes|None, optional): End-of-token token. Default is None, which represents EOT as None.\n        eos_tokens (set[bytes], optional): Set of tokens that should be treated as EOS (End of Sequence).\n        max_batch_size (int, optional): Maximum batch size for weight sum sparse matrix multiplication.\n    \"\"\"\n    self.decode = decode\n    self.max_batch_size = max_batch_size\n\n    self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    if self.device not in [\"cpu\", \"cuda\"]:\n        raise ValueError(f\"Invalid device: {device}. Must be 'cpu', 'cuda' or None\")\n\n    self.eot_token = eot_token\n    self.eos_tokens = set(eos_tokens or [])\n    self.eos_token_ids = [\n        i for i, token in enumerate(decode) if token in self.eos_tokens\n    ]\n\n    self._build_trie(atomic_tokens or [])\n    self._renumber()\n    self._build_node2prefix()\n    self._build_reachability_matrix()\n    self.token_ids = torch.tensor(\n        self.token_id_to_leaf[:, 0], dtype=torch.long, device=self.device\n    )\n</code></pre>"},{"location":"reference/genlm/bytes/trie/#genlm.bytes.trie.TokenByteTrie.weight_sum","title":"<code>weight_sum(ws, mode=None)</code>","text":"<p>Computes the sum of weights of all leaf nodes (tokens) that are descendants of each node in the trie.</p> <p>Parameters:</p> Name Type Description Default <code>ws</code> <code>Tensor</code> <p>Token weights, shape (<code>len(self.decode)</code>,).</p> required <code>mode</code> <code>TrieMode</code> <p>Trie mode - determines matrix selection.                      If None, defaults to WITHOUT_EOS.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Summed weights for each node in the trie, shape (num_nodes,).</p> Source code in <code>genlm/bytes/trie.py</code> <pre><code>def weight_sum(self, ws, mode=None):\n    \"\"\"Computes the sum of weights of all leaf nodes (tokens) that are descendants of each node in the trie.\n\n    Args:\n        ws (torch.Tensor): Token weights, shape (`len(self.decode)`,).\n        mode (TrieMode, optional): Trie mode - determines matrix selection.\n                                 If None, defaults to WITHOUT_EOS.\n\n    Returns:\n        (numpy.ndarray): Summed weights for each node in the trie, shape (num_nodes,).\n    \"\"\"\n    mode = mode or TrieMode.WITHOUT_EOS\n    return self.batch_weight_sum(self._preprocess_ws([ws]), mode=mode)[0]\n</code></pre>"},{"location":"reference/genlm/bytes/trie/#genlm.bytes.trie.TokenByteTrie.batch_weight_sum","title":"<code>batch_weight_sum(ws, mode=None)</code>","text":"<p>Batch version of <code>weight_sum</code>.</p> <p>Parameters:</p> Name Type Description Default <code>ws</code> <code>Tensor</code> <p>Batch of token weights, shape (batch_size \u00d7 <code>len(self.decode)</code>).</p> required <code>mode</code> <code>TrieMode</code> <p>Trie mode - determines matrix selection.                      If None, defaults to WITHOUT_EOS.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Summed weights for each node in the trie, shape (batch_size \u00d7 num_nodes).</p> Source code in <code>genlm/bytes/trie.py</code> <pre><code>def batch_weight_sum(self, ws, mode=None):\n    \"\"\"Batch version of `weight_sum`.\n\n    Args:\n        ws (torch.Tensor): Batch of token weights, shape (batch_size \u00d7 `len(self.decode)`).\n        mode (TrieMode, optional): Trie mode - determines matrix selection.\n                                 If None, defaults to WITHOUT_EOS.\n\n    Returns:\n        (numpy.ndarray): Summed weights for each node in the trie, shape (batch_size \u00d7 num_nodes).\n    \"\"\"\n    mode = mode or TrieMode.WITHOUT_EOS\n\n    ws = self._preprocess_ws(ws)\n    batch_size = ws.shape[0]\n    all_masses = []\n\n    # Choose matrix based on mode\n    matrix = self.M_with_eos if mode == TrieMode.WITH_EOS else self.M_no_eos\n\n    # If you are getting illegal memory access errors here,\n    # try reducing the max_batch_size.\n    for i in range(0, batch_size, self.max_batch_size):\n        batch_ws = ws[i : i + self.max_batch_size]\n        masses = torch.sparse.mm(batch_ws[:, self.token_ids], matrix)\n        all_masses.append(masses)\n    return torch.cat(all_masses, dim=0)\n</code></pre>"},{"location":"reference/genlm/bytes/trie/#genlm.bytes.trie.TokenByteTrie.weight_max","title":"<code>weight_max(ws)</code>","text":"<p>Computes the maximum weight of all descendant leaf nodes (tokens) for each node in the trie.</p> <p>Parameters:</p> Name Type Description Default <code>ws</code> <code>Tensor</code> <p>Token weights, shape (<code>len(self.decode)</code>,).</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Maximum weights for each node in the trie, shape (num_nodes,).</p> Source code in <code>genlm/bytes/trie.py</code> <pre><code>def weight_max(self, ws):\n    \"\"\"Computes the maximum weight of all descendant leaf nodes (tokens) for each node in the trie.\n\n    Args:\n        ws (torch.Tensor): Token weights, shape (`len(self.decode)`,).\n\n    Returns:\n        (numpy.ndarray): Maximum weights for each node in the trie, shape (num_nodes,).\n    \"\"\"\n    return self.batch_weight_max(self._preprocess_ws([ws]))[0]\n</code></pre>"},{"location":"reference/genlm/bytes/trie/#genlm.bytes.trie.TokenByteTrie.batch_weight_max","title":"<code>batch_weight_max(ws)</code>","text":"<p>Batch version of <code>weight_max</code>.</p> <p>Parameters:</p> Name Type Description Default <code>ws</code> <code>Tensor</code> <p>Batch of token weights, shape (batch_size \u00d7 <code>len(self.decode)</code>).</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Maximum weights for each node in the trie, shape (batch_size \u00d7 num_nodes).</p> Source code in <code>genlm/bytes/trie.py</code> <pre><code>def batch_weight_max(self, ws):\n    \"\"\"Batch version of `weight_max`.\n\n    Args:\n        ws (torch.Tensor): Batch of token weights, shape (batch_size \u00d7 `len(self.decode)`).\n\n    Returns:\n        (numpy.ndarray): Maximum weights for each node in the trie, shape (batch_size \u00d7 num_nodes).\n    \"\"\"\n    ws = self._preprocess_ws(ws)\n\n    # Get leaf weights\n    leaf_weights = ws[:, self.token_ids]  # shape: (batch_size \u00d7 num_leafs)\n    batch_size = leaf_weights.shape[0]\n\n    # Use scatter_reduce to propagate maximum values in parallel\n    result = torch.zeros((batch_size, len(self.children)), device=self.device)\n    result.scatter_reduce_(\n        dim=1,\n        index=self.dst_indices.expand(batch_size, -1),\n        src=leaf_weights[:, self.src_indices],\n        reduce=\"amax\",\n        include_self=False,\n    )\n\n    return result\n</code></pre>"},{"location":"reference/genlm/bytes/trie/#genlm.bytes.trie.TokenByteTrie.visualize","title":"<code>visualize(ws=None)</code>","text":"<p>Visualize the trie structure using Graphviz.</p> <p>Parameters:</p> Name Type Description Default <code>ws</code> <code>ndarray | None</code> <p>Optional weight vector to display at each node. Should be of length <code>len(self.children)</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>Digraph</code> <p>The generated graph object</p> Source code in <code>genlm/bytes/trie.py</code> <pre><code>def visualize(self, ws=None):\n    \"\"\"Visualize the trie structure using Graphviz.\n\n    Args:\n        ws (np.ndarray|None): Optional weight vector to display at each node. Should be of length `len(self.children)`.\n\n    Returns:\n        (graphviz.Digraph): The generated graph object\n    \"\"\"\n    try:\n        import graphviz\n    except ImportError:  # pragma: no cover\n        raise ImportError(\n            \"Please install graphviz: pip install graphviz\"\n        )  # pragma: no cover\n\n    if ws is not None and len(ws) != len(self.children):\n        raise ValueError(\n            f\"Weight vector length ({len(ws)}) must match number of nodes ({len(self.children)})\"\n        )\n\n    dot = graphviz.Digraph(comment=\"Token Character Trie\")\n    dot.attr(rankdir=\"LR\")\n\n    # Create a subgraph for the legend\n    with dot.subgraph(name=\"cluster_legend\") as legend:\n        legend.attr(label=\"Legend\", fontsize=\"10\")\n        legend.attr(\"node\", fontsize=\"7\", width=\"0.1\", height=\"0.1\")\n\n        # Example internal node\n        legend.node(\n            \"legend_internal\",\n            \"Internal Node ID\\n'Prefix'\\nWeight (if provided)\",\n            shape=\"circle\",\n        )\n\n        # Example leaf node\n        legend.node(\"legend_leaf\", \"Complete Token\", shape=\"doublecircle\")\n\n        legend.edge(\n            \"legend_internal\",\n            \"legend_leaf\",\n            label=\"Token item\",\n            fontsize=\"10\",\n        )\n\n        # Align legend horizontally\n        legend.attr(rankdir=\"TB\")\n        legend.attr(rank=\"same\")\n\n    # Add the main trie nodes and edges\n    for node_id in range(len(self.children)):\n        prefix = self.node2prefix[node_id]\n\n        if ws is not None:\n            label = f\"{node_id}\\n'{prefix}'\\n{ws[node_id]:.4f}\"\n        else:\n            label = f\"{node_id}\\n'{prefix}'\"\n\n        # Color nodes based on mass if provided\n        if ws is not None:\n            max_ws = ws.max()\n            if max_ws &gt; 0:\n                intensity = int(255 * (1 - ws[node_id] / max_ws))\n                color = f\"#{intensity:02x}{255:02x}{intensity:02x}\"\n            else:\n                color = \"#ffffff\"  # white for zero mass\n        else:\n            color = \"#ffffff\"  # default white\n\n        if node_id in self.leaf2word:\n            dot.node(\n                str(node_id),\n                label,\n                shape=\"doublecircle\",\n                style=\"filled\",\n                fillcolor=color,\n            )\n        else:\n            dot.node(\n                str(node_id), label, shape=\"circle\", style=\"filled\", fillcolor=color\n            )\n\n    for node_id, children in enumerate(self.children):\n        for char, child_id in children.items():\n            if char is not None:\n                edge_label = str(char)\n            else:\n                edge_label = \"End-of-Token\"\n\n            dot.edge(str(node_id), str(child_id), label=edge_label)\n\n    return dot\n</code></pre>"},{"location":"reference/genlm/bytes/trie/#genlm.bytes.trie.TrieOp","title":"<code>TrieOp</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Enumeration of supported trie operations.</p> Source code in <code>genlm/bytes/trie.py</code> <pre><code>class TrieOp(Enum):\n    \"\"\"Enumeration of supported trie operations.\"\"\"\n\n    SUM = \"sum\"\n    MAX = \"max\"\n</code></pre>"},{"location":"reference/genlm/bytes/trie/#genlm.bytes.trie.AsyncTokenByteTrie","title":"<code>AsyncTokenByteTrie</code>","text":"<p>An asynchronous wrapper for TokenByteTrie implementations that provides automatic request batching.</p> Source code in <code>genlm/bytes/trie.py</code> <pre><code>class AsyncTokenByteTrie:\n    \"\"\"An asynchronous wrapper for TokenByteTrie implementations that provides automatic request batching.\"\"\"\n\n    def __init__(self, trie):\n        \"\"\"Initialize an `AsyncTokenByteTrie`.\n\n        Args:\n            trie (TokenByteTrie): The underlying `TokenByteTrie` instance\n        \"\"\"\n        self.trie = trie\n        self._queue = None\n        self._task = None\n\n    @classmethod\n    def from_vocab(cls, vocab, **kwargs):\n        \"\"\"Creates an `AsyncTokenByteTrie` from a vocabulary.\n\n        Args:\n            vocab (list): The vocabulary over which the trie will be defined.\n            **kwargs (dict): Additional arguments passed to the trie constructor.\n                             Can include 'eos_tokens' for EOS support.\n\n        Returns:\n            (AsyncTokenByteTrie): The initialized asynchronous trie instance.\n        \"\"\"\n        trie = TokenByteTrie(decode=vocab, **kwargs)\n        return cls(trie)\n\n    def _queue_request(self, ws, mode, op):\n        if not self._task or self._task.done():\n            self.start()\n\n        future = asyncio.get_running_loop().create_future()\n        self._queue.put_nowait(((ws, mode), future, op))\n        return future\n\n    async def weight_sum(self, ws, mode=None):\n        \"\"\"Queue a `weight_sum` request. Multiple concurrent calls will be automatically batched\n        together by (operation, mode) pairs.\n\n        Args:\n            ws (torch.Tensor): Token weights, shape (`len(self.trie.decode)`,).\n            mode (TrieMode, optional): Trie mode determining EOS handling. Defaults to WITHOUT_EOS.\n\n        Returns:\n            (np.ndarray): The calculated mass sums for the given distribution.\n        \"\"\"\n        mode = mode or TrieMode.WITHOUT_EOS\n        return await self._queue_request(ws, mode, TrieOp.SUM)\n\n    async def weight_max(self, ws):\n        \"\"\"Queue a `weight_max` request. Multiple concurrent calls will be automatically batched\n        together.\n\n        Args:\n            ws (torch.Tensor): Token weights, shape (`len(self.trie.decode)`,).\n\n        Returns:\n            (np.ndarray): The calculated max weights for the given distribution.\n        \"\"\"\n        # For MAX, mode doesn't matter so use WITHOUT_EOS as default\n        return await self._queue_request(ws, TrieMode.WITHOUT_EOS, TrieOp.MAX)\n\n    def start(self):\n        \"\"\"Start the background processing task if not already running.\"\"\"\n        if not self._task or self._task.done():\n            logger.debug(\"starting background loop\")\n            # Create a new queue so that it is bound to the current event loop\n            self._queue = asyncio.Queue()\n            self._task = asyncio.create_task(self._background_loop())\n\n    async def _background_loop(self):\n        \"\"\"Background task that processes queued weight sum and max requests.\n\n        Continuously monitors the queue for new requests and processes them in batches\n        grouped by (operation, mode) pairs using the underlying trie implementation.\n\n        Raises:\n            (Exception): If any error occurs during processing, it is propagated to all\n                         pending futures in the current batch.\n        \"\"\"\n        while True:\n            try:\n                # Group by (operation, mode) pairs for efficient batching\n                op_mode_groups = defaultdict(list)\n\n                (ws, mode), future, op = await self._queue.get()\n                op_mode_groups[(op, mode)].append(((ws, mode), future))\n\n                try:\n                    while True:\n                        (ws, mode), future, op = self._queue.get_nowait()\n                        op_mode_groups[(op, mode)].append(((ws, mode), future))\n                except asyncio.QueueEmpty:\n                    pass\n\n                for (op, mode), group in op_mode_groups.items():\n                    requests, futures = zip(*group)\n                    # Extract just the ws tensors from the (ws, mode) tuples\n                    ws_list = [req[0] for req in requests]\n\n                    if op == TrieOp.SUM:\n                        if logger.isEnabledFor(logging.DEBUG):\n                            logger.debug(\n                                f\"processing {len(ws_list)} sum requests with mode {mode}\"\n                            )  # pragma: no cover\n                        results = self.trie.batch_weight_sum(ws_list, mode=mode)\n                    elif op == TrieOp.MAX:\n                        if logger.isEnabledFor(logging.DEBUG):\n                            logger.debug(\n                                f\"processing {len(ws_list)} max requests\"\n                            )  # pragma: no cover\n                        # MAX operations don't need mode, so use the original batch_weight_max\n                        results = self.trie.batch_weight_max(ws_list)\n                    else:\n                        raise ValueError(f\"Unknown trie operation: {op}\")\n\n                    for future, result in zip(futures, results):\n                        future.set_result(result)\n\n            except Exception as e:\n                for group in op_mode_groups.values():\n                    for _, future in group:\n                        if not future.done():\n                            future.set_exception(e)\n                raise\n\n    async def cleanup(self):\n        \"\"\"Async cleanup - preferred method\"\"\"\n        if self._task and not self._task.done():\n            self._task.cancel()\n            try:\n                await self._task\n            except asyncio.CancelledError:\n                pass\n            self._task = None\n\n    def shutdown(self):\n        \"\"\"Stop the background processing task and cleanup resources.\"\"\"\n        if self._task is not None:\n            try:\n                self._task.cancel()\n            except RuntimeError:  # pragma: no cover\n                # Ignore runtime errors that might occur if event loop is closed\n                pass\n            self._task = None\n\n    def __del__(self):\n        self.shutdown()\n</code></pre>"},{"location":"reference/genlm/bytes/trie/#genlm.bytes.trie.AsyncTokenByteTrie.__init__","title":"<code>__init__(trie)</code>","text":"<p>Initialize an <code>AsyncTokenByteTrie</code>.</p> <p>Parameters:</p> Name Type Description Default <code>trie</code> <code>TokenByteTrie</code> <p>The underlying <code>TokenByteTrie</code> instance</p> required Source code in <code>genlm/bytes/trie.py</code> <pre><code>def __init__(self, trie):\n    \"\"\"Initialize an `AsyncTokenByteTrie`.\n\n    Args:\n        trie (TokenByteTrie): The underlying `TokenByteTrie` instance\n    \"\"\"\n    self.trie = trie\n    self._queue = None\n    self._task = None\n</code></pre>"},{"location":"reference/genlm/bytes/trie/#genlm.bytes.trie.AsyncTokenByteTrie.from_vocab","title":"<code>from_vocab(vocab, **kwargs)</code>  <code>classmethod</code>","text":"<p>Creates an <code>AsyncTokenByteTrie</code> from a vocabulary.</p> <p>Parameters:</p> Name Type Description Default <code>vocab</code> <code>list</code> <p>The vocabulary over which the trie will be defined.</p> required <code>**kwargs</code> <code>dict</code> <p>Additional arguments passed to the trie constructor.              Can include 'eos_tokens' for EOS support.</p> <code>{}</code> <p>Returns:</p> Type Description <code>AsyncTokenByteTrie</code> <p>The initialized asynchronous trie instance.</p> Source code in <code>genlm/bytes/trie.py</code> <pre><code>@classmethod\ndef from_vocab(cls, vocab, **kwargs):\n    \"\"\"Creates an `AsyncTokenByteTrie` from a vocabulary.\n\n    Args:\n        vocab (list): The vocabulary over which the trie will be defined.\n        **kwargs (dict): Additional arguments passed to the trie constructor.\n                         Can include 'eos_tokens' for EOS support.\n\n    Returns:\n        (AsyncTokenByteTrie): The initialized asynchronous trie instance.\n    \"\"\"\n    trie = TokenByteTrie(decode=vocab, **kwargs)\n    return cls(trie)\n</code></pre>"},{"location":"reference/genlm/bytes/trie/#genlm.bytes.trie.AsyncTokenByteTrie.weight_sum","title":"<code>weight_sum(ws, mode=None)</code>  <code>async</code>","text":"<p>Queue a <code>weight_sum</code> request. Multiple concurrent calls will be automatically batched together by (operation, mode) pairs.</p> <p>Parameters:</p> Name Type Description Default <code>ws</code> <code>Tensor</code> <p>Token weights, shape (<code>len(self.trie.decode)</code>,).</p> required <code>mode</code> <code>TrieMode</code> <p>Trie mode determining EOS handling. Defaults to WITHOUT_EOS.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>The calculated mass sums for the given distribution.</p> Source code in <code>genlm/bytes/trie.py</code> <pre><code>async def weight_sum(self, ws, mode=None):\n    \"\"\"Queue a `weight_sum` request. Multiple concurrent calls will be automatically batched\n    together by (operation, mode) pairs.\n\n    Args:\n        ws (torch.Tensor): Token weights, shape (`len(self.trie.decode)`,).\n        mode (TrieMode, optional): Trie mode determining EOS handling. Defaults to WITHOUT_EOS.\n\n    Returns:\n        (np.ndarray): The calculated mass sums for the given distribution.\n    \"\"\"\n    mode = mode or TrieMode.WITHOUT_EOS\n    return await self._queue_request(ws, mode, TrieOp.SUM)\n</code></pre>"},{"location":"reference/genlm/bytes/trie/#genlm.bytes.trie.AsyncTokenByteTrie.weight_max","title":"<code>weight_max(ws)</code>  <code>async</code>","text":"<p>Queue a <code>weight_max</code> request. Multiple concurrent calls will be automatically batched together.</p> <p>Parameters:</p> Name Type Description Default <code>ws</code> <code>Tensor</code> <p>Token weights, shape (<code>len(self.trie.decode)</code>,).</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>The calculated max weights for the given distribution.</p> Source code in <code>genlm/bytes/trie.py</code> <pre><code>async def weight_max(self, ws):\n    \"\"\"Queue a `weight_max` request. Multiple concurrent calls will be automatically batched\n    together.\n\n    Args:\n        ws (torch.Tensor): Token weights, shape (`len(self.trie.decode)`,).\n\n    Returns:\n        (np.ndarray): The calculated max weights for the given distribution.\n    \"\"\"\n    # For MAX, mode doesn't matter so use WITHOUT_EOS as default\n    return await self._queue_request(ws, TrieMode.WITHOUT_EOS, TrieOp.MAX)\n</code></pre>"},{"location":"reference/genlm/bytes/trie/#genlm.bytes.trie.AsyncTokenByteTrie.start","title":"<code>start()</code>","text":"<p>Start the background processing task if not already running.</p> Source code in <code>genlm/bytes/trie.py</code> <pre><code>def start(self):\n    \"\"\"Start the background processing task if not already running.\"\"\"\n    if not self._task or self._task.done():\n        logger.debug(\"starting background loop\")\n        # Create a new queue so that it is bound to the current event loop\n        self._queue = asyncio.Queue()\n        self._task = asyncio.create_task(self._background_loop())\n</code></pre>"},{"location":"reference/genlm/bytes/trie/#genlm.bytes.trie.AsyncTokenByteTrie.cleanup","title":"<code>cleanup()</code>  <code>async</code>","text":"<p>Async cleanup - preferred method</p> Source code in <code>genlm/bytes/trie.py</code> <pre><code>async def cleanup(self):\n    \"\"\"Async cleanup - preferred method\"\"\"\n    if self._task and not self._task.done():\n        self._task.cancel()\n        try:\n            await self._task\n        except asyncio.CancelledError:\n            pass\n        self._task = None\n</code></pre>"},{"location":"reference/genlm/bytes/trie/#genlm.bytes.trie.AsyncTokenByteTrie.shutdown","title":"<code>shutdown()</code>","text":"<p>Stop the background processing task and cleanup resources.</p> Source code in <code>genlm/bytes/trie.py</code> <pre><code>def shutdown(self):\n    \"\"\"Stop the background processing task and cleanup resources.\"\"\"\n    if self._task is not None:\n        try:\n            self._task.cancel()\n        except RuntimeError:  # pragma: no cover\n            # Ignore runtime errors that might occur if event loop is closed\n            pass\n        self._task = None\n</code></pre>"},{"location":"reference/genlm/bytes/util/","title":"util","text":""},{"location":"reference/genlm/bytes/util/#genlm.bytes.util","title":"<code>genlm.bytes.util</code>","text":""},{"location":"reference/genlm/bytes/util/#genlm.bytes.util.LazyByteProbs","title":"<code>LazyByteProbs</code>","text":"<p>Represents a lazy (log) probability distribution over bytes.</p> <p>Handles probability distributions over 256 possible bytes plus an EOT (End of Token) symbol and a single EOS (End of Sequence) symbol.</p> <p>Parameters:</p> Name Type Description Default <code>ps</code> <code>list</code> <p>List of probabilities (256 bytes + 1 EOT + 1 EOS = 258 total)</p> required <code>log_space</code> <code>bool</code> <p>Whether probabilities are in log space. Defaults to True</p> <code>True</code> Source code in <code>genlm/bytes/util.py</code> <pre><code>class LazyByteProbs:\n    \"\"\"Represents a lazy (log) probability distribution over bytes.\n\n    Handles probability distributions over 256 possible bytes plus an EOT (End of Token) symbol and a single EOS (End of Sequence) symbol.\n\n    Args:\n        ps (list): List of probabilities (256 bytes + 1 EOT + 1 EOS = 258 total)\n        log_space (bool, optional): Whether probabilities are in log space. Defaults to True\n    \"\"\"\n\n    def __init__(self, ps, log_space=True):\n        assert len(ps) == 258  # Fixed size: 256 bytes + 1 EOT + 1 EOS\n        self.ps = ps\n        self.log_space = log_space\n\n    def __getitem__(self, b):\n        \"\"\"Get probability for a byte, EOT, or EOS.\n\n        Args:\n            b (int|None): Byte value, None for EOT, or 257 for EOS\n\n        Returns:\n            (float): Probability (or log probability) for the byte/EOT/EOS\n        \"\"\"\n        if b is None:  # EOT\n            return self.ps[256]\n        elif b == 257:  # EOS token\n            return self.ps[257]\n        elif b &gt;= 258:  # invalid index\n            raise ValueError(\n                f\"Invalid index: {b}. Must be between 0 and 257, or None for EOT.\"\n            )\n        else:  # Regular byte\n            return self.ps[b]\n\n    def materialize(self):\n        \"\"\"Materializes the probability distribution into a Chart.\n\n        Returns:\n            (Chart): Chart with probabilities for each byte/EOT/EOS\n        \"\"\"\n        Q = Chart(-np.inf if self.log_space else 0)\n        # Regular bytes (0-255)\n        for b, p in enumerate(self.ps[:256]):\n            Q[b] = p\n        # EOT (256)\n        Q[None] = self.ps[256]\n        # EOS (257)\n        Q[257] = self.ps[257]\n        return Q\n\n    def pretty(self):\n        \"\"\"Returns a pretty string representation of the probability distribution.\n\n        Returns:\n            (str): Pretty string representation of the probability distribution\n        \"\"\"\n        return self.materialize().map_keys(\n            lambda x: bytes([x])\n            if isinstance(x, int) and 0 &lt;= x &lt;= 255\n            else \"EOS\"\n            if x == 257\n            else \"EOT\"\n        )\n</code></pre>"},{"location":"reference/genlm/bytes/util/#genlm.bytes.util.LazyByteProbs.__getitem__","title":"<code>__getitem__(b)</code>","text":"<p>Get probability for a byte, EOT, or EOS.</p> <p>Parameters:</p> Name Type Description Default <code>b</code> <code>int | None</code> <p>Byte value, None for EOT, or 257 for EOS</p> required <p>Returns:</p> Type Description <code>float</code> <p>Probability (or log probability) for the byte/EOT/EOS</p> Source code in <code>genlm/bytes/util.py</code> <pre><code>def __getitem__(self, b):\n    \"\"\"Get probability for a byte, EOT, or EOS.\n\n    Args:\n        b (int|None): Byte value, None for EOT, or 257 for EOS\n\n    Returns:\n        (float): Probability (or log probability) for the byte/EOT/EOS\n    \"\"\"\n    if b is None:  # EOT\n        return self.ps[256]\n    elif b == 257:  # EOS token\n        return self.ps[257]\n    elif b &gt;= 258:  # invalid index\n        raise ValueError(\n            f\"Invalid index: {b}. Must be between 0 and 257, or None for EOT.\"\n        )\n    else:  # Regular byte\n        return self.ps[b]\n</code></pre>"},{"location":"reference/genlm/bytes/util/#genlm.bytes.util.LazyByteProbs.materialize","title":"<code>materialize()</code>","text":"<p>Materializes the probability distribution into a Chart.</p> <p>Returns:</p> Type Description <code>Chart</code> <p>Chart with probabilities for each byte/EOT/EOS</p> Source code in <code>genlm/bytes/util.py</code> <pre><code>def materialize(self):\n    \"\"\"Materializes the probability distribution into a Chart.\n\n    Returns:\n        (Chart): Chart with probabilities for each byte/EOT/EOS\n    \"\"\"\n    Q = Chart(-np.inf if self.log_space else 0)\n    # Regular bytes (0-255)\n    for b, p in enumerate(self.ps[:256]):\n        Q[b] = p\n    # EOT (256)\n    Q[None] = self.ps[256]\n    # EOS (257)\n    Q[257] = self.ps[257]\n    return Q\n</code></pre>"},{"location":"reference/genlm/bytes/util/#genlm.bytes.util.LazyByteProbs.pretty","title":"<code>pretty()</code>","text":"<p>Returns a pretty string representation of the probability distribution.</p> <p>Returns:</p> Type Description <code>str</code> <p>Pretty string representation of the probability distribution</p> Source code in <code>genlm/bytes/util.py</code> <pre><code>def pretty(self):\n    \"\"\"Returns a pretty string representation of the probability distribution.\n\n    Returns:\n        (str): Pretty string representation of the probability distribution\n    \"\"\"\n    return self.materialize().map_keys(\n        lambda x: bytes([x])\n        if isinstance(x, int) and 0 &lt;= x &lt;= 255\n        else \"EOS\"\n        if x == 257\n        else \"EOT\"\n    )\n</code></pre>"},{"location":"reference/genlm/bytes/util/#genlm.bytes.util.logsumexp","title":"<code>logsumexp(arr)</code>","text":"<p>Compute <code>log(sum(exp(arr)))</code> without overflow.</p> Source code in <code>genlm/bytes/util.py</code> <pre><code>def logsumexp(arr):\n    \"\"\"\n    Compute `log(sum(exp(arr)))` without overflow.\n    \"\"\"\n    arr = np.array(arr, dtype=np.float64)\n    arr = arr[arr &gt; -np.inf]\n    if len(arr) == 0:\n        return -np.inf\n    vmax = arr.max()\n    arr -= vmax\n    np.exp(arr, out=arr)\n    out = np.log(arr.sum())\n    out += vmax\n    return out\n</code></pre>"},{"location":"reference/genlm/bytes/util/#genlm.bytes.util.Chart","title":"<code>Chart</code>","text":"<p>               Bases: <code>dict</code></p> <p>A specialized dictionary for managing probability distributions.</p> <p>Extends dict with operations useful for probability distributions and numeric computations, including arithmetic operations, normalization, and visualization.</p> <p>Parameters:</p> Name Type Description Default <code>zero</code> <code>Any</code> <p>Default value for missing keys</p> required <code>vals</code> <code>tuple</code> <p>Initial (key, value) pairs</p> <code>()</code> Source code in <code>genlm/bytes/util.py</code> <pre><code>class Chart(dict):\n    \"\"\"A specialized dictionary for managing probability distributions.\n\n    Extends dict with operations useful for probability distributions and numeric computations,\n    including arithmetic operations, normalization, and visualization.\n\n    Args:\n        zero (Any): Default value for missing keys\n        vals (tuple, optional): Initial (key, value) pairs\n    \"\"\"\n\n    def __init__(self, zero, vals=()):\n        self.zero = zero\n        super().__init__(vals)\n\n    def __missing__(self, k):\n        return self.zero\n\n    def spawn(self):\n        return Chart(self.zero)\n\n    def __add__(self, other):\n        new = self.spawn()\n        for k, v in self.items():\n            new[k] += v\n        for k, v in other.items():\n            new[k] += v\n        return new\n\n    def __mul__(self, other):\n        new = self.spawn()\n        for k in self:\n            v = self[k] * other[k]\n            if v == self.zero:\n                continue\n            new[k] += v\n        return new\n\n    def copy(self):\n        return Chart(self.zero, self)\n\n    def trim(self):\n        return Chart(self.zero, {k: v for k, v in self.items() if v != self.zero})\n\n    def metric(self, other):\n        assert isinstance(other, Chart)\n        err = 0\n        for x in self.keys() | other.keys():\n            err = max(err, abs(self[x] - other[x]))\n        return err\n\n    def _repr_html_(self):\n        return (\n            '&lt;div style=\"font-family: Monospace;\"&gt;'\n            + format_table(self.trim().items(), headings=[\"key\", \"value\"])\n            + \"&lt;/div&gt;\"\n        )\n\n    def __repr__(self):\n        return repr({k: v for k, v in self.items() if v != self.zero})\n\n    def __str__(self, style_value=lambda k, v: str(v)):\n        def key(k):\n            return -self[k]\n\n        return (\n            \"Chart {\\n\"\n            + \"\\n\".join(\n                f\"  {k!r}: {style_value(k, self[k])},\"\n                for k in sorted(self, key=key)\n                if self[k] != self.zero\n            )\n            + \"\\n}\"\n        )\n\n    def assert_equal(self, want, *, domain=None, tol=1e-5, verbose=False, throw=True):\n        if not isinstance(want, Chart):\n            want = Chart(self.zero, want)\n        if domain is None:\n            domain = self.keys() | want.keys()\n        assert verbose or throw\n        errors = []\n        for x in domain:\n            if abs(self[x] - want[x]) &lt;= tol:\n                if verbose:\n                    print(colors.mark(True), x, self[x])\n            else:\n                if verbose:\n                    print(colors.mark(False), x, self[x], want[x])\n                errors.append(x)\n        if throw:\n            for x in errors:\n                raise AssertionError(f\"{x}: {self[x]} {want[x]}\")\n\n    def argmax(self):\n        return max(self, key=self.__getitem__)\n\n    def argmin(self):\n        return min(self, key=self.__getitem__)\n\n    def top(self, k):\n        return Chart(\n            self.zero,\n            {k: self[k] for k in sorted(self, key=self.__getitem__, reverse=True)[:k]},\n        )\n\n    def max(self):\n        return max(self.values())\n\n    def min(self):\n        return min(self.values())\n\n    def sum(self):\n        return sum(self.values())\n\n    def sort(self, **kwargs):\n        return Chart(self.zero, [(k, self[k]) for k in sorted(self, **kwargs)])\n\n    def sort_descending(self):\n        return Chart(\n            self.zero, [(k, self[k]) for k in sorted(self, key=lambda k: -self[k])]\n        )\n\n    def normalize(self):\n        Z = self.sum()\n        if Z == 0:\n            return self\n        return Chart(self.zero, [(k, v / Z) for k, v in self.items()])\n\n    def filter(self, f):\n        return Chart(self.zero, [(k, v) for k, v in self.items() if f(k)])\n\n    def map_values(self, f):\n        return Chart(f(self.zero), [(k, f(v)) for k, v in self.items()])\n\n    def map_keys(self, f):\n        return Chart(self.zero, [(f(k), v) for k, v in self.items()])\n\n    def project(self, f):\n        \"Apply the function `f` to each key; summing when f-transformed keys overlap.\"\n        out = self.spawn()\n        for k, v in self.items():\n            out[f(k)] += v\n        return out\n\n    # TODO: the more general version of this method is join\n    def compare(self, other, *, domain=None):\n        if not isinstance(other, Chart):\n            other = Chart(self.zero, other)\n        if domain is None:\n            domain = self.keys() | other.keys()\n        rows = []\n        for x in domain:\n            m = abs(self[x] - other[x])\n            rows.append(dict(key=x, self=self[x], other=other[x], metric=m))\n        return pd.DataFrame(rows)\n\n    def to_dict(self):\n        return {k: v for k, v in self.items()}\n</code></pre>"},{"location":"reference/genlm/bytes/util/#genlm.bytes.util.Chart.project","title":"<code>project(f)</code>","text":"<p>Apply the function <code>f</code> to each key; summing when f-transformed keys overlap.</p> Source code in <code>genlm/bytes/util.py</code> <pre><code>def project(self, f):\n    \"Apply the function `f` to each key; summing when f-transformed keys overlap.\"\n    out = self.spawn()\n    for k, v in self.items():\n        out[f(k)] += v\n    return out\n</code></pre>"},{"location":"reference/genlm/bytes/util/#genlm.bytes.util.format_byte","title":"<code>format_byte(byte_val)</code>","text":"<p>Format a byte value for display/debugging.</p> <p>Parameters:</p> Name Type Description Default <code>byte_val</code> <code>int</code> <p>Integer byte value (0-255 for normal bytes, 256 for EOT, 257 for EOS)</p> required <p>Returns:</p> Type Description <code>str</code> <p>String representation like \"b'A'\" for normal bytes or \"256\" for special values</p> Source code in <code>genlm/bytes/util.py</code> <pre><code>def format_byte(byte_val: int) -&gt; str:\n    \"\"\"Format a byte value for display/debugging.\n\n    Args:\n        byte_val: Integer byte value (0-255 for normal bytes, 256 for EOT, 257 for EOS)\n\n    Returns:\n        String representation like \"b'A'\" for normal bytes or \"256\" for special values\n    \"\"\"\n    try:\n        return repr(bytes([byte_val])) if 0 &lt;= byte_val &lt;= 255 else str(byte_val)\n    except Exception:\n        return str(byte_val)\n</code></pre>"},{"location":"reference/genlm/bytes/byte_lm/__init__/","title":"byte_lm","text":""},{"location":"reference/genlm/bytes/byte_lm/__init__/#genlm.bytes.byte_lm","title":"<code>genlm.bytes.byte_lm</code>","text":""},{"location":"reference/genlm/bytes/byte_lm/__init__/#genlm.bytes.byte_lm.ByteBeamState","title":"<code>ByteBeamState</code>","text":"<p>               Bases: <code>StatefulByteLM</code></p> <p>Represents the state of the beam during byte-level language modeling.</p> <p>Tracks multiple candidate states and their probabilities, pruning low-probability candidates.</p> <p>Parameters:</p> Name Type Description Default <code>states</code> <code>list[LazyTrieState]</code> <p>List of candidate states to track</p> required <code>params</code> <code>BeamParams</code> <p>Parameters controlling beam search behavior</p> required Source code in <code>genlm/bytes/byte_lm/beam.py</code> <pre><code>class ByteBeamState(StatefulByteLM):\n    \"\"\"Represents the state of the beam during byte-level language modeling.\n\n    Tracks multiple candidate states and their probabilities, pruning low-probability\n    candidates.\n\n    Args:\n        states (list[LazyTrieState]): List of candidate states to track\n        params (BeamParams): Parameters controlling beam search behavior\n    \"\"\"\n\n    def __init__(self, states, params):\n        self.states = sorted(states, key=lambda b: -b.weight)\n        self.params = params\n\n    @classmethod\n    async def initial(cls, llm, params, trie_opts=None):\n        \"\"\"Creates initial beam state.\n\n        Args:\n            llm (StatefulTokenizedLM): Token-level language model to use.\n            params (BeamParams): Beam search parameters.\n            trie_opts (dict, optional): Additional keyword arguments passed to\n                AsyncTokenByteTrie.from_vocab. For example, {\"max_batch_size\": 100}.\n\n        Returns:\n            (ByteBeamState): Initial beam state.\n        \"\"\"\n        # Handle EOS tokens\n        trie_opts = trie_opts or {}\n        trie_opts[\"eos_tokens\"] = params.eos_tokens\n\n        async_trie = AsyncTokenByteTrie.from_vocab(\n            get_byte_vocab(llm.tokenizer), **trie_opts\n        )\n        state = LazyTrieState.initial(llm, async_trie, mode=TrieMode.WITH_EOS)\n        return cls([await state.materialize()], params)\n\n    def __iter__(self):\n        return iter(self.states)\n\n    def __len__(self):\n        return len(self.states)\n\n    @cached_property\n    def logZ(self):\n        \"\"\"Estimate of the partition function (sum of weights) for current beam.\n        This is the estimate of the prefix probability of the bytes consumed so far.\n        \"\"\"\n        return logsumexp([state.weight for state in self])\n\n    async def __lshift__(self, a):\n        \"\"\"Advances the beam state with a new byte.\n\n        Args:\n            a (int): Byte to add to states.\n\n        Returns:\n            (ByteBeamState): New beam state after processing the byte.\n        \"\"\"\n        new_states = []\n        for state in self:\n            if new_state := state &lt;&lt; a:\n                new_states.append(new_state)\n\n        logZ = logsumexp([s.weight for s in new_states]) if new_states else -np.inf\n        for state in await self.extend(logZ):\n            if new_state := state &lt;&lt; a:\n                new_states.append(new_state)\n\n        new_state = ByteBeamState(new_states, self.params)\n\n        # If advancing would empty the beam, do adaptive healing if enabled\n        if self.params.heal and len(new_state) == 0:\n            healed = await self._adaptive_heal(a)\n            if healed is not None:\n                if self.params.verbose:\n                    print(\"[heal] Applied adaptive token healing\")\n                return healed\n\n        if self.params.verbose:\n            print()\n            print(new_state)\n\n        return new_state\n\n    async def logp_next(self):\n        \"\"\"Computes log probabilities for the next byte across all beam candidates.\n\n        Returns:\n            (LazyByteProbs): Log probabilities for next possible bytes.\n        \"\"\"\n        assert len(self) &gt; 0, \"Beam is empty\"\n\n        logqs = []\n        for state in self:\n            logqs.append(state.logp_next.ps + state.weight)\n\n        for state in await self.extend(self.logZ):\n            logqs.append(state.logp_next.ps + state.weight)\n\n        logqs = np.stack(logqs, axis=0)  # shape: (num_states, array_size)\n        # mask EOT positions of non-extended (EOT is at index 256)\n        logqs[: len(self), -2] = -np.inf\n        logps = scipy_logsumexp(logqs, axis=0)\n\n        return LazyByteProbs(logps - logsumexp(logps))\n\n    async def extend(self, logZ):\n        \"\"\"Attempts to advance each candidate in the beam by a token (EOT).\n\n        For each candididate with EOT available, this ends the current token and\n        starts a new one in preparation for the next byte.\n\n        Args:\n            logZ (float): Current estimated of the partition function for pruning\n\n        Returns:\n            (list[LazyTrieState]): New candidate states after extension\n        \"\"\"\n        extends = []\n        for state in self:\n            if new_state := state.extend():\n                logZ = np.logaddexp(logZ, new_state.weight)\n                extends.append(new_state)\n\n        coros = []\n        for state in extends:\n            if state.weight - logZ &gt; self.params.log_prune_threshold:\n                coros.append(state.materialize())\n\n        return await asyncio.gather(*coros)\n\n    def prune(self):\n        \"\"\"Prunes beam to maintain beam width and probability threshold.\n\n        Returns:\n            (ByteBeamState): New state with pruned candidates.\n        \"\"\"\n        new_states = [\n            state\n            for state in self\n            if state.weight - self.logZ &gt; self.params.log_prune_threshold\n        ][: self.params.K]\n        return ByteBeamState(new_states, self.params)\n\n    def __repr__(self):\n        desc = colors.bold % f\"Z: {self.logZ}\\n\" + colors.bold % \"Candidates:\\n\"\n        for state in self:\n            P = np.exp(state.weight - self.logZ)\n            color = colors.green if P &gt; self.params.prune_threshold else colors.red\n            desc += f\"({color % f'{P:.4f}'}) {repr(state)}\\n\"\n        return desc\n\n    def with_mode(self, mode):\n        \"\"\"Create a new beam state with specified trie mode.\n\n        Args:\n            mode (TrieMode): Trie mode for the new beam state\n\n        Returns:\n            (ByteBeamState): New beam state with updated mode\n        \"\"\"\n        return ByteBeamState(\n            states=[state.with_mode(mode) for state in self.states],\n            params=self.params,\n        )\n\n    async def prefill(self, bs):\n        \"\"\"Prefill the beam on a sequence of bytes.\n\n        During prefilling, EOS tokens are treated as normal tokens and don't cause termination.\n\n        Args:\n            bs (bytes): Byte sequence to prefill on\n\n        Returns:\n            (ByteBeamState): New beam state after prefilling\n        \"\"\"\n        # Create no_eos beam for prefill (EOS tokens treated as normal)\n        no_eos_beam = self.with_mode(TrieMode.WITHOUT_EOS)\n\n        # Do prefill operations on no_eos beam\n        for b in bs:\n            no_eos_beam = await (no_eos_beam.prune() &lt;&lt; b)\n\n        # Return as with_eos beam (EOS tokens get special handling after prefill)\n        return no_eos_beam.with_mode(TrieMode.WITH_EOS)\n\n    async def cleanup(self):\n        \"\"\"Cleans up resources used by the candidates.\"\"\"\n        await asyncio.gather(*[state.cleanup() for state in self])\n\n    async def _adaptive_heal(self, next_byte: int):\n        \"\"\"Attempt adaptive token healing using TokenHealer.\n\n        Returns a new beam advanced by `next_byte` if healing succeeds, else None.\n        \"\"\"\n        healer = TokenHealer(\n            max_backoff=self.params.heal_max_backoff,\n            max_splits=self.params.heal_max_splits,\n            verbose=self.params.verbose,\n        )\n\n        for state in self.states:\n            healed_state = await healer.try_heal(state, next_byte)\n            if healed_state is not None:\n                return ByteBeamState([healed_state], self.params)\n\n        return None\n</code></pre>"},{"location":"reference/genlm/bytes/byte_lm/__init__/#genlm.bytes.byte_lm.ByteBeamState.initial","title":"<code>initial(llm, params, trie_opts=None)</code>  <code>async</code> <code>classmethod</code>","text":"<p>Creates initial beam state.</p> <p>Parameters:</p> Name Type Description Default <code>llm</code> <code>StatefulTokenizedLM</code> <p>Token-level language model to use.</p> required <code>params</code> <code>BeamParams</code> <p>Beam search parameters.</p> required <code>trie_opts</code> <code>dict</code> <p>Additional keyword arguments passed to AsyncTokenByteTrie.from_vocab. For example, {\"max_batch_size\": 100}.</p> <code>None</code> <p>Returns:</p> Type Description <code>ByteBeamState</code> <p>Initial beam state.</p> Source code in <code>genlm/bytes/byte_lm/beam.py</code> <pre><code>@classmethod\nasync def initial(cls, llm, params, trie_opts=None):\n    \"\"\"Creates initial beam state.\n\n    Args:\n        llm (StatefulTokenizedLM): Token-level language model to use.\n        params (BeamParams): Beam search parameters.\n        trie_opts (dict, optional): Additional keyword arguments passed to\n            AsyncTokenByteTrie.from_vocab. For example, {\"max_batch_size\": 100}.\n\n    Returns:\n        (ByteBeamState): Initial beam state.\n    \"\"\"\n    # Handle EOS tokens\n    trie_opts = trie_opts or {}\n    trie_opts[\"eos_tokens\"] = params.eos_tokens\n\n    async_trie = AsyncTokenByteTrie.from_vocab(\n        get_byte_vocab(llm.tokenizer), **trie_opts\n    )\n    state = LazyTrieState.initial(llm, async_trie, mode=TrieMode.WITH_EOS)\n    return cls([await state.materialize()], params)\n</code></pre>"},{"location":"reference/genlm/bytes/byte_lm/__init__/#genlm.bytes.byte_lm.ByteBeamState.logZ","title":"<code>logZ</code>  <code>cached</code> <code>property</code>","text":"<p>Estimate of the partition function (sum of weights) for current beam. This is the estimate of the prefix probability of the bytes consumed so far.</p>"},{"location":"reference/genlm/bytes/byte_lm/__init__/#genlm.bytes.byte_lm.ByteBeamState.__lshift__","title":"<code>__lshift__(a)</code>  <code>async</code>","text":"<p>Advances the beam state with a new byte.</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>int</code> <p>Byte to add to states.</p> required <p>Returns:</p> Type Description <code>ByteBeamState</code> <p>New beam state after processing the byte.</p> Source code in <code>genlm/bytes/byte_lm/beam.py</code> <pre><code>async def __lshift__(self, a):\n    \"\"\"Advances the beam state with a new byte.\n\n    Args:\n        a (int): Byte to add to states.\n\n    Returns:\n        (ByteBeamState): New beam state after processing the byte.\n    \"\"\"\n    new_states = []\n    for state in self:\n        if new_state := state &lt;&lt; a:\n            new_states.append(new_state)\n\n    logZ = logsumexp([s.weight for s in new_states]) if new_states else -np.inf\n    for state in await self.extend(logZ):\n        if new_state := state &lt;&lt; a:\n            new_states.append(new_state)\n\n    new_state = ByteBeamState(new_states, self.params)\n\n    # If advancing would empty the beam, do adaptive healing if enabled\n    if self.params.heal and len(new_state) == 0:\n        healed = await self._adaptive_heal(a)\n        if healed is not None:\n            if self.params.verbose:\n                print(\"[heal] Applied adaptive token healing\")\n            return healed\n\n    if self.params.verbose:\n        print()\n        print(new_state)\n\n    return new_state\n</code></pre>"},{"location":"reference/genlm/bytes/byte_lm/__init__/#genlm.bytes.byte_lm.ByteBeamState.logp_next","title":"<code>logp_next()</code>  <code>async</code>","text":"<p>Computes log probabilities for the next byte across all beam candidates.</p> <p>Returns:</p> Type Description <code>LazyByteProbs</code> <p>Log probabilities for next possible bytes.</p> Source code in <code>genlm/bytes/byte_lm/beam.py</code> <pre><code>async def logp_next(self):\n    \"\"\"Computes log probabilities for the next byte across all beam candidates.\n\n    Returns:\n        (LazyByteProbs): Log probabilities for next possible bytes.\n    \"\"\"\n    assert len(self) &gt; 0, \"Beam is empty\"\n\n    logqs = []\n    for state in self:\n        logqs.append(state.logp_next.ps + state.weight)\n\n    for state in await self.extend(self.logZ):\n        logqs.append(state.logp_next.ps + state.weight)\n\n    logqs = np.stack(logqs, axis=0)  # shape: (num_states, array_size)\n    # mask EOT positions of non-extended (EOT is at index 256)\n    logqs[: len(self), -2] = -np.inf\n    logps = scipy_logsumexp(logqs, axis=0)\n\n    return LazyByteProbs(logps - logsumexp(logps))\n</code></pre>"},{"location":"reference/genlm/bytes/byte_lm/__init__/#genlm.bytes.byte_lm.ByteBeamState.extend","title":"<code>extend(logZ)</code>  <code>async</code>","text":"<p>Attempts to advance each candidate in the beam by a token (EOT).</p> <p>For each candididate with EOT available, this ends the current token and starts a new one in preparation for the next byte.</p> <p>Parameters:</p> Name Type Description Default <code>logZ</code> <code>float</code> <p>Current estimated of the partition function for pruning</p> required <p>Returns:</p> Type Description <code>list[LazyTrieState]</code> <p>New candidate states after extension</p> Source code in <code>genlm/bytes/byte_lm/beam.py</code> <pre><code>async def extend(self, logZ):\n    \"\"\"Attempts to advance each candidate in the beam by a token (EOT).\n\n    For each candididate with EOT available, this ends the current token and\n    starts a new one in preparation for the next byte.\n\n    Args:\n        logZ (float): Current estimated of the partition function for pruning\n\n    Returns:\n        (list[LazyTrieState]): New candidate states after extension\n    \"\"\"\n    extends = []\n    for state in self:\n        if new_state := state.extend():\n            logZ = np.logaddexp(logZ, new_state.weight)\n            extends.append(new_state)\n\n    coros = []\n    for state in extends:\n        if state.weight - logZ &gt; self.params.log_prune_threshold:\n            coros.append(state.materialize())\n\n    return await asyncio.gather(*coros)\n</code></pre>"},{"location":"reference/genlm/bytes/byte_lm/__init__/#genlm.bytes.byte_lm.ByteBeamState.prune","title":"<code>prune()</code>","text":"<p>Prunes beam to maintain beam width and probability threshold.</p> <p>Returns:</p> Type Description <code>ByteBeamState</code> <p>New state with pruned candidates.</p> Source code in <code>genlm/bytes/byte_lm/beam.py</code> <pre><code>def prune(self):\n    \"\"\"Prunes beam to maintain beam width and probability threshold.\n\n    Returns:\n        (ByteBeamState): New state with pruned candidates.\n    \"\"\"\n    new_states = [\n        state\n        for state in self\n        if state.weight - self.logZ &gt; self.params.log_prune_threshold\n    ][: self.params.K]\n    return ByteBeamState(new_states, self.params)\n</code></pre>"},{"location":"reference/genlm/bytes/byte_lm/__init__/#genlm.bytes.byte_lm.ByteBeamState.with_mode","title":"<code>with_mode(mode)</code>","text":"<p>Create a new beam state with specified trie mode.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>TrieMode</code> <p>Trie mode for the new beam state</p> required <p>Returns:</p> Type Description <code>ByteBeamState</code> <p>New beam state with updated mode</p> Source code in <code>genlm/bytes/byte_lm/beam.py</code> <pre><code>def with_mode(self, mode):\n    \"\"\"Create a new beam state with specified trie mode.\n\n    Args:\n        mode (TrieMode): Trie mode for the new beam state\n\n    Returns:\n        (ByteBeamState): New beam state with updated mode\n    \"\"\"\n    return ByteBeamState(\n        states=[state.with_mode(mode) for state in self.states],\n        params=self.params,\n    )\n</code></pre>"},{"location":"reference/genlm/bytes/byte_lm/__init__/#genlm.bytes.byte_lm.ByteBeamState.prefill","title":"<code>prefill(bs)</code>  <code>async</code>","text":"<p>Prefill the beam on a sequence of bytes.</p> <p>During prefilling, EOS tokens are treated as normal tokens and don't cause termination.</p> <p>Parameters:</p> Name Type Description Default <code>bs</code> <code>bytes</code> <p>Byte sequence to prefill on</p> required <p>Returns:</p> Type Description <code>ByteBeamState</code> <p>New beam state after prefilling</p> Source code in <code>genlm/bytes/byte_lm/beam.py</code> <pre><code>async def prefill(self, bs):\n    \"\"\"Prefill the beam on a sequence of bytes.\n\n    During prefilling, EOS tokens are treated as normal tokens and don't cause termination.\n\n    Args:\n        bs (bytes): Byte sequence to prefill on\n\n    Returns:\n        (ByteBeamState): New beam state after prefilling\n    \"\"\"\n    # Create no_eos beam for prefill (EOS tokens treated as normal)\n    no_eos_beam = self.with_mode(TrieMode.WITHOUT_EOS)\n\n    # Do prefill operations on no_eos beam\n    for b in bs:\n        no_eos_beam = await (no_eos_beam.prune() &lt;&lt; b)\n\n    # Return as with_eos beam (EOS tokens get special handling after prefill)\n    return no_eos_beam.with_mode(TrieMode.WITH_EOS)\n</code></pre>"},{"location":"reference/genlm/bytes/byte_lm/__init__/#genlm.bytes.byte_lm.ByteBeamState.cleanup","title":"<code>cleanup()</code>  <code>async</code>","text":"<p>Cleans up resources used by the candidates.</p> Source code in <code>genlm/bytes/byte_lm/beam.py</code> <pre><code>async def cleanup(self):\n    \"\"\"Cleans up resources used by the candidates.\"\"\"\n    await asyncio.gather(*[state.cleanup() for state in self])\n</code></pre>"},{"location":"reference/genlm/bytes/byte_lm/__init__/#genlm.bytes.byte_lm.BeamParams","title":"<code>BeamParams</code>  <code>dataclass</code>","text":"<p>Parameters for byte-level beam summing algorithm.</p> <p>Parameters:</p> Name Type Description Default <code>K</code> <code>int</code> <p>Beam width - maximum number of candidates to maintain.</p> required <code>prune_threshold</code> <code>float</code> <p>Probability threshold for pruning candidates. Candidates with probability below this are removed. Defaults to 0.0</p> <code>0.0</code> <code>verbose</code> <code>bool</code> <p>Whether to print the beam state at each step. Defaults to False</p> <code>False</code> <code>eos_tokens</code> <code>list[bytes]</code> <p>List of tokens that should be treated as EOS. When configured, EOS tokens will terminate generation when sampled. Defaults to None</p> <code>None</code> <code>heal</code> <code>bool</code> <p>Whether to enable adaptive token healing. Defaults to True</p> <code>True</code> <code>heal_max_backoff</code> <code>int</code> <p>Maximum number of bytes to back off when healing. Defaults to None</p> <code>None</code> <code>heal_max_splits</code> <code>int</code> <p>Maximum number of intra-suffix commits allowed during a single healing attempt. Defaults to None</p> <code>None</code> Source code in <code>genlm/bytes/byte_lm/beam.py</code> <pre><code>@dataclass\nclass BeamParams:\n    \"\"\"Parameters for byte-level beam summing algorithm.\n\n    Args:\n        K (int): Beam width - maximum number of candidates to maintain.\n        prune_threshold (float, optional): Probability threshold for pruning candidates.\n            Candidates with probability below this are removed. Defaults to 0.0\n        verbose (bool, optional): Whether to print the beam state at each step. Defaults to False\n        eos_tokens (list[bytes], optional): List of tokens that should be treated as EOS. When configured,\n            EOS tokens will terminate generation when sampled. Defaults to None\n        heal (bool, optional): Whether to enable adaptive token healing. Defaults to True\n        heal_max_backoff (int, optional): Maximum number of bytes to back off when healing. Defaults to None\n        heal_max_splits (int, optional): Maximum number of intra-suffix commits allowed during a single healing attempt. Defaults to None\n    \"\"\"\n\n    K: int\n    prune_threshold: float = 0.0\n    verbose: bool = False\n    eos_tokens: list[bytes] = None\n    heal: bool = True\n    heal_max_backoff: int | None = None\n    # Optional cap on how many intra-partial commits are allowed during a\n    # single healing attempt. None means unlimited. Set to 0 to disable\n    # multi-split behavior (i.e., single-split only).\n    heal_max_splits: int | None = None\n\n    def __post_init__(self):\n        if self.prune_threshold &lt; 0:\n            raise ValueError(\n                f\"prune_threshold must be non-negative, got {self.prune_threshold}\"\n            )\n        self.log_prune_threshold = (\n            np.log(self.prune_threshold) if self.prune_threshold &gt; 0 else -np.inf\n        )\n        self.eos_tokens = set(self.eos_tokens) if self.eos_tokens else set()\n</code></pre>"},{"location":"reference/genlm/bytes/byte_lm/__init__/#genlm.bytes.byte_lm.LazyTrieState","title":"<code>LazyTrieState</code>","text":"<p>A lazy-evaluated state of a TokenByteTrie traversal.</p> <p>This class maintains the state of a language model while traversing a trie structure, lazily evaluating probabilities and maintaining the weight of the current path through the trie for beam search.</p> <p>Parameters:</p> Name Type Description Default <code>lm_state</code> <code>StatefulTokenizedLM</code> <p>Current language model state</p> required <code>trie</code> <code>TokenByteTrie</code> <p>Trie structure mapping tokens to byte sequences</p> required <code>node</code> <code>int</code> <p>Current node in the trie</p> required <code>weight</code> <code>float</code> <p>Cumulative log probability of the path to this node</p> required <code>mass</code> <code>ndarray</code> <p>Masses for each node in the trie for the current state</p> <code>None</code> <code>mode</code> <code>TrieMode</code> <p>Trie mode to use</p> <code>WITH_EOS</code> <code>terminated</code> <code>bool</code> <p>Whether the state is terminated (EOS has been consumed)</p> <code>False</code> Source code in <code>genlm/bytes/byte_lm/trie_state.py</code> <pre><code>class LazyTrieState:\n    \"\"\"A lazy-evaluated state of a TokenByteTrie traversal.\n\n    This class maintains the state of a language model while traversing a trie structure,\n    lazily evaluating probabilities and maintaining the weight of the current path through the trie\n    for beam search.\n\n    Args:\n        lm_state (StatefulTokenizedLM): Current language model state\n        trie (TokenByteTrie): Trie structure mapping tokens to byte sequences\n        node (int): Current node in the trie\n        weight (float): Cumulative log probability of the path to this node\n        mass (numpy.ndarray, optional): Masses for each node in the trie for the current state\n        mode (TrieMode): Trie mode to use\n        terminated (bool): Whether the state is terminated (EOS has been consumed)\n    \"\"\"\n\n    def __init__(\n        self,\n        lm_state,\n        trie,\n        node,\n        weight,\n        mass=None,\n        mode=TrieMode.WITH_EOS,\n        terminated=False,\n    ):\n        self.lm_state = lm_state\n        self.trie = trie\n        self.node = node\n        self.weight = weight\n        self._mass = mass\n        self._extend = None\n        self.mode = mode\n        self.root = self.trie.trie.root\n        self.children = self.trie.trie.children\n        self.terminated = terminated\n\n    @classmethod\n    def initial(cls, lm, trie, mode=TrieMode.WITH_EOS):\n        \"\"\"Creates an initial trie state.\n\n        Args:\n            lm (genlm.backend.AsyncLM): Language model to use\n            trie (TokenByteTrie): TokenByteTrie structure for byte-to-token mapping\n            mode (TrieMode): Trie mode to use\n\n        Returns:\n            (LazyTrieState): Initial state at root of trie with weight 0.0\n        \"\"\"\n        return cls(\n            trie=trie,\n            node=trie.trie.root,\n            lm_state=StatefulTokenizedLM.initial(lm),\n            weight=0.0,\n            mode=mode,\n        )\n\n    @property\n    def partial(self):\n        \"\"\"Returns the byte sequence corresponding to the current node in the trie.\"\"\"\n        return self.trie.trie.node2prefix[self.node]\n\n    @property\n    def mass(self):\n        \"\"\"Returns the log mass for each node in the trie.\n\n        The mass at a node corresponds to the sum of the probabilities of all\n        tokens which share the prefix (`self.partial`) represented by that node.\n\n        Raises:\n            ValueError: If state hasn't been materialized yet\n        \"\"\"\n        if self._mass is None:\n            raise ValueError(\"State is not yet materialized.\")\n        return self._mass\n\n    def with_mode(self, mode):\n        \"\"\"Returns a new state with the given mode.\"\"\"\n        return LazyTrieState(\n            lm_state=self.lm_state,\n            trie=self.trie,\n            node=self.node,\n            weight=self.weight,\n            mass=self._mass,\n            mode=mode,\n            terminated=self.terminated,\n        )\n\n    def actions(self):\n        \"\"\"Returns possible byte transitions from current node.\"\"\"\n        return self.children[self.node]\n\n    def get_EOT(self):\n        \"\"\"Returns the end-of-token node if available from current position in the trie.\"\"\"\n        return self.children[self.node].get(self.trie.trie.eot_token)\n\n    def __lshift__(self, b):\n        \"\"\"Transitions to a new state by consuming a byte.\n\n        Args:\n            b (int): Byte to consume\n\n        Returns:\n            (LazyTrieState|None): New state after consuming byte, or None if transition invalid (terminated or EOS)\n        \"\"\"\n        if self.terminated:\n            return None\n\n        if node := self.children[self.node].get(b):\n            mass = self.mass\n            return LazyTrieState(\n                lm_state=self.lm_state,\n                trie=self.trie,\n                mass=mass,\n                node=node,\n                weight=self.weight + mass[node] - mass[self.node],\n                mode=self.mode,\n                terminated=b == EOS,\n            )\n\n    def extend(self):\n        \"\"\"Extends current state by consuming an end-of-token if possible.\n\n        Returns:\n            (LazyTrieState|None): New state after consuming EOT, or None if not possible\n        \"\"\"\n        if self._extend is None:\n            if (eot_node := self.get_EOT()) is not None:\n                mass = self.mass\n                self._extend = LazyTrieState(\n                    lm_state=self.lm_state\n                    &lt;&lt; int(self.trie.trie.leaf2token_id[eot_node]),\n                    trie=self.trie,\n                    node=self.root,\n                    weight=self.weight + mass[eot_node] - mass[self.node],\n                    mode=self.mode,\n                )\n        return self._extend\n\n    @cached_property\n    def logp_next(self):\n        \"\"\"Computes log probabilities for next possible transitions.\n\n        Returns:\n            (LazyByteProbs): Lazy log probability distribution over possible next bytes\n        \"\"\"\n        logps = np.full(258, -np.inf)  # 258 for EOT, EOS + 256 for normal bytes\n        mass = self.mass\n        logZ = mass[self.node]\n\n        for byte, node in self.actions().items():\n            logps[byte if byte is not None else 256] = mass[node] - logZ\n\n        return LazyByteProbs(logps)\n\n    async def materialize(self):\n        \"\"\"Materializes the masses for each node in the trie for the current state.\n\n        This makes a call to the language model and the underlying trie.\n\n        Returns:\n            (LazyTrieState): Self with materialized masses\n        \"\"\"\n        if self._mass is None:\n            logp_next = await self.lm_state.logp_next()\n            log_mass = await self.trie.weight_sum(torch.exp(logp_next), self.mode)\n            mass = torch.log(log_mass)\n            self._mass = mass.cpu().numpy()\n        return self\n\n    def __repr__(self):\n        context = colors.green % (\"|\" + escape(bytes(self.partial)))\n        if self.terminated:\n            context += colors.yellow % \"&lt;EOS&gt;\"\n        return f\"{self.weight:.2f}: {self.lm_state}\" + context\n\n    async def cleanup(self):\n        \"\"\"Cleans up resources used by the trie.\"\"\"\n        await self.trie.cleanup()\n</code></pre>"},{"location":"reference/genlm/bytes/byte_lm/__init__/#genlm.bytes.byte_lm.LazyTrieState.initial","title":"<code>initial(lm, trie, mode=TrieMode.WITH_EOS)</code>  <code>classmethod</code>","text":"<p>Creates an initial trie state.</p> <p>Parameters:</p> Name Type Description Default <code>lm</code> <code>AsyncLM</code> <p>Language model to use</p> required <code>trie</code> <code>TokenByteTrie</code> <p>TokenByteTrie structure for byte-to-token mapping</p> required <code>mode</code> <code>TrieMode</code> <p>Trie mode to use</p> <code>WITH_EOS</code> <p>Returns:</p> Type Description <code>LazyTrieState</code> <p>Initial state at root of trie with weight 0.0</p> Source code in <code>genlm/bytes/byte_lm/trie_state.py</code> <pre><code>@classmethod\ndef initial(cls, lm, trie, mode=TrieMode.WITH_EOS):\n    \"\"\"Creates an initial trie state.\n\n    Args:\n        lm (genlm.backend.AsyncLM): Language model to use\n        trie (TokenByteTrie): TokenByteTrie structure for byte-to-token mapping\n        mode (TrieMode): Trie mode to use\n\n    Returns:\n        (LazyTrieState): Initial state at root of trie with weight 0.0\n    \"\"\"\n    return cls(\n        trie=trie,\n        node=trie.trie.root,\n        lm_state=StatefulTokenizedLM.initial(lm),\n        weight=0.0,\n        mode=mode,\n    )\n</code></pre>"},{"location":"reference/genlm/bytes/byte_lm/__init__/#genlm.bytes.byte_lm.LazyTrieState.partial","title":"<code>partial</code>  <code>property</code>","text":"<p>Returns the byte sequence corresponding to the current node in the trie.</p>"},{"location":"reference/genlm/bytes/byte_lm/__init__/#genlm.bytes.byte_lm.LazyTrieState.mass","title":"<code>mass</code>  <code>property</code>","text":"<p>Returns the log mass for each node in the trie.</p> <p>The mass at a node corresponds to the sum of the probabilities of all tokens which share the prefix (<code>self.partial</code>) represented by that node.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If state hasn't been materialized yet</p>"},{"location":"reference/genlm/bytes/byte_lm/__init__/#genlm.bytes.byte_lm.LazyTrieState.with_mode","title":"<code>with_mode(mode)</code>","text":"<p>Returns a new state with the given mode.</p> Source code in <code>genlm/bytes/byte_lm/trie_state.py</code> <pre><code>def with_mode(self, mode):\n    \"\"\"Returns a new state with the given mode.\"\"\"\n    return LazyTrieState(\n        lm_state=self.lm_state,\n        trie=self.trie,\n        node=self.node,\n        weight=self.weight,\n        mass=self._mass,\n        mode=mode,\n        terminated=self.terminated,\n    )\n</code></pre>"},{"location":"reference/genlm/bytes/byte_lm/__init__/#genlm.bytes.byte_lm.LazyTrieState.actions","title":"<code>actions()</code>","text":"<p>Returns possible byte transitions from current node.</p> Source code in <code>genlm/bytes/byte_lm/trie_state.py</code> <pre><code>def actions(self):\n    \"\"\"Returns possible byte transitions from current node.\"\"\"\n    return self.children[self.node]\n</code></pre>"},{"location":"reference/genlm/bytes/byte_lm/__init__/#genlm.bytes.byte_lm.LazyTrieState.get_EOT","title":"<code>get_EOT()</code>","text":"<p>Returns the end-of-token node if available from current position in the trie.</p> Source code in <code>genlm/bytes/byte_lm/trie_state.py</code> <pre><code>def get_EOT(self):\n    \"\"\"Returns the end-of-token node if available from current position in the trie.\"\"\"\n    return self.children[self.node].get(self.trie.trie.eot_token)\n</code></pre>"},{"location":"reference/genlm/bytes/byte_lm/__init__/#genlm.bytes.byte_lm.LazyTrieState.__lshift__","title":"<code>__lshift__(b)</code>","text":"<p>Transitions to a new state by consuming a byte.</p> <p>Parameters:</p> Name Type Description Default <code>b</code> <code>int</code> <p>Byte to consume</p> required <p>Returns:</p> Type Description <code>LazyTrieState | None</code> <p>New state after consuming byte, or None if transition invalid (terminated or EOS)</p> Source code in <code>genlm/bytes/byte_lm/trie_state.py</code> <pre><code>def __lshift__(self, b):\n    \"\"\"Transitions to a new state by consuming a byte.\n\n    Args:\n        b (int): Byte to consume\n\n    Returns:\n        (LazyTrieState|None): New state after consuming byte, or None if transition invalid (terminated or EOS)\n    \"\"\"\n    if self.terminated:\n        return None\n\n    if node := self.children[self.node].get(b):\n        mass = self.mass\n        return LazyTrieState(\n            lm_state=self.lm_state,\n            trie=self.trie,\n            mass=mass,\n            node=node,\n            weight=self.weight + mass[node] - mass[self.node],\n            mode=self.mode,\n            terminated=b == EOS,\n        )\n</code></pre>"},{"location":"reference/genlm/bytes/byte_lm/__init__/#genlm.bytes.byte_lm.LazyTrieState.extend","title":"<code>extend()</code>","text":"<p>Extends current state by consuming an end-of-token if possible.</p> <p>Returns:</p> Type Description <code>LazyTrieState | None</code> <p>New state after consuming EOT, or None if not possible</p> Source code in <code>genlm/bytes/byte_lm/trie_state.py</code> <pre><code>def extend(self):\n    \"\"\"Extends current state by consuming an end-of-token if possible.\n\n    Returns:\n        (LazyTrieState|None): New state after consuming EOT, or None if not possible\n    \"\"\"\n    if self._extend is None:\n        if (eot_node := self.get_EOT()) is not None:\n            mass = self.mass\n            self._extend = LazyTrieState(\n                lm_state=self.lm_state\n                &lt;&lt; int(self.trie.trie.leaf2token_id[eot_node]),\n                trie=self.trie,\n                node=self.root,\n                weight=self.weight + mass[eot_node] - mass[self.node],\n                mode=self.mode,\n            )\n    return self._extend\n</code></pre>"},{"location":"reference/genlm/bytes/byte_lm/__init__/#genlm.bytes.byte_lm.LazyTrieState.logp_next","title":"<code>logp_next</code>  <code>cached</code> <code>property</code>","text":"<p>Computes log probabilities for next possible transitions.</p> <p>Returns:</p> Type Description <code>LazyByteProbs</code> <p>Lazy log probability distribution over possible next bytes</p>"},{"location":"reference/genlm/bytes/byte_lm/__init__/#genlm.bytes.byte_lm.LazyTrieState.materialize","title":"<code>materialize()</code>  <code>async</code>","text":"<p>Materializes the masses for each node in the trie for the current state.</p> <p>This makes a call to the language model and the underlying trie.</p> <p>Returns:</p> Type Description <code>LazyTrieState</code> <p>Self with materialized masses</p> Source code in <code>genlm/bytes/byte_lm/trie_state.py</code> <pre><code>async def materialize(self):\n    \"\"\"Materializes the masses for each node in the trie for the current state.\n\n    This makes a call to the language model and the underlying trie.\n\n    Returns:\n        (LazyTrieState): Self with materialized masses\n    \"\"\"\n    if self._mass is None:\n        logp_next = await self.lm_state.logp_next()\n        log_mass = await self.trie.weight_sum(torch.exp(logp_next), self.mode)\n        mass = torch.log(log_mass)\n        self._mass = mass.cpu().numpy()\n    return self\n</code></pre>"},{"location":"reference/genlm/bytes/byte_lm/__init__/#genlm.bytes.byte_lm.LazyTrieState.cleanup","title":"<code>cleanup()</code>  <code>async</code>","text":"<p>Cleans up resources used by the trie.</p> Source code in <code>genlm/bytes/byte_lm/trie_state.py</code> <pre><code>async def cleanup(self):\n    \"\"\"Cleans up resources used by the trie.\"\"\"\n    await self.trie.cleanup()\n</code></pre>"},{"location":"reference/genlm/bytes/byte_lm/__init__/#genlm.bytes.byte_lm.StatefulTokenizedLM","title":"<code>StatefulTokenizedLM</code>","text":"<p>A stateful tokenized language model that maintains context and generates next token logprobs.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>AsyncLM</code> <p>The underlying language model</p> required <code>context</code> <code>list</code> <p>List of token IDs representing the current context</p> required <code>n_calls</code> <code>int</code> <p>Number of times the model has been called</p> <code>0</code> <code>max_context_length</code> <code>int</code> <p>Maximum length of context to maintain</p> <code>None</code> Source code in <code>genlm/bytes/byte_lm/lm_state.py</code> <pre><code>class StatefulTokenizedLM:\n    \"\"\"A stateful tokenized language model that maintains context and generates next token logprobs.\n\n    Args:\n        model (genlm.backend.AsyncLM): The underlying language model\n        context (list): List of token IDs representing the current context\n        n_calls (int): Number of times the model has been called\n        max_context_length (int, optional): Maximum length of context to maintain\n    \"\"\"\n\n    def __init__(self, model, context, n_calls=0, max_context_length=None):\n        self.model = model\n        self.context = context\n        self._n_calls = n_calls\n        self.max_context_length = max_context_length\n\n    @classmethod\n    def initial(cls, model, initial_context=None, max_context_length=None):\n        \"\"\"Creates an initial state for the language model.\n\n        Args:\n            model (genlm.backend.AsyncLM): The language model to use\n            initial_context (list, optional): Initial context of token IDs. Defaults to [tokenizer.bos_token_id]\n            max_context_length (int, optional): Maximum context length to maintain\n\n        Returns:\n            (StatefulTokenizedLM): A new instance with initial state\n        \"\"\"\n        if initial_context is None:\n            initial_context = [model.tokenizer.bos_token_id]\n        return cls(model, initial_context, max_context_length=max_context_length)\n\n    def __lshift__(self, token):\n        \"\"\"Adds a new token to the context and returns a new state.\n\n        Args:\n            token (int): Token ID to add to context\n\n        Returns:\n            (StatefulTokenizedLM): New state with updated context\n        \"\"\"\n        assert isinstance(token, int)\n        if (\n            self.max_context_length is not None\n            and len(self.context) &gt;= self.max_context_length\n        ):\n            self.context = self.context[-(self.max_context_length - 1) :]\n        return StatefulTokenizedLM(\n            self.model, self.context + [token], n_calls=self._n_calls\n        )\n\n    async def logp_next(self):\n        \"\"\"Computes log probabilities for the next token given the current context.\n\n        Returns:\n            (torch.Tensor): Log probabilities for next tokens\n        \"\"\"\n        self._n_calls += 1\n        return await self.model.next_token_logprobs(self.context)\n\n    def __repr__(self):\n        return colors.purple % (\n            \"|\".join([escape(self.model.byte_vocab[x]) for x in self.context])\n        )\n</code></pre>"},{"location":"reference/genlm/bytes/byte_lm/__init__/#genlm.bytes.byte_lm.StatefulTokenizedLM.initial","title":"<code>initial(model, initial_context=None, max_context_length=None)</code>  <code>classmethod</code>","text":"<p>Creates an initial state for the language model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>AsyncLM</code> <p>The language model to use</p> required <code>initial_context</code> <code>list</code> <p>Initial context of token IDs. Defaults to [tokenizer.bos_token_id]</p> <code>None</code> <code>max_context_length</code> <code>int</code> <p>Maximum context length to maintain</p> <code>None</code> <p>Returns:</p> Type Description <code>StatefulTokenizedLM</code> <p>A new instance with initial state</p> Source code in <code>genlm/bytes/byte_lm/lm_state.py</code> <pre><code>@classmethod\ndef initial(cls, model, initial_context=None, max_context_length=None):\n    \"\"\"Creates an initial state for the language model.\n\n    Args:\n        model (genlm.backend.AsyncLM): The language model to use\n        initial_context (list, optional): Initial context of token IDs. Defaults to [tokenizer.bos_token_id]\n        max_context_length (int, optional): Maximum context length to maintain\n\n    Returns:\n        (StatefulTokenizedLM): A new instance with initial state\n    \"\"\"\n    if initial_context is None:\n        initial_context = [model.tokenizer.bos_token_id]\n    return cls(model, initial_context, max_context_length=max_context_length)\n</code></pre>"},{"location":"reference/genlm/bytes/byte_lm/__init__/#genlm.bytes.byte_lm.StatefulTokenizedLM.__lshift__","title":"<code>__lshift__(token)</code>","text":"<p>Adds a new token to the context and returns a new state.</p> <p>Parameters:</p> Name Type Description Default <code>token</code> <code>int</code> <p>Token ID to add to context</p> required <p>Returns:</p> Type Description <code>StatefulTokenizedLM</code> <p>New state with updated context</p> Source code in <code>genlm/bytes/byte_lm/lm_state.py</code> <pre><code>def __lshift__(self, token):\n    \"\"\"Adds a new token to the context and returns a new state.\n\n    Args:\n        token (int): Token ID to add to context\n\n    Returns:\n        (StatefulTokenizedLM): New state with updated context\n    \"\"\"\n    assert isinstance(token, int)\n    if (\n        self.max_context_length is not None\n        and len(self.context) &gt;= self.max_context_length\n    ):\n        self.context = self.context[-(self.max_context_length - 1) :]\n    return StatefulTokenizedLM(\n        self.model, self.context + [token], n_calls=self._n_calls\n    )\n</code></pre>"},{"location":"reference/genlm/bytes/byte_lm/__init__/#genlm.bytes.byte_lm.StatefulTokenizedLM.logp_next","title":"<code>logp_next()</code>  <code>async</code>","text":"<p>Computes log probabilities for the next token given the current context.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>Log probabilities for next tokens</p> Source code in <code>genlm/bytes/byte_lm/lm_state.py</code> <pre><code>async def logp_next(self):\n    \"\"\"Computes log probabilities for the next token given the current context.\n\n    Returns:\n        (torch.Tensor): Log probabilities for next tokens\n    \"\"\"\n    self._n_calls += 1\n    return await self.model.next_token_logprobs(self.context)\n</code></pre>"},{"location":"reference/genlm/bytes/byte_lm/beam/","title":"beam","text":""},{"location":"reference/genlm/bytes/byte_lm/beam/#genlm.bytes.byte_lm.beam","title":"<code>genlm.bytes.byte_lm.beam</code>","text":""},{"location":"reference/genlm/bytes/byte_lm/beam/#genlm.bytes.byte_lm.beam.BeamParams","title":"<code>BeamParams</code>  <code>dataclass</code>","text":"<p>Parameters for byte-level beam summing algorithm.</p> <p>Parameters:</p> Name Type Description Default <code>K</code> <code>int</code> <p>Beam width - maximum number of candidates to maintain.</p> required <code>prune_threshold</code> <code>float</code> <p>Probability threshold for pruning candidates. Candidates with probability below this are removed. Defaults to 0.0</p> <code>0.0</code> <code>verbose</code> <code>bool</code> <p>Whether to print the beam state at each step. Defaults to False</p> <code>False</code> <code>eos_tokens</code> <code>list[bytes]</code> <p>List of tokens that should be treated as EOS. When configured, EOS tokens will terminate generation when sampled. Defaults to None</p> <code>None</code> <code>heal</code> <code>bool</code> <p>Whether to enable adaptive token healing. Defaults to True</p> <code>True</code> <code>heal_max_backoff</code> <code>int</code> <p>Maximum number of bytes to back off when healing. Defaults to None</p> <code>None</code> <code>heal_max_splits</code> <code>int</code> <p>Maximum number of intra-suffix commits allowed during a single healing attempt. Defaults to None</p> <code>None</code> Source code in <code>genlm/bytes/byte_lm/beam.py</code> <pre><code>@dataclass\nclass BeamParams:\n    \"\"\"Parameters for byte-level beam summing algorithm.\n\n    Args:\n        K (int): Beam width - maximum number of candidates to maintain.\n        prune_threshold (float, optional): Probability threshold for pruning candidates.\n            Candidates with probability below this are removed. Defaults to 0.0\n        verbose (bool, optional): Whether to print the beam state at each step. Defaults to False\n        eos_tokens (list[bytes], optional): List of tokens that should be treated as EOS. When configured,\n            EOS tokens will terminate generation when sampled. Defaults to None\n        heal (bool, optional): Whether to enable adaptive token healing. Defaults to True\n        heal_max_backoff (int, optional): Maximum number of bytes to back off when healing. Defaults to None\n        heal_max_splits (int, optional): Maximum number of intra-suffix commits allowed during a single healing attempt. Defaults to None\n    \"\"\"\n\n    K: int\n    prune_threshold: float = 0.0\n    verbose: bool = False\n    eos_tokens: list[bytes] = None\n    heal: bool = True\n    heal_max_backoff: int | None = None\n    # Optional cap on how many intra-partial commits are allowed during a\n    # single healing attempt. None means unlimited. Set to 0 to disable\n    # multi-split behavior (i.e., single-split only).\n    heal_max_splits: int | None = None\n\n    def __post_init__(self):\n        if self.prune_threshold &lt; 0:\n            raise ValueError(\n                f\"prune_threshold must be non-negative, got {self.prune_threshold}\"\n            )\n        self.log_prune_threshold = (\n            np.log(self.prune_threshold) if self.prune_threshold &gt; 0 else -np.inf\n        )\n        self.eos_tokens = set(self.eos_tokens) if self.eos_tokens else set()\n</code></pre>"},{"location":"reference/genlm/bytes/byte_lm/beam/#genlm.bytes.byte_lm.beam.ByteBeamState","title":"<code>ByteBeamState</code>","text":"<p>               Bases: <code>StatefulByteLM</code></p> <p>Represents the state of the beam during byte-level language modeling.</p> <p>Tracks multiple candidate states and their probabilities, pruning low-probability candidates.</p> <p>Parameters:</p> Name Type Description Default <code>states</code> <code>list[LazyTrieState]</code> <p>List of candidate states to track</p> required <code>params</code> <code>BeamParams</code> <p>Parameters controlling beam search behavior</p> required Source code in <code>genlm/bytes/byte_lm/beam.py</code> <pre><code>class ByteBeamState(StatefulByteLM):\n    \"\"\"Represents the state of the beam during byte-level language modeling.\n\n    Tracks multiple candidate states and their probabilities, pruning low-probability\n    candidates.\n\n    Args:\n        states (list[LazyTrieState]): List of candidate states to track\n        params (BeamParams): Parameters controlling beam search behavior\n    \"\"\"\n\n    def __init__(self, states, params):\n        self.states = sorted(states, key=lambda b: -b.weight)\n        self.params = params\n\n    @classmethod\n    async def initial(cls, llm, params, trie_opts=None):\n        \"\"\"Creates initial beam state.\n\n        Args:\n            llm (StatefulTokenizedLM): Token-level language model to use.\n            params (BeamParams): Beam search parameters.\n            trie_opts (dict, optional): Additional keyword arguments passed to\n                AsyncTokenByteTrie.from_vocab. For example, {\"max_batch_size\": 100}.\n\n        Returns:\n            (ByteBeamState): Initial beam state.\n        \"\"\"\n        # Handle EOS tokens\n        trie_opts = trie_opts or {}\n        trie_opts[\"eos_tokens\"] = params.eos_tokens\n\n        async_trie = AsyncTokenByteTrie.from_vocab(\n            get_byte_vocab(llm.tokenizer), **trie_opts\n        )\n        state = LazyTrieState.initial(llm, async_trie, mode=TrieMode.WITH_EOS)\n        return cls([await state.materialize()], params)\n\n    def __iter__(self):\n        return iter(self.states)\n\n    def __len__(self):\n        return len(self.states)\n\n    @cached_property\n    def logZ(self):\n        \"\"\"Estimate of the partition function (sum of weights) for current beam.\n        This is the estimate of the prefix probability of the bytes consumed so far.\n        \"\"\"\n        return logsumexp([state.weight for state in self])\n\n    async def __lshift__(self, a):\n        \"\"\"Advances the beam state with a new byte.\n\n        Args:\n            a (int): Byte to add to states.\n\n        Returns:\n            (ByteBeamState): New beam state after processing the byte.\n        \"\"\"\n        new_states = []\n        for state in self:\n            if new_state := state &lt;&lt; a:\n                new_states.append(new_state)\n\n        logZ = logsumexp([s.weight for s in new_states]) if new_states else -np.inf\n        for state in await self.extend(logZ):\n            if new_state := state &lt;&lt; a:\n                new_states.append(new_state)\n\n        new_state = ByteBeamState(new_states, self.params)\n\n        # If advancing would empty the beam, do adaptive healing if enabled\n        if self.params.heal and len(new_state) == 0:\n            healed = await self._adaptive_heal(a)\n            if healed is not None:\n                if self.params.verbose:\n                    print(\"[heal] Applied adaptive token healing\")\n                return healed\n\n        if self.params.verbose:\n            print()\n            print(new_state)\n\n        return new_state\n\n    async def logp_next(self):\n        \"\"\"Computes log probabilities for the next byte across all beam candidates.\n\n        Returns:\n            (LazyByteProbs): Log probabilities for next possible bytes.\n        \"\"\"\n        assert len(self) &gt; 0, \"Beam is empty\"\n\n        logqs = []\n        for state in self:\n            logqs.append(state.logp_next.ps + state.weight)\n\n        for state in await self.extend(self.logZ):\n            logqs.append(state.logp_next.ps + state.weight)\n\n        logqs = np.stack(logqs, axis=0)  # shape: (num_states, array_size)\n        # mask EOT positions of non-extended (EOT is at index 256)\n        logqs[: len(self), -2] = -np.inf\n        logps = scipy_logsumexp(logqs, axis=0)\n\n        return LazyByteProbs(logps - logsumexp(logps))\n\n    async def extend(self, logZ):\n        \"\"\"Attempts to advance each candidate in the beam by a token (EOT).\n\n        For each candididate with EOT available, this ends the current token and\n        starts a new one in preparation for the next byte.\n\n        Args:\n            logZ (float): Current estimated of the partition function for pruning\n\n        Returns:\n            (list[LazyTrieState]): New candidate states after extension\n        \"\"\"\n        extends = []\n        for state in self:\n            if new_state := state.extend():\n                logZ = np.logaddexp(logZ, new_state.weight)\n                extends.append(new_state)\n\n        coros = []\n        for state in extends:\n            if state.weight - logZ &gt; self.params.log_prune_threshold:\n                coros.append(state.materialize())\n\n        return await asyncio.gather(*coros)\n\n    def prune(self):\n        \"\"\"Prunes beam to maintain beam width and probability threshold.\n\n        Returns:\n            (ByteBeamState): New state with pruned candidates.\n        \"\"\"\n        new_states = [\n            state\n            for state in self\n            if state.weight - self.logZ &gt; self.params.log_prune_threshold\n        ][: self.params.K]\n        return ByteBeamState(new_states, self.params)\n\n    def __repr__(self):\n        desc = colors.bold % f\"Z: {self.logZ}\\n\" + colors.bold % \"Candidates:\\n\"\n        for state in self:\n            P = np.exp(state.weight - self.logZ)\n            color = colors.green if P &gt; self.params.prune_threshold else colors.red\n            desc += f\"({color % f'{P:.4f}'}) {repr(state)}\\n\"\n        return desc\n\n    def with_mode(self, mode):\n        \"\"\"Create a new beam state with specified trie mode.\n\n        Args:\n            mode (TrieMode): Trie mode for the new beam state\n\n        Returns:\n            (ByteBeamState): New beam state with updated mode\n        \"\"\"\n        return ByteBeamState(\n            states=[state.with_mode(mode) for state in self.states],\n            params=self.params,\n        )\n\n    async def prefill(self, bs):\n        \"\"\"Prefill the beam on a sequence of bytes.\n\n        During prefilling, EOS tokens are treated as normal tokens and don't cause termination.\n\n        Args:\n            bs (bytes): Byte sequence to prefill on\n\n        Returns:\n            (ByteBeamState): New beam state after prefilling\n        \"\"\"\n        # Create no_eos beam for prefill (EOS tokens treated as normal)\n        no_eos_beam = self.with_mode(TrieMode.WITHOUT_EOS)\n\n        # Do prefill operations on no_eos beam\n        for b in bs:\n            no_eos_beam = await (no_eos_beam.prune() &lt;&lt; b)\n\n        # Return as with_eos beam (EOS tokens get special handling after prefill)\n        return no_eos_beam.with_mode(TrieMode.WITH_EOS)\n\n    async def cleanup(self):\n        \"\"\"Cleans up resources used by the candidates.\"\"\"\n        await asyncio.gather(*[state.cleanup() for state in self])\n\n    async def _adaptive_heal(self, next_byte: int):\n        \"\"\"Attempt adaptive token healing using TokenHealer.\n\n        Returns a new beam advanced by `next_byte` if healing succeeds, else None.\n        \"\"\"\n        healer = TokenHealer(\n            max_backoff=self.params.heal_max_backoff,\n            max_splits=self.params.heal_max_splits,\n            verbose=self.params.verbose,\n        )\n\n        for state in self.states:\n            healed_state = await healer.try_heal(state, next_byte)\n            if healed_state is not None:\n                return ByteBeamState([healed_state], self.params)\n\n        return None\n</code></pre>"},{"location":"reference/genlm/bytes/byte_lm/beam/#genlm.bytes.byte_lm.beam.ByteBeamState.initial","title":"<code>initial(llm, params, trie_opts=None)</code>  <code>async</code> <code>classmethod</code>","text":"<p>Creates initial beam state.</p> <p>Parameters:</p> Name Type Description Default <code>llm</code> <code>StatefulTokenizedLM</code> <p>Token-level language model to use.</p> required <code>params</code> <code>BeamParams</code> <p>Beam search parameters.</p> required <code>trie_opts</code> <code>dict</code> <p>Additional keyword arguments passed to AsyncTokenByteTrie.from_vocab. For example, {\"max_batch_size\": 100}.</p> <code>None</code> <p>Returns:</p> Type Description <code>ByteBeamState</code> <p>Initial beam state.</p> Source code in <code>genlm/bytes/byte_lm/beam.py</code> <pre><code>@classmethod\nasync def initial(cls, llm, params, trie_opts=None):\n    \"\"\"Creates initial beam state.\n\n    Args:\n        llm (StatefulTokenizedLM): Token-level language model to use.\n        params (BeamParams): Beam search parameters.\n        trie_opts (dict, optional): Additional keyword arguments passed to\n            AsyncTokenByteTrie.from_vocab. For example, {\"max_batch_size\": 100}.\n\n    Returns:\n        (ByteBeamState): Initial beam state.\n    \"\"\"\n    # Handle EOS tokens\n    trie_opts = trie_opts or {}\n    trie_opts[\"eos_tokens\"] = params.eos_tokens\n\n    async_trie = AsyncTokenByteTrie.from_vocab(\n        get_byte_vocab(llm.tokenizer), **trie_opts\n    )\n    state = LazyTrieState.initial(llm, async_trie, mode=TrieMode.WITH_EOS)\n    return cls([await state.materialize()], params)\n</code></pre>"},{"location":"reference/genlm/bytes/byte_lm/beam/#genlm.bytes.byte_lm.beam.ByteBeamState.logZ","title":"<code>logZ</code>  <code>cached</code> <code>property</code>","text":"<p>Estimate of the partition function (sum of weights) for current beam. This is the estimate of the prefix probability of the bytes consumed so far.</p>"},{"location":"reference/genlm/bytes/byte_lm/beam/#genlm.bytes.byte_lm.beam.ByteBeamState.__lshift__","title":"<code>__lshift__(a)</code>  <code>async</code>","text":"<p>Advances the beam state with a new byte.</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>int</code> <p>Byte to add to states.</p> required <p>Returns:</p> Type Description <code>ByteBeamState</code> <p>New beam state after processing the byte.</p> Source code in <code>genlm/bytes/byte_lm/beam.py</code> <pre><code>async def __lshift__(self, a):\n    \"\"\"Advances the beam state with a new byte.\n\n    Args:\n        a (int): Byte to add to states.\n\n    Returns:\n        (ByteBeamState): New beam state after processing the byte.\n    \"\"\"\n    new_states = []\n    for state in self:\n        if new_state := state &lt;&lt; a:\n            new_states.append(new_state)\n\n    logZ = logsumexp([s.weight for s in new_states]) if new_states else -np.inf\n    for state in await self.extend(logZ):\n        if new_state := state &lt;&lt; a:\n            new_states.append(new_state)\n\n    new_state = ByteBeamState(new_states, self.params)\n\n    # If advancing would empty the beam, do adaptive healing if enabled\n    if self.params.heal and len(new_state) == 0:\n        healed = await self._adaptive_heal(a)\n        if healed is not None:\n            if self.params.verbose:\n                print(\"[heal] Applied adaptive token healing\")\n            return healed\n\n    if self.params.verbose:\n        print()\n        print(new_state)\n\n    return new_state\n</code></pre>"},{"location":"reference/genlm/bytes/byte_lm/beam/#genlm.bytes.byte_lm.beam.ByteBeamState.logp_next","title":"<code>logp_next()</code>  <code>async</code>","text":"<p>Computes log probabilities for the next byte across all beam candidates.</p> <p>Returns:</p> Type Description <code>LazyByteProbs</code> <p>Log probabilities for next possible bytes.</p> Source code in <code>genlm/bytes/byte_lm/beam.py</code> <pre><code>async def logp_next(self):\n    \"\"\"Computes log probabilities for the next byte across all beam candidates.\n\n    Returns:\n        (LazyByteProbs): Log probabilities for next possible bytes.\n    \"\"\"\n    assert len(self) &gt; 0, \"Beam is empty\"\n\n    logqs = []\n    for state in self:\n        logqs.append(state.logp_next.ps + state.weight)\n\n    for state in await self.extend(self.logZ):\n        logqs.append(state.logp_next.ps + state.weight)\n\n    logqs = np.stack(logqs, axis=0)  # shape: (num_states, array_size)\n    # mask EOT positions of non-extended (EOT is at index 256)\n    logqs[: len(self), -2] = -np.inf\n    logps = scipy_logsumexp(logqs, axis=0)\n\n    return LazyByteProbs(logps - logsumexp(logps))\n</code></pre>"},{"location":"reference/genlm/bytes/byte_lm/beam/#genlm.bytes.byte_lm.beam.ByteBeamState.extend","title":"<code>extend(logZ)</code>  <code>async</code>","text":"<p>Attempts to advance each candidate in the beam by a token (EOT).</p> <p>For each candididate with EOT available, this ends the current token and starts a new one in preparation for the next byte.</p> <p>Parameters:</p> Name Type Description Default <code>logZ</code> <code>float</code> <p>Current estimated of the partition function for pruning</p> required <p>Returns:</p> Type Description <code>list[LazyTrieState]</code> <p>New candidate states after extension</p> Source code in <code>genlm/bytes/byte_lm/beam.py</code> <pre><code>async def extend(self, logZ):\n    \"\"\"Attempts to advance each candidate in the beam by a token (EOT).\n\n    For each candididate with EOT available, this ends the current token and\n    starts a new one in preparation for the next byte.\n\n    Args:\n        logZ (float): Current estimated of the partition function for pruning\n\n    Returns:\n        (list[LazyTrieState]): New candidate states after extension\n    \"\"\"\n    extends = []\n    for state in self:\n        if new_state := state.extend():\n            logZ = np.logaddexp(logZ, new_state.weight)\n            extends.append(new_state)\n\n    coros = []\n    for state in extends:\n        if state.weight - logZ &gt; self.params.log_prune_threshold:\n            coros.append(state.materialize())\n\n    return await asyncio.gather(*coros)\n</code></pre>"},{"location":"reference/genlm/bytes/byte_lm/beam/#genlm.bytes.byte_lm.beam.ByteBeamState.prune","title":"<code>prune()</code>","text":"<p>Prunes beam to maintain beam width and probability threshold.</p> <p>Returns:</p> Type Description <code>ByteBeamState</code> <p>New state with pruned candidates.</p> Source code in <code>genlm/bytes/byte_lm/beam.py</code> <pre><code>def prune(self):\n    \"\"\"Prunes beam to maintain beam width and probability threshold.\n\n    Returns:\n        (ByteBeamState): New state with pruned candidates.\n    \"\"\"\n    new_states = [\n        state\n        for state in self\n        if state.weight - self.logZ &gt; self.params.log_prune_threshold\n    ][: self.params.K]\n    return ByteBeamState(new_states, self.params)\n</code></pre>"},{"location":"reference/genlm/bytes/byte_lm/beam/#genlm.bytes.byte_lm.beam.ByteBeamState.with_mode","title":"<code>with_mode(mode)</code>","text":"<p>Create a new beam state with specified trie mode.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>TrieMode</code> <p>Trie mode for the new beam state</p> required <p>Returns:</p> Type Description <code>ByteBeamState</code> <p>New beam state with updated mode</p> Source code in <code>genlm/bytes/byte_lm/beam.py</code> <pre><code>def with_mode(self, mode):\n    \"\"\"Create a new beam state with specified trie mode.\n\n    Args:\n        mode (TrieMode): Trie mode for the new beam state\n\n    Returns:\n        (ByteBeamState): New beam state with updated mode\n    \"\"\"\n    return ByteBeamState(\n        states=[state.with_mode(mode) for state in self.states],\n        params=self.params,\n    )\n</code></pre>"},{"location":"reference/genlm/bytes/byte_lm/beam/#genlm.bytes.byte_lm.beam.ByteBeamState.prefill","title":"<code>prefill(bs)</code>  <code>async</code>","text":"<p>Prefill the beam on a sequence of bytes.</p> <p>During prefilling, EOS tokens are treated as normal tokens and don't cause termination.</p> <p>Parameters:</p> Name Type Description Default <code>bs</code> <code>bytes</code> <p>Byte sequence to prefill on</p> required <p>Returns:</p> Type Description <code>ByteBeamState</code> <p>New beam state after prefilling</p> Source code in <code>genlm/bytes/byte_lm/beam.py</code> <pre><code>async def prefill(self, bs):\n    \"\"\"Prefill the beam on a sequence of bytes.\n\n    During prefilling, EOS tokens are treated as normal tokens and don't cause termination.\n\n    Args:\n        bs (bytes): Byte sequence to prefill on\n\n    Returns:\n        (ByteBeamState): New beam state after prefilling\n    \"\"\"\n    # Create no_eos beam for prefill (EOS tokens treated as normal)\n    no_eos_beam = self.with_mode(TrieMode.WITHOUT_EOS)\n\n    # Do prefill operations on no_eos beam\n    for b in bs:\n        no_eos_beam = await (no_eos_beam.prune() &lt;&lt; b)\n\n    # Return as with_eos beam (EOS tokens get special handling after prefill)\n    return no_eos_beam.with_mode(TrieMode.WITH_EOS)\n</code></pre>"},{"location":"reference/genlm/bytes/byte_lm/beam/#genlm.bytes.byte_lm.beam.ByteBeamState.cleanup","title":"<code>cleanup()</code>  <code>async</code>","text":"<p>Cleans up resources used by the candidates.</p> Source code in <code>genlm/bytes/byte_lm/beam.py</code> <pre><code>async def cleanup(self):\n    \"\"\"Cleans up resources used by the candidates.\"\"\"\n    await asyncio.gather(*[state.cleanup() for state in self])\n</code></pre>"},{"location":"reference/genlm/bytes/byte_lm/heal/","title":"heal","text":""},{"location":"reference/genlm/bytes/byte_lm/heal/#genlm.bytes.byte_lm.heal","title":"<code>genlm.bytes.byte_lm.heal</code>","text":""},{"location":"reference/genlm/bytes/byte_lm/heal/#genlm.bytes.byte_lm.heal.TokenHealer","title":"<code>TokenHealer</code>","text":"<p>Handles adaptive token healing for ByteBeamState. Token healing finds alternative tokenizations when the current tokenization cannot consume the next byte. It works by: 1. Trying different \"backoff\" positions k (commit partial[:k] as a token) 2. Replaying the remaining bytes (partial[k:]) from fresh root 3. Using extend() when stuck to commit intermediate tokens 4. Finally consuming the target next_byte</p> <p>Parameters:</p> Name Type Description Default <code>max_backoff</code> <code>int | None</code> <p>Maximum bytes to back off (None = unlimited)</p> <code>None</code> <code>max_splits</code> <code>int | None</code> <p>Maximum intra-suffix commits allowed (None = unlimited)</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Whether to print debug information</p> <code>False</code> Source code in <code>genlm/bytes/byte_lm/heal.py</code> <pre><code>class TokenHealer:\n    \"\"\"Handles adaptive token healing for ByteBeamState.\n    Token healing finds alternative tokenizations when the current tokenization\n    cannot consume the next byte. It works by:\n    1. Trying different \"backoff\" positions k (commit partial[:k] as a token)\n    2. Replaying the remaining bytes (partial[k:]) from fresh root\n    3. Using extend() when stuck to commit intermediate tokens\n    4. Finally consuming the target next_byte\n\n    Args:\n        max_backoff: Maximum bytes to back off (None = unlimited)\n        max_splits: Maximum intra-suffix commits allowed (None = unlimited)\n        verbose: Whether to print debug information\n    \"\"\"\n\n    def __init__(\n        self,\n        max_backoff: int | None = None,\n        max_splits: int | None = None,\n        verbose: bool = False,\n    ):\n        self.max_backoff = max_backoff\n        self.max_splits = max_splits\n        self.verbose = verbose\n\n    async def try_heal(self, state, next_byte: int):\n        \"\"\"Try to heal a state so it can consume next_byte.\n\n        Args:\n            state: A materialized LazyTrieState that cannot consume next_byte\n            next_byte: The byte we want to consume\n\n        Returns:\n            LazyTrieState if healing succeeds, None otherwise\n        \"\"\"\n        partial = state.partial\n        partial_len = len(partial)\n\n        if self.verbose:\n            print(\n                f\"[heal] Start: next_byte={format_byte(next_byte)}, partial={bytes(partial)!r}, max_backoff={self.max_backoff}\"\n            )\n\n        # Extract invariants computed once for all k values\n        trie = state.trie.trie\n        # base_weight undoes current path contribution: weight + mass[root] - mass[node]\n        # NOTE: mass[root] terms cancel, written this way to show undo current path contribution, add commit path\n        base_weight = state.weight - (state.mass[state.node] - state.mass[trie.root])\n\n        # Calculate how far back we're allowed to go\n        min_k = (\n            0 if self.max_backoff is None else max(0, partial_len - self.max_backoff)\n        )\n\n        # Try each backoff position k (from longest prefix to shortest)\n        for k in range(partial_len, min_k - 1, -1):\n            result = await self._try_at_k(state, trie, base_weight, k, next_byte)\n            if result is not None:\n                return result\n\n        if self.verbose:\n            print(\"[heal] FAILED: no valid healing found\")\n        return None\n\n    async def _try_at_k(self, state, trie, base_weight: float, k: int, next_byte: int):\n        \"\"\"Try healing by committing partial[:k], replaying partial[k:], then consuming next_byte.\n\n        Args:\n            state: The original state to heal from\n            trie: The trie structure (state.trie.trie)\n            base_weight: Precomputed weight after undoing current path\n            k: Backoff position to try\n            next_byte: The byte we want to consume\n\n        Returns:\n            LazyTrieState if successful, None otherwise\n        \"\"\"\n        children = trie.children\n        partial = state.partial\n\n        # Navigate to position k to check if we can commit there\n        node_at_k = trie.root\n        for b in partial[:k]:\n            node_at_k = children[node_at_k].get(b)\n            if node_at_k is None:\n                return None  # Path doesn't exist\n\n        # Check if there's an EOT at position k\n        eot_node = children[node_at_k].get(trie.eot_token)\n        if eot_node is None:\n            if self.verbose:\n                print(f\"[heal] k={k}: no EOT at {bytes(partial[:k])!r}\")\n            return None\n\n        # Commit at position k\n        weight_after_commit = base_weight + (\n            state.mass[eot_node] - state.mass[trie.root]\n        )\n        token_id = int(trie.leaf2token_id[eot_node])\n\n        current = LazyTrieState(\n            lm_state=(state.lm_state &lt;&lt; token_id),\n            trie=state.trie,\n            node=trie.root,\n            weight=weight_after_commit,\n            mass=None,\n            mode=state.mode,\n            terminated=False,\n        )\n        current = await current.materialize()\n\n        if self.verbose:\n            print(\n                f\"[heal] k={k}: commit {trie.decode[token_id]!r}, w={weight_after_commit:.2f}\"\n            )\n\n        # Replay suffix bytes then consume next_byte\n        all_bytes = list(partial[k:]) + [next_byte]\n        splits_used = 0\n\n        for b in all_bytes:\n            next_state = current &lt;&lt; b\n            if next_state is not None:\n                current = next_state\n                continue\n\n            # Can't consume this byte - try extend (commit current partial) first\n            if self.max_splits is not None and splits_used &gt;= self.max_splits:\n                if self.verbose:\n                    print(f\"[heal] k={k}: hit max_splits={self.max_splits}\")\n                return None\n\n            extended = current.extend()\n            if extended is None:\n                if self.verbose:\n                    print(f\"[heal] k={k}: can't extend at {bytes(current.partial)!r}\")\n                return None\n\n            current = await extended.materialize()\n            splits_used += 1\n            if self.verbose:\n                print(f\"[heal] k={k}: split #{splits_used}, w={current.weight:.2f}\")\n\n            # Retry consuming the byte after extend\n            next_state = current &lt;&lt; b\n            if next_state is None:\n                if self.verbose:\n                    print(\n                        f\"[heal] k={k}: couldn't consume {format_byte(b)} even after extend\"\n                    )\n                return None\n            current = next_state\n\n        if self.verbose:\n            print(f\"[heal] SUCCESS at k={k}: w={current.weight:.2f}\")\n        return current\n</code></pre>"},{"location":"reference/genlm/bytes/byte_lm/heal/#genlm.bytes.byte_lm.heal.TokenHealer.try_heal","title":"<code>try_heal(state, next_byte)</code>  <code>async</code>","text":"<p>Try to heal a state so it can consume next_byte.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <p>A materialized LazyTrieState that cannot consume next_byte</p> required <code>next_byte</code> <code>int</code> <p>The byte we want to consume</p> required <p>Returns:</p> Type Description <p>LazyTrieState if healing succeeds, None otherwise</p> Source code in <code>genlm/bytes/byte_lm/heal.py</code> <pre><code>async def try_heal(self, state, next_byte: int):\n    \"\"\"Try to heal a state so it can consume next_byte.\n\n    Args:\n        state: A materialized LazyTrieState that cannot consume next_byte\n        next_byte: The byte we want to consume\n\n    Returns:\n        LazyTrieState if healing succeeds, None otherwise\n    \"\"\"\n    partial = state.partial\n    partial_len = len(partial)\n\n    if self.verbose:\n        print(\n            f\"[heal] Start: next_byte={format_byte(next_byte)}, partial={bytes(partial)!r}, max_backoff={self.max_backoff}\"\n        )\n\n    # Extract invariants computed once for all k values\n    trie = state.trie.trie\n    # base_weight undoes current path contribution: weight + mass[root] - mass[node]\n    # NOTE: mass[root] terms cancel, written this way to show undo current path contribution, add commit path\n    base_weight = state.weight - (state.mass[state.node] - state.mass[trie.root])\n\n    # Calculate how far back we're allowed to go\n    min_k = (\n        0 if self.max_backoff is None else max(0, partial_len - self.max_backoff)\n    )\n\n    # Try each backoff position k (from longest prefix to shortest)\n    for k in range(partial_len, min_k - 1, -1):\n        result = await self._try_at_k(state, trie, base_weight, k, next_byte)\n        if result is not None:\n            return result\n\n    if self.verbose:\n        print(\"[heal] FAILED: no valid healing found\")\n    return None\n</code></pre>"},{"location":"reference/genlm/bytes/byte_lm/lm_state/","title":"lm_state","text":""},{"location":"reference/genlm/bytes/byte_lm/lm_state/#genlm.bytes.byte_lm.lm_state","title":"<code>genlm.bytes.byte_lm.lm_state</code>","text":""},{"location":"reference/genlm/bytes/byte_lm/lm_state/#genlm.bytes.byte_lm.lm_state.StatefulTokenizedLM","title":"<code>StatefulTokenizedLM</code>","text":"<p>A stateful tokenized language model that maintains context and generates next token logprobs.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>AsyncLM</code> <p>The underlying language model</p> required <code>context</code> <code>list</code> <p>List of token IDs representing the current context</p> required <code>n_calls</code> <code>int</code> <p>Number of times the model has been called</p> <code>0</code> <code>max_context_length</code> <code>int</code> <p>Maximum length of context to maintain</p> <code>None</code> Source code in <code>genlm/bytes/byte_lm/lm_state.py</code> <pre><code>class StatefulTokenizedLM:\n    \"\"\"A stateful tokenized language model that maintains context and generates next token logprobs.\n\n    Args:\n        model (genlm.backend.AsyncLM): The underlying language model\n        context (list): List of token IDs representing the current context\n        n_calls (int): Number of times the model has been called\n        max_context_length (int, optional): Maximum length of context to maintain\n    \"\"\"\n\n    def __init__(self, model, context, n_calls=0, max_context_length=None):\n        self.model = model\n        self.context = context\n        self._n_calls = n_calls\n        self.max_context_length = max_context_length\n\n    @classmethod\n    def initial(cls, model, initial_context=None, max_context_length=None):\n        \"\"\"Creates an initial state for the language model.\n\n        Args:\n            model (genlm.backend.AsyncLM): The language model to use\n            initial_context (list, optional): Initial context of token IDs. Defaults to [tokenizer.bos_token_id]\n            max_context_length (int, optional): Maximum context length to maintain\n\n        Returns:\n            (StatefulTokenizedLM): A new instance with initial state\n        \"\"\"\n        if initial_context is None:\n            initial_context = [model.tokenizer.bos_token_id]\n        return cls(model, initial_context, max_context_length=max_context_length)\n\n    def __lshift__(self, token):\n        \"\"\"Adds a new token to the context and returns a new state.\n\n        Args:\n            token (int): Token ID to add to context\n\n        Returns:\n            (StatefulTokenizedLM): New state with updated context\n        \"\"\"\n        assert isinstance(token, int)\n        if (\n            self.max_context_length is not None\n            and len(self.context) &gt;= self.max_context_length\n        ):\n            self.context = self.context[-(self.max_context_length - 1) :]\n        return StatefulTokenizedLM(\n            self.model, self.context + [token], n_calls=self._n_calls\n        )\n\n    async def logp_next(self):\n        \"\"\"Computes log probabilities for the next token given the current context.\n\n        Returns:\n            (torch.Tensor): Log probabilities for next tokens\n        \"\"\"\n        self._n_calls += 1\n        return await self.model.next_token_logprobs(self.context)\n\n    def __repr__(self):\n        return colors.purple % (\n            \"|\".join([escape(self.model.byte_vocab[x]) for x in self.context])\n        )\n</code></pre>"},{"location":"reference/genlm/bytes/byte_lm/lm_state/#genlm.bytes.byte_lm.lm_state.StatefulTokenizedLM.initial","title":"<code>initial(model, initial_context=None, max_context_length=None)</code>  <code>classmethod</code>","text":"<p>Creates an initial state for the language model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>AsyncLM</code> <p>The language model to use</p> required <code>initial_context</code> <code>list</code> <p>Initial context of token IDs. Defaults to [tokenizer.bos_token_id]</p> <code>None</code> <code>max_context_length</code> <code>int</code> <p>Maximum context length to maintain</p> <code>None</code> <p>Returns:</p> Type Description <code>StatefulTokenizedLM</code> <p>A new instance with initial state</p> Source code in <code>genlm/bytes/byte_lm/lm_state.py</code> <pre><code>@classmethod\ndef initial(cls, model, initial_context=None, max_context_length=None):\n    \"\"\"Creates an initial state for the language model.\n\n    Args:\n        model (genlm.backend.AsyncLM): The language model to use\n        initial_context (list, optional): Initial context of token IDs. Defaults to [tokenizer.bos_token_id]\n        max_context_length (int, optional): Maximum context length to maintain\n\n    Returns:\n        (StatefulTokenizedLM): A new instance with initial state\n    \"\"\"\n    if initial_context is None:\n        initial_context = [model.tokenizer.bos_token_id]\n    return cls(model, initial_context, max_context_length=max_context_length)\n</code></pre>"},{"location":"reference/genlm/bytes/byte_lm/lm_state/#genlm.bytes.byte_lm.lm_state.StatefulTokenizedLM.__lshift__","title":"<code>__lshift__(token)</code>","text":"<p>Adds a new token to the context and returns a new state.</p> <p>Parameters:</p> Name Type Description Default <code>token</code> <code>int</code> <p>Token ID to add to context</p> required <p>Returns:</p> Type Description <code>StatefulTokenizedLM</code> <p>New state with updated context</p> Source code in <code>genlm/bytes/byte_lm/lm_state.py</code> <pre><code>def __lshift__(self, token):\n    \"\"\"Adds a new token to the context and returns a new state.\n\n    Args:\n        token (int): Token ID to add to context\n\n    Returns:\n        (StatefulTokenizedLM): New state with updated context\n    \"\"\"\n    assert isinstance(token, int)\n    if (\n        self.max_context_length is not None\n        and len(self.context) &gt;= self.max_context_length\n    ):\n        self.context = self.context[-(self.max_context_length - 1) :]\n    return StatefulTokenizedLM(\n        self.model, self.context + [token], n_calls=self._n_calls\n    )\n</code></pre>"},{"location":"reference/genlm/bytes/byte_lm/lm_state/#genlm.bytes.byte_lm.lm_state.StatefulTokenizedLM.logp_next","title":"<code>logp_next()</code>  <code>async</code>","text":"<p>Computes log probabilities for the next token given the current context.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>Log probabilities for next tokens</p> Source code in <code>genlm/bytes/byte_lm/lm_state.py</code> <pre><code>async def logp_next(self):\n    \"\"\"Computes log probabilities for the next token given the current context.\n\n    Returns:\n        (torch.Tensor): Log probabilities for next tokens\n    \"\"\"\n    self._n_calls += 1\n    return await self.model.next_token_logprobs(self.context)\n</code></pre>"},{"location":"reference/genlm/bytes/byte_lm/lm_state/#genlm.bytes.byte_lm.lm_state.StatefulByteLM","title":"<code>StatefulByteLM</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for byte-level language models with state.</p> Source code in <code>genlm/bytes/byte_lm/lm_state.py</code> <pre><code>class StatefulByteLM(ABC):\n    \"\"\"Abstract base class for byte-level language models with state.\"\"\"\n\n    @abstractmethod\n    async def __lshift__(self, b: int):\n        \"\"\"Adds a byte to the state and returns new state.\n\n        Args:\n            b (int): Byte to add\n\n        Returns:\n            (StatefulByteLM): New state with added byte\n        \"\"\"\n        pass  # pragma: no cover\n\n    def prune(self):\n        \"\"\"Prunes the current state if needed.\n\n        Override in subclasses.\n\n        Returns:\n            (StatefulByteLM): Pruned state\n        \"\"\"\n        raise NotImplementedError(\n            \"Subclasses must implement this method\"\n        )  # pragma: no cover\n\n    @abstractmethod\n    async def logp_next(self):\n        \"\"\"Computes the log probability distribution for the next byte.\n\n        Returns:\n            (LazyByteProbs): Log probabilities for next possible bytes\n        \"\"\"\n        pass  # pragma: no cover\n\n    async def greedy(self, context, steps):\n        \"\"\"Performs greedy decoding for given number of steps.\n\n        Args:\n            context (bytes): Initial byte context\n            steps (int): Number of generation steps\n\n        Returns:\n            (bytes): Generated byte sequence\n        \"\"\"\n        context = list(context)\n        state = await self.prefill(context)\n        for _ in range(steps):\n            Q = (await state.logp_next()).materialize()\n            b = Q.argmax()\n            state = await (state.prune() &lt;&lt; b)\n            context.append(b)\n        return bytes(context)\n\n    async def sample(self, context, steps, draw=sample_dict):\n        \"\"\"Samples from the model for given number of steps.\n\n        Args:\n            context (bytes): Initial byte context\n            steps (int): Number of generation steps\n            draw: Sampling function to use (default: sample_dict)\n\n        Returns:\n            (bytes): Generated byte sequence\n        \"\"\"\n        context = list(context)\n        state = await self.prefill(context)\n        for _ in range(steps):\n            Q = (await state.logp_next()).materialize()\n            b = draw(Q.map_values(exp))\n            state = await (state.prune() &lt;&lt; b)\n            context.append(b)\n        return bytes(context)\n\n    async def cleanup(self):\n        \"\"\"Performs any necessary cleanup of the model state.\"\"\"\n        pass  # pragma: no cover\n</code></pre>"},{"location":"reference/genlm/bytes/byte_lm/lm_state/#genlm.bytes.byte_lm.lm_state.StatefulByteLM.__lshift__","title":"<code>__lshift__(b)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Adds a byte to the state and returns new state.</p> <p>Parameters:</p> Name Type Description Default <code>b</code> <code>int</code> <p>Byte to add</p> required <p>Returns:</p> Type Description <code>StatefulByteLM</code> <p>New state with added byte</p> Source code in <code>genlm/bytes/byte_lm/lm_state.py</code> <pre><code>@abstractmethod\nasync def __lshift__(self, b: int):\n    \"\"\"Adds a byte to the state and returns new state.\n\n    Args:\n        b (int): Byte to add\n\n    Returns:\n        (StatefulByteLM): New state with added byte\n    \"\"\"\n    pass  # pragma: no cover\n</code></pre>"},{"location":"reference/genlm/bytes/byte_lm/lm_state/#genlm.bytes.byte_lm.lm_state.StatefulByteLM.prune","title":"<code>prune()</code>","text":"<p>Prunes the current state if needed.</p> <p>Override in subclasses.</p> <p>Returns:</p> Type Description <code>StatefulByteLM</code> <p>Pruned state</p> Source code in <code>genlm/bytes/byte_lm/lm_state.py</code> <pre><code>def prune(self):\n    \"\"\"Prunes the current state if needed.\n\n    Override in subclasses.\n\n    Returns:\n        (StatefulByteLM): Pruned state\n    \"\"\"\n    raise NotImplementedError(\n        \"Subclasses must implement this method\"\n    )  # pragma: no cover\n</code></pre>"},{"location":"reference/genlm/bytes/byte_lm/lm_state/#genlm.bytes.byte_lm.lm_state.StatefulByteLM.logp_next","title":"<code>logp_next()</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Computes the log probability distribution for the next byte.</p> <p>Returns:</p> Type Description <code>LazyByteProbs</code> <p>Log probabilities for next possible bytes</p> Source code in <code>genlm/bytes/byte_lm/lm_state.py</code> <pre><code>@abstractmethod\nasync def logp_next(self):\n    \"\"\"Computes the log probability distribution for the next byte.\n\n    Returns:\n        (LazyByteProbs): Log probabilities for next possible bytes\n    \"\"\"\n    pass  # pragma: no cover\n</code></pre>"},{"location":"reference/genlm/bytes/byte_lm/lm_state/#genlm.bytes.byte_lm.lm_state.StatefulByteLM.greedy","title":"<code>greedy(context, steps)</code>  <code>async</code>","text":"<p>Performs greedy decoding for given number of steps.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>bytes</code> <p>Initial byte context</p> required <code>steps</code> <code>int</code> <p>Number of generation steps</p> required <p>Returns:</p> Type Description <code>bytes</code> <p>Generated byte sequence</p> Source code in <code>genlm/bytes/byte_lm/lm_state.py</code> <pre><code>async def greedy(self, context, steps):\n    \"\"\"Performs greedy decoding for given number of steps.\n\n    Args:\n        context (bytes): Initial byte context\n        steps (int): Number of generation steps\n\n    Returns:\n        (bytes): Generated byte sequence\n    \"\"\"\n    context = list(context)\n    state = await self.prefill(context)\n    for _ in range(steps):\n        Q = (await state.logp_next()).materialize()\n        b = Q.argmax()\n        state = await (state.prune() &lt;&lt; b)\n        context.append(b)\n    return bytes(context)\n</code></pre>"},{"location":"reference/genlm/bytes/byte_lm/lm_state/#genlm.bytes.byte_lm.lm_state.StatefulByteLM.sample","title":"<code>sample(context, steps, draw=sample_dict)</code>  <code>async</code>","text":"<p>Samples from the model for given number of steps.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>bytes</code> <p>Initial byte context</p> required <code>steps</code> <code>int</code> <p>Number of generation steps</p> required <code>draw</code> <p>Sampling function to use (default: sample_dict)</p> <code>sample_dict</code> <p>Returns:</p> Type Description <code>bytes</code> <p>Generated byte sequence</p> Source code in <code>genlm/bytes/byte_lm/lm_state.py</code> <pre><code>async def sample(self, context, steps, draw=sample_dict):\n    \"\"\"Samples from the model for given number of steps.\n\n    Args:\n        context (bytes): Initial byte context\n        steps (int): Number of generation steps\n        draw: Sampling function to use (default: sample_dict)\n\n    Returns:\n        (bytes): Generated byte sequence\n    \"\"\"\n    context = list(context)\n    state = await self.prefill(context)\n    for _ in range(steps):\n        Q = (await state.logp_next()).materialize()\n        b = draw(Q.map_values(exp))\n        state = await (state.prune() &lt;&lt; b)\n        context.append(b)\n    return bytes(context)\n</code></pre>"},{"location":"reference/genlm/bytes/byte_lm/lm_state/#genlm.bytes.byte_lm.lm_state.StatefulByteLM.cleanup","title":"<code>cleanup()</code>  <code>async</code>","text":"<p>Performs any necessary cleanup of the model state.</p> Source code in <code>genlm/bytes/byte_lm/lm_state.py</code> <pre><code>async def cleanup(self):\n    \"\"\"Performs any necessary cleanup of the model state.\"\"\"\n    pass  # pragma: no cover\n</code></pre>"},{"location":"reference/genlm/bytes/byte_lm/trie_state/","title":"trie_state","text":""},{"location":"reference/genlm/bytes/byte_lm/trie_state/#genlm.bytes.byte_lm.trie_state","title":"<code>genlm.bytes.byte_lm.trie_state</code>","text":""},{"location":"reference/genlm/bytes/byte_lm/trie_state/#genlm.bytes.byte_lm.trie_state.LazyTrieState","title":"<code>LazyTrieState</code>","text":"<p>A lazy-evaluated state of a TokenByteTrie traversal.</p> <p>This class maintains the state of a language model while traversing a trie structure, lazily evaluating probabilities and maintaining the weight of the current path through the trie for beam search.</p> <p>Parameters:</p> Name Type Description Default <code>lm_state</code> <code>StatefulTokenizedLM</code> <p>Current language model state</p> required <code>trie</code> <code>TokenByteTrie</code> <p>Trie structure mapping tokens to byte sequences</p> required <code>node</code> <code>int</code> <p>Current node in the trie</p> required <code>weight</code> <code>float</code> <p>Cumulative log probability of the path to this node</p> required <code>mass</code> <code>ndarray</code> <p>Masses for each node in the trie for the current state</p> <code>None</code> <code>mode</code> <code>TrieMode</code> <p>Trie mode to use</p> <code>WITH_EOS</code> <code>terminated</code> <code>bool</code> <p>Whether the state is terminated (EOS has been consumed)</p> <code>False</code> Source code in <code>genlm/bytes/byte_lm/trie_state.py</code> <pre><code>class LazyTrieState:\n    \"\"\"A lazy-evaluated state of a TokenByteTrie traversal.\n\n    This class maintains the state of a language model while traversing a trie structure,\n    lazily evaluating probabilities and maintaining the weight of the current path through the trie\n    for beam search.\n\n    Args:\n        lm_state (StatefulTokenizedLM): Current language model state\n        trie (TokenByteTrie): Trie structure mapping tokens to byte sequences\n        node (int): Current node in the trie\n        weight (float): Cumulative log probability of the path to this node\n        mass (numpy.ndarray, optional): Masses for each node in the trie for the current state\n        mode (TrieMode): Trie mode to use\n        terminated (bool): Whether the state is terminated (EOS has been consumed)\n    \"\"\"\n\n    def __init__(\n        self,\n        lm_state,\n        trie,\n        node,\n        weight,\n        mass=None,\n        mode=TrieMode.WITH_EOS,\n        terminated=False,\n    ):\n        self.lm_state = lm_state\n        self.trie = trie\n        self.node = node\n        self.weight = weight\n        self._mass = mass\n        self._extend = None\n        self.mode = mode\n        self.root = self.trie.trie.root\n        self.children = self.trie.trie.children\n        self.terminated = terminated\n\n    @classmethod\n    def initial(cls, lm, trie, mode=TrieMode.WITH_EOS):\n        \"\"\"Creates an initial trie state.\n\n        Args:\n            lm (genlm.backend.AsyncLM): Language model to use\n            trie (TokenByteTrie): TokenByteTrie structure for byte-to-token mapping\n            mode (TrieMode): Trie mode to use\n\n        Returns:\n            (LazyTrieState): Initial state at root of trie with weight 0.0\n        \"\"\"\n        return cls(\n            trie=trie,\n            node=trie.trie.root,\n            lm_state=StatefulTokenizedLM.initial(lm),\n            weight=0.0,\n            mode=mode,\n        )\n\n    @property\n    def partial(self):\n        \"\"\"Returns the byte sequence corresponding to the current node in the trie.\"\"\"\n        return self.trie.trie.node2prefix[self.node]\n\n    @property\n    def mass(self):\n        \"\"\"Returns the log mass for each node in the trie.\n\n        The mass at a node corresponds to the sum of the probabilities of all\n        tokens which share the prefix (`self.partial`) represented by that node.\n\n        Raises:\n            ValueError: If state hasn't been materialized yet\n        \"\"\"\n        if self._mass is None:\n            raise ValueError(\"State is not yet materialized.\")\n        return self._mass\n\n    def with_mode(self, mode):\n        \"\"\"Returns a new state with the given mode.\"\"\"\n        return LazyTrieState(\n            lm_state=self.lm_state,\n            trie=self.trie,\n            node=self.node,\n            weight=self.weight,\n            mass=self._mass,\n            mode=mode,\n            terminated=self.terminated,\n        )\n\n    def actions(self):\n        \"\"\"Returns possible byte transitions from current node.\"\"\"\n        return self.children[self.node]\n\n    def get_EOT(self):\n        \"\"\"Returns the end-of-token node if available from current position in the trie.\"\"\"\n        return self.children[self.node].get(self.trie.trie.eot_token)\n\n    def __lshift__(self, b):\n        \"\"\"Transitions to a new state by consuming a byte.\n\n        Args:\n            b (int): Byte to consume\n\n        Returns:\n            (LazyTrieState|None): New state after consuming byte, or None if transition invalid (terminated or EOS)\n        \"\"\"\n        if self.terminated:\n            return None\n\n        if node := self.children[self.node].get(b):\n            mass = self.mass\n            return LazyTrieState(\n                lm_state=self.lm_state,\n                trie=self.trie,\n                mass=mass,\n                node=node,\n                weight=self.weight + mass[node] - mass[self.node],\n                mode=self.mode,\n                terminated=b == EOS,\n            )\n\n    def extend(self):\n        \"\"\"Extends current state by consuming an end-of-token if possible.\n\n        Returns:\n            (LazyTrieState|None): New state after consuming EOT, or None if not possible\n        \"\"\"\n        if self._extend is None:\n            if (eot_node := self.get_EOT()) is not None:\n                mass = self.mass\n                self._extend = LazyTrieState(\n                    lm_state=self.lm_state\n                    &lt;&lt; int(self.trie.trie.leaf2token_id[eot_node]),\n                    trie=self.trie,\n                    node=self.root,\n                    weight=self.weight + mass[eot_node] - mass[self.node],\n                    mode=self.mode,\n                )\n        return self._extend\n\n    @cached_property\n    def logp_next(self):\n        \"\"\"Computes log probabilities for next possible transitions.\n\n        Returns:\n            (LazyByteProbs): Lazy log probability distribution over possible next bytes\n        \"\"\"\n        logps = np.full(258, -np.inf)  # 258 for EOT, EOS + 256 for normal bytes\n        mass = self.mass\n        logZ = mass[self.node]\n\n        for byte, node in self.actions().items():\n            logps[byte if byte is not None else 256] = mass[node] - logZ\n\n        return LazyByteProbs(logps)\n\n    async def materialize(self):\n        \"\"\"Materializes the masses for each node in the trie for the current state.\n\n        This makes a call to the language model and the underlying trie.\n\n        Returns:\n            (LazyTrieState): Self with materialized masses\n        \"\"\"\n        if self._mass is None:\n            logp_next = await self.lm_state.logp_next()\n            log_mass = await self.trie.weight_sum(torch.exp(logp_next), self.mode)\n            mass = torch.log(log_mass)\n            self._mass = mass.cpu().numpy()\n        return self\n\n    def __repr__(self):\n        context = colors.green % (\"|\" + escape(bytes(self.partial)))\n        if self.terminated:\n            context += colors.yellow % \"&lt;EOS&gt;\"\n        return f\"{self.weight:.2f}: {self.lm_state}\" + context\n\n    async def cleanup(self):\n        \"\"\"Cleans up resources used by the trie.\"\"\"\n        await self.trie.cleanup()\n</code></pre>"},{"location":"reference/genlm/bytes/byte_lm/trie_state/#genlm.bytes.byte_lm.trie_state.LazyTrieState.initial","title":"<code>initial(lm, trie, mode=TrieMode.WITH_EOS)</code>  <code>classmethod</code>","text":"<p>Creates an initial trie state.</p> <p>Parameters:</p> Name Type Description Default <code>lm</code> <code>AsyncLM</code> <p>Language model to use</p> required <code>trie</code> <code>TokenByteTrie</code> <p>TokenByteTrie structure for byte-to-token mapping</p> required <code>mode</code> <code>TrieMode</code> <p>Trie mode to use</p> <code>WITH_EOS</code> <p>Returns:</p> Type Description <code>LazyTrieState</code> <p>Initial state at root of trie with weight 0.0</p> Source code in <code>genlm/bytes/byte_lm/trie_state.py</code> <pre><code>@classmethod\ndef initial(cls, lm, trie, mode=TrieMode.WITH_EOS):\n    \"\"\"Creates an initial trie state.\n\n    Args:\n        lm (genlm.backend.AsyncLM): Language model to use\n        trie (TokenByteTrie): TokenByteTrie structure for byte-to-token mapping\n        mode (TrieMode): Trie mode to use\n\n    Returns:\n        (LazyTrieState): Initial state at root of trie with weight 0.0\n    \"\"\"\n    return cls(\n        trie=trie,\n        node=trie.trie.root,\n        lm_state=StatefulTokenizedLM.initial(lm),\n        weight=0.0,\n        mode=mode,\n    )\n</code></pre>"},{"location":"reference/genlm/bytes/byte_lm/trie_state/#genlm.bytes.byte_lm.trie_state.LazyTrieState.partial","title":"<code>partial</code>  <code>property</code>","text":"<p>Returns the byte sequence corresponding to the current node in the trie.</p>"},{"location":"reference/genlm/bytes/byte_lm/trie_state/#genlm.bytes.byte_lm.trie_state.LazyTrieState.mass","title":"<code>mass</code>  <code>property</code>","text":"<p>Returns the log mass for each node in the trie.</p> <p>The mass at a node corresponds to the sum of the probabilities of all tokens which share the prefix (<code>self.partial</code>) represented by that node.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If state hasn't been materialized yet</p>"},{"location":"reference/genlm/bytes/byte_lm/trie_state/#genlm.bytes.byte_lm.trie_state.LazyTrieState.with_mode","title":"<code>with_mode(mode)</code>","text":"<p>Returns a new state with the given mode.</p> Source code in <code>genlm/bytes/byte_lm/trie_state.py</code> <pre><code>def with_mode(self, mode):\n    \"\"\"Returns a new state with the given mode.\"\"\"\n    return LazyTrieState(\n        lm_state=self.lm_state,\n        trie=self.trie,\n        node=self.node,\n        weight=self.weight,\n        mass=self._mass,\n        mode=mode,\n        terminated=self.terminated,\n    )\n</code></pre>"},{"location":"reference/genlm/bytes/byte_lm/trie_state/#genlm.bytes.byte_lm.trie_state.LazyTrieState.actions","title":"<code>actions()</code>","text":"<p>Returns possible byte transitions from current node.</p> Source code in <code>genlm/bytes/byte_lm/trie_state.py</code> <pre><code>def actions(self):\n    \"\"\"Returns possible byte transitions from current node.\"\"\"\n    return self.children[self.node]\n</code></pre>"},{"location":"reference/genlm/bytes/byte_lm/trie_state/#genlm.bytes.byte_lm.trie_state.LazyTrieState.get_EOT","title":"<code>get_EOT()</code>","text":"<p>Returns the end-of-token node if available from current position in the trie.</p> Source code in <code>genlm/bytes/byte_lm/trie_state.py</code> <pre><code>def get_EOT(self):\n    \"\"\"Returns the end-of-token node if available from current position in the trie.\"\"\"\n    return self.children[self.node].get(self.trie.trie.eot_token)\n</code></pre>"},{"location":"reference/genlm/bytes/byte_lm/trie_state/#genlm.bytes.byte_lm.trie_state.LazyTrieState.__lshift__","title":"<code>__lshift__(b)</code>","text":"<p>Transitions to a new state by consuming a byte.</p> <p>Parameters:</p> Name Type Description Default <code>b</code> <code>int</code> <p>Byte to consume</p> required <p>Returns:</p> Type Description <code>LazyTrieState | None</code> <p>New state after consuming byte, or None if transition invalid (terminated or EOS)</p> Source code in <code>genlm/bytes/byte_lm/trie_state.py</code> <pre><code>def __lshift__(self, b):\n    \"\"\"Transitions to a new state by consuming a byte.\n\n    Args:\n        b (int): Byte to consume\n\n    Returns:\n        (LazyTrieState|None): New state after consuming byte, or None if transition invalid (terminated or EOS)\n    \"\"\"\n    if self.terminated:\n        return None\n\n    if node := self.children[self.node].get(b):\n        mass = self.mass\n        return LazyTrieState(\n            lm_state=self.lm_state,\n            trie=self.trie,\n            mass=mass,\n            node=node,\n            weight=self.weight + mass[node] - mass[self.node],\n            mode=self.mode,\n            terminated=b == EOS,\n        )\n</code></pre>"},{"location":"reference/genlm/bytes/byte_lm/trie_state/#genlm.bytes.byte_lm.trie_state.LazyTrieState.extend","title":"<code>extend()</code>","text":"<p>Extends current state by consuming an end-of-token if possible.</p> <p>Returns:</p> Type Description <code>LazyTrieState | None</code> <p>New state after consuming EOT, or None if not possible</p> Source code in <code>genlm/bytes/byte_lm/trie_state.py</code> <pre><code>def extend(self):\n    \"\"\"Extends current state by consuming an end-of-token if possible.\n\n    Returns:\n        (LazyTrieState|None): New state after consuming EOT, or None if not possible\n    \"\"\"\n    if self._extend is None:\n        if (eot_node := self.get_EOT()) is not None:\n            mass = self.mass\n            self._extend = LazyTrieState(\n                lm_state=self.lm_state\n                &lt;&lt; int(self.trie.trie.leaf2token_id[eot_node]),\n                trie=self.trie,\n                node=self.root,\n                weight=self.weight + mass[eot_node] - mass[self.node],\n                mode=self.mode,\n            )\n    return self._extend\n</code></pre>"},{"location":"reference/genlm/bytes/byte_lm/trie_state/#genlm.bytes.byte_lm.trie_state.LazyTrieState.logp_next","title":"<code>logp_next</code>  <code>cached</code> <code>property</code>","text":"<p>Computes log probabilities for next possible transitions.</p> <p>Returns:</p> Type Description <code>LazyByteProbs</code> <p>Lazy log probability distribution over possible next bytes</p>"},{"location":"reference/genlm/bytes/byte_lm/trie_state/#genlm.bytes.byte_lm.trie_state.LazyTrieState.materialize","title":"<code>materialize()</code>  <code>async</code>","text":"<p>Materializes the masses for each node in the trie for the current state.</p> <p>This makes a call to the language model and the underlying trie.</p> <p>Returns:</p> Type Description <code>LazyTrieState</code> <p>Self with materialized masses</p> Source code in <code>genlm/bytes/byte_lm/trie_state.py</code> <pre><code>async def materialize(self):\n    \"\"\"Materializes the masses for each node in the trie for the current state.\n\n    This makes a call to the language model and the underlying trie.\n\n    Returns:\n        (LazyTrieState): Self with materialized masses\n    \"\"\"\n    if self._mass is None:\n        logp_next = await self.lm_state.logp_next()\n        log_mass = await self.trie.weight_sum(torch.exp(logp_next), self.mode)\n        mass = torch.log(log_mass)\n        self._mass = mass.cpu().numpy()\n    return self\n</code></pre>"},{"location":"reference/genlm/bytes/byte_lm/trie_state/#genlm.bytes.byte_lm.trie_state.LazyTrieState.cleanup","title":"<code>cleanup()</code>  <code>async</code>","text":"<p>Cleans up resources used by the trie.</p> Source code in <code>genlm/bytes/byte_lm/trie_state.py</code> <pre><code>async def cleanup(self):\n    \"\"\"Cleans up resources used by the trie.\"\"\"\n    await self.trie.cleanup()\n</code></pre>"}]}